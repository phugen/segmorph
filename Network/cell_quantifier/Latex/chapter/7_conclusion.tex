\chapter {Conclusion}
\label{chap:conclusion}

The experiments conducted in Chapter \textbf{\ref{chap:results}} indicate that both the F-Measure score and the weighted Cross-Entropy loss work as loss functions for the U-Net architecture, although it the experiments were indecisive as to whether the F-Measure loss function could potentially perform better than the weighted Cross-Entropy loss on a different dataset, requiring further testing. However, it definitely possesses the advantage of requiring no additional knowledge about the distribution of classes inside the dataset, which means that it works ``out of the box'' and does not depend on a per-image or an average per-dataset weightmap to produce good results.

Adding Batch Normalization layers to the network was shown to be extremely beneficial, allowing to use a learning rates five times higher than the previous one, while shortening the training by a factor of five. It might even be possible to use a learning rate that is even higher, as \cite{batchnorm} claims that increasing the learning rate tenfold should still lead to a converging network when using BN. However, using BN comes with the drawback of having to include BN layers for each convolutional layer in the network, drastically increasing the GPU memory needed for running the network. Even with a top-of-the-line GPU such as the Nvidia TITAN X Pascal, this forces one to use a low batch size during SGD training, leading to a noisy gradient because otherwise, memory would not suffice - this also puts a constraint on increasing the depth or width of the network further as the memory usage increases much faster with each convolutional layer.

Shuffling the training set further reduced the training time by $\approx$35\%, while having no discernable impact on the validation results. MSRA initialization and activation functions other than ReLU might be useful for other applications, but for the Drosophila segmentation, the improvements, if any, were within the weight initialization error range and thus negligible.

It should be noted, however, that the best networks converge after about 2 epochs, with a constant validation loss. This shows that overfitting is not yet taking place, because the validation loss would start increasing again, but still, no improvement is made. This is believed to be the cause of two main factors: Firstly, the training set is rather small for such a complex task. Surely, providing more than 90 original ground truth images would have led to a better result, at least concerning the detection of the Lamellopodia and Filopodia classes. Alternatively, more aggressive data augmentation could be performed by introducing artificial noise to the images and rotating by arbitrary degrees instead of multiple of 90 degrees only. Secondly, the U-Net might simply not be an architecture that has the capacity to learn the task to a higher degree of precision than previously attained. As mentioned before, increasing the size of the network further is not possible with the currently available hardware and might not even lead to much better results. Instead, it might be necessary to try a new network architecture altogether. For instance, Hinton \cite{capsules} proposes replacing the popular maximum pooling operation altogether by grouping neurons together in ``capsules'', a sort of sub-networks that specialize in a certain recognition task. The intuition why this is supposed to be better than maximum pooling is that while maximum pooling reduces the complexity of a CNN and makes it translationally invariant, it also irreversibly discards spatial information about activations, which seems completely counter-productive when trying to achieve precise segmentations that rely on the spatial relationships between image components. The U-Net's skip connections try to alleviate this problem by feature map concatenation before maximum pooling is done, but whether this performs better than Hinton's experimental ``capsule'' approach is still unclear at the time of writing.

Concludingly, the best networks still achieve a qualitatively well-interpretable result that is far above the results of methods that are not based on neural networks, especially when reducing the dataset to 3 classes because learning the thin Filopodia pixels seems to be a hard task - they are often ``broken'', with bogus Filopodia pixels that are not attached to the cell body (see \textbf{\ref{fig:qualitative}}).