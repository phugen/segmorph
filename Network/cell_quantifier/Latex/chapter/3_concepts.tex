\chapter{Basic Concepts}
\label{chap:concepts}

Image segmentation is one of the main challenges in image processing and has been an active field of study for years, which results in many segmentation methods being available. In the following pages, a selection of methods that can perform multi-class segmentation, i.e. segmentation with more than two classes, is presented. As the number of images to be segmented is usually high, and computation time can be a crucial factor, human interaction should remain at a minimum level. Therefore, only methods that, once the segmentation process has been started, work without human interaction were chosen. All methods described here are later used to segment a set of images and compared to the ground truth to measure the quality of the segmentations (see Chapter \ref{chap:results}).
	
	\section{Thresholding}
\label{sec:thresholding}

Thresholding is the most basic segmentation algorithm there is: Given an input image with dimensions $x \times y$ and intensity values $z$ (for instance, $[0, 255]$ at 8-bit color depth), defined as a function 

\[I(x, y) \to z, \text{ for } x, y, z \in \mathbb{N}_0,\]

\noindent the thresholding function is defined as follows:

\[ T(I(x, y), \theta) =  \begin{cases}
				1 \text{ if } I(x, y) \, \geq \, \theta \\
			           0 \text{ otherwise}
			     \end{cases}
\]


\noindent This yields a binary segmentation of the image into two classes, given that $\theta$ is chosen properly. The choice of $\theta$ is therefore crucial for the success of the segmentation. One way to find a suitable threshold parameter is (automated) image histogram analysis, such as in \textit{Otsu thresholding}.\\

\noindent The Otsu thresholding algorithm \cite{Otsu} iterates through all possible values for $\theta$ and calculates the ``between-class variance'' ${\sigma^{2}}_B$ for each $\theta$, defined as

\[ {\sigma^2}_{B} = w_0 \, w_1 (\mu_0 - \mu_1)^2,\]

\noindent where $w_0$ and $w_1$ are the weights, defined as

\begin {align}
	w_0 &= \sum \limits_{i=1}^{\theta} p_i \\
	w_1 &= \sum \limits_{i=\theta+1}^{k} p_i
\end {align}

\noindent for a histogram of the image consisting of $k$ grayscale intensity bins which each contain the probability for a pixel being in this bin, $p_i$.\footnote{Here, it is assumed that Otsu's algorithm is applied to a grayscale image. For RGB images, each channel can be thresholded separately using the same algorithm, or the image can be converted to grayscale and then be segmented.} $\mu_0$ and $\mu_1$ are the statistical mean values, defined as

\begin {align}
	\mu_0 &= \sum \limits_{i=1}^{\theta} i \, p_i \\
	\mu_1 &= \sum \limits_{i=\theta + 1}^{k} i \, p_i\\
\end {align}

\noindent The threshold with the maximum \textbf{between}-class variance corresponds to a segmentation in which the pixels of each class have minimum \textbf{within}-class variance: This means that the pixels of each class are very much alike.

\begin {figure}[!ht]
	\begin {subfigure}[t]{0.3\linewidth}
		\scalebox{0.65}{\inputpgf{img}{fig_otsu_histogram.pgf}}
	\end {subfigure}
	\hspace{5.5cm}
	\begin {subfigure}[b]{0.3\linewidth}
		\scalebox{0.65}{\inputpgf{img}{fig_otsu.pgf}}
	\end {subfigure}
		\caption[]{\textbf{Left:} Histogram of an 8-bit grayscale input image. Optimal thresholds found by Otsu's algorithm are marked in orange. \textbf{Right:} Original image (top) and four-way segmentation result.}
		\label{fig:otsu}
	\end {figure}

\noindent To extend the normal Otsu algorithm to $n$-class segmentation, the weight and mean formulas are changed to correspond to $n$ classes defined by a set of thresholds $\theta = \{t_1 \dots t_{n-1}\}$. The between-class variance formula is changed to respect the variances of all classes, so that the best $\theta$ is the one that minimizes the overall within-class variance by maximizing the overall between-class variance:

\[ \sigma_B^2 = \sum \limits_{i=1}^{n} w_i (\mu_i - \mu_t)^2 \]

\noindent Here, $w_i$ is the weight of the $i$th class, $\mu_i$ is the mean of that class and $\mu_t$ is the total mean of all classes, i.e. $\sum_{i=1}^{n} w_i \mu_i$. The thresholds can then be applied using

\[ T(I(x, y), \theta) =  \begin{cases}
				n \text{ if } I(x, y) \, \geq \, t_{n-1} \\
				n-1 \text { if } I(x, y) \, \geq t_{n-2} \land I(x, y) < t_{n-1}\\
				\dots\\
			           0 \text{ if } I(x, y) \, < t_1
			     \end{cases}
\]

\noindent However, this approach doesn't work as well for images whose histograms do not exhibit distinct histogram peaks - for example, due to noise - and also doesn't perform well for cases in which $n$ is large, neither considering the time needed to perform a segmentation\footnote{While variants of Otsu exist that are optimized for speed, such as \cite{otsu_fast}, only the original Otsu algorithm is evaluated here because it is the most common.} nor considering the segmentation quality, due to the method inspecting only the histogram data. \cite{Otsu}\\

%\noindent The Otsu algorithm belongs to a family of thresholding mechanisms that is called ``global thresholding'' because the same threshold is applied to the entire image. However, there are also local or ``adaptive'' thresholding algorithms which segment different parts of the image with different thresholds. The need for such algorithms often arises when the illumination of the input image is highly irregular, for example when cast shadows overlay part of the image (see \ref{fig:illumination}). One approach is to calculate a threshold for each pixel of the image while examining a neighborhood of size $L \times L$ around the pixel in question, using the mean, the mean of the maximum and minimum values or the median of the resulting local pixel intensity distribution to determine a local threshold.

%\textbf{TODO: are there adaptive thresholds for multi-label purposes?}\\
%Local thresholding, however, is much slower than global thresholding because it has to process the neighborhoods of each pixel, while it also depends on choosing $L$ so that the neighborhoods contain enough pixels of either class, or otherwise the thresholds won't be chosen well. \cite[pp.~84--93]{machine_vision}


\begin{comment}
	\section{Canny Edge Detection}
\label{sec:canny}
The \textit{Canny Edge Detector} \cite{canny} is one of the most famous edge-detection algorithms and actually is a composite algorithm that returns a binary segmentation of an image into edges and non-edges.\\

\noindent First, a Gaussian convolution filter is applied to try and subdue noise in the image. A \textit{convolution} is a matrix operation often used in image processing: A matrix $A$, describing the pixel values of an image, is convolved pixel-wise with a convolution matrix $B$ with dimensions $n \times n$ - that is, for each pixel $p$ of $A$, the pixel's $n \times n$-neighborhood pixel values are summed up while being weighted according to corresponding value in $B$. The resulting sum is then assigned to the output matrix in place of the previous value of the center pixel, which intuitively assumes the distance-weighted average value of its neighborhood. In the case of pixels that lie on the edge of the matrix to be convoluted, out-of-bounds considerations have to be made: A constant value such as zero can be assumed for the pixels in the neighborhood that would lie ``outside'' of the image, the existing image values can be mirrored or clamped to provide a torus-like out-of-bounds handling, or the convolution can be done only on those pixels whose neighborhood fully lies inside of the image, resulting in smaller output images.

The aforementioned ``Gaussian filter'' or ``kernel'' is a matrix whose values are defined so that performing a convolution using that matrix approximates the behavior of the two-dimensional Gaussian function with uniform variances for its $x$- and $y$-dimensions:

\[ f(x, y) = \exp \left(- \left( \frac{\left(x - p_x \right)^2}{2\sigma^2} + \frac{ \left(y - p_y \right)^2}{2\sigma^2} \right) \right), \]

\noindent where $p$ is the center pixel of the current neighborhood. Subsequently, $p_x$ and $p_y$ are the coordinates of this pixel within the matrix and $\sigma$, the standard variance, acts as the smoothing constant. The higher this constant is, the stronger the blur effect becomes.

In Gaussian filters, the $\sigma$ constant is expressed through the dimensions of the matrix - the larger the filter matrix, the stronger the blur effect. An example for a $3 \times 3$ Gaussian filter is the following matrix:

\[ \frac{1}{16} \left [ \begin{tabular}{ccc}
				1& 2& 1\\
				2& 4& 2\\
				1& 2& 1 
			   \end{tabular} \right ]\]

\noindent The coefficient $\frac{1}{16}$ is equal to the sum of the matrix values and ensures that the convolution does not change the average image value. \cite[p. 41]{machine_vision}\\

\noindent As the second step, an gradient-based edge detector filter is applied to the smoothed image. The most famous of these is the \textit{Sobel filter} \cite{sobel}, which approximates the local partial derivatives $\frac{\partial I}{\partial x}$ and $\frac{\partial I}{\partial y}$ of each pixel of the image function $I(x, y)$, using the $3 \times 3$ neighborhood of that pixel. It is given by the following matrices \cite[pp. 113 -- 114]{machine_vision}:

\[ \text{Sobel}_x = \left [ \begin{tabular}{ccc}
				-1& 0& 1\\
				-2& 0& 2\\
				-1& 0& 1 
			   \end{tabular} \right ] \text{ and } 
\text{Sobel}_y = \left [ \begin{tabular}{ccc}
				1& 2& 1\\
				0& 0& 0\\
				-1& -2& 1 
			   \end{tabular} \right ] 
\]

\noindent The result of these convolutions are two images, $G_x$ and $G_y$, which represent the partial local derivatives of each pixel. The gradient image of the original input image $I$ is then defined as

\[G = |\nabla I| = \sqrt{{G_x}^2 + {G_y}^2}.\]

\noindent Additionally, the gradient direction of each pixel can be calculated from the derivative images by measuring the angle between the x-axis and the gradient pixel coordinates by employing the atan2 function:

\[G_\phi = \text{atan2}(G_y, G_x) \]

\noindent Using $G_\phi$, edge thinning via non-maximum suppression is applied to the gradient image as the third step in the algorithm: For each pixel, the gradient direction acts as a criterion to decide which two neighboring pixels, that are each on opposite sides (positive and negative direction of the gradient), should be compared to the current pixel. If the value of the current pixel is not larger than the two neighbors' values, the pixel's value is not a local maximum and is set to zero. The gradient direction angles can either be rounded so that each angle represents one of the north-south, west-east directions and so forth, or linear interpolation can be used.

In the final step, a hysteresis threshold is applied. This process consists of defining two thresholds, $\theta_{high}$ and $\theta_{low}$. The definition for the thresholding function as given in \ref{sec:thresholding} is slightly altered:

\[ T(I(x, y), \theta_{high}, \theta_{low}) =  \begin{cases}
							2 \text{ if } I(x, y) \, \geq \, \theta_{high} \\
							1 \text{ if } I(x, y) \, \geq \, \theta_{low} \text{ and } < \theta_{high} \\
			          				0 \text{ otherwise}
			   			        \end{cases}
\]

\noindent Pixels that have a value of $2$ are called strong pixels because they had values larger than the high threshold, whereas pixels with a value of $1$ are called weak pixels. Finally, the algorithm checks for each pixel if an 8-connected path between that pixel and a strong pixel exists - if not, then the pixel is dropped. This can be done with the help of connected component-finding algorithms by dropping each ``$1$''-component which is not connected to at least one ``2''-value.\\

\noindent The thresholds $\theta_{high}$ and $\theta_{low}$ have to be set by the user, although there exists the possibility to set these by using the Otsu threshold described in \ref{sec:thresholding} . Using this combination approach, $\theta_{high}$ is set to the Otsu threshold value for $I(x, y)$, and $\theta_{low}$ is set to $0.5 * \theta_{high}$. \cite{otsu_combine}

\end {comment}

	\section{K-Means}
K-Means \cite{kmeans} is a general-purpose data clustering algorithm whose aim is to create $k$ data clusters from all $n$-dimensional data points $d = (f_1, f_2, \dots, f_n)$ so that the squared distance from each data point in the cluster to the cluster mean is minimized overall. Mathematically, this means calculating

\[ \argmin \limits_{C} \sum \limits_{i=1}^{k} \sum \limits_{d \in C_i} || d - \mu_i||^2 ,\]

\noindent where $C_{i \dots k}$ are the $k$ clusters and $\mu_{i \dots k}$ is the mean of the respective cluster. Viewed graphically, this is equal to computing a higher-order Voronoi diagram for the data, using the $k$ cluster centroids as the Voronoi cell centers.

The algorithm is initialized with $k$ either random or differently selected cluster centers. Then, the algorithm executes the two steps described in \ref{alg:kmeans_pseudo} alternatingly until either a set number of iterations is reached or the overall difference between the current and the last centroid positions falls below a threshold $\epsilon$  (see Figure \ref{fig:kmeans}).

\begin {algorithm}[!ht]
	\caption{K-Means ($\epsilon$, iter\_max)}\label{alg:kmeans_pseudo}
	\begin {algorithmic}[1]
		\State iter = 0
		\State Assign each data point to exactly one of the $k$ clusters by selecting the cluster that has the closest mean distance as defined above.
		\State Calculate new cluster centers by recalculating the mean of all elements assigned to each cluster and calculate difference $\Delta$ compared to previous iteration.
		\If {$\Delta < \epsilon$ \textbf{or} iter == iter\_max}
			\State \textbf{end}
		\Else
			\State iter += 1
			\State \textbf{goto} step 2
		\EndIf
	\end{algorithmic}
\end{algorithm}

\begin {figure}[!ht]
	\begin {center}
		\includegraphics[scale=0.8]{img/fig_kmeans}
	\end{center}
	\caption{Application of the K-Means algorithm to a test image. \textbf{From left to right}: Input image,  K-Means results with $k=4$, results with $k=6$.}
	\label{fig:kmeans}
\end {figure}

\noindent Obviously, this algorithm can also be used to segment images into $k$ different classes, but since K-Means assigns classes to data by using the distance from the mean, the algorithm output favors segmentations in which the classes have roughly the same size, which isn't necessarily the correct way to classify pixels in arbitrary image data.




	\section{Gaussian Mixture Models}
Gaussian Mixtures Models (GMMs) are a subclass of mixture models, that is, probabilistic models which are combined with other models of the same distribution type to form a more complex model that is able to model the distribution of a data set more accurately than a simple model could. In the case of GMMs, the base distributions are often $n$-multivariate Gaussian distributions:

\[ pdf(x) = \mathcal{N}_n (x\,|\,\mu,\, \Sigma) \]

\noindent Here, $pdf(x)$ denotes the probability, or density, of an $n$-dimensional piece of data $x$, while $\mu$ and $\Sigma$ are the $n$-dimensional mean vector and the $n \times n$ covariance matrix of the distribution. Since the shape such a distribution can take is limited, a single Gaussian cannot accurately model a multimodal distribution (see Figure \ref{fig:normal_vs_gmm}). Instead, a weighted combination of multiple Gaussians can be used instead:

\[ pdf_m (x) = \sum \limits_{k=1}^{K} \pi_k \, \mathcal{N}_n (x\,|\, \mu_k, \Sigma_k) \]

\noindent A mixture model consists of $K$ models that each have different parameters $\mu$ and $\Sigma$ and are weighted by weights $\pi_k \in \mathbb{R}$, $1 > \pi_k > 0$, with $\sum_{i=1}^{k} \pi_i = 1$. \cite[pp. 430]{bishop_pattern}

\begin {figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.75]{img/fig_normal_vs_gmm}
	\end{center}
	\caption{Comparison of a single Gaussian fit (upper) with a GMM fit, using $K=3$, on a two-dimensional dataset randomly sampled from three Gaussian distributions. The single Gaussian fails to capture the structure of the data, while the GMM succeeds.}
	\label{fig:normal_vs_gmm}
\end {figure}

\noindent The parameters of such a mixture model can be calculated by applying the Expectation-Maximization \cite{em_algorithm} algorithm, which can additionally be combined with a K-Means initialization to avoid shallow local minima. GMMs can also be used for segmenting images: Images are intepreted as numerical data in which each pixel position corresponds to an intensity value. If the number of components to be found in the image is known beforehand - as it is in the case of cell segmentation - the image can be segmented according to the fitted GMM (see Figure \ref{fig:gmm_vs_gt}). To do this, for each pixel in the image the posterior probabilities of that pixel in respect to each of the Gaussians is calculated and the Gaussian with the highest probability is chosen as the source distribution of the pixel, thus yielding a class with which the pixel is labelled:

\[ label_x = \argmax \limits_{k} \, p(\mu_k, \Sigma_k\, | \, x) \]

\begin {figure}[!ht]
	\centering
	\includegraphics[scale=0.55]{img/fig_gmm_vs_gt}
	\caption{\textbf{Left:} Input image. \textbf{Right:} Image labels in pseudocolor, assigned by a GMM with $K=4$, fitted to the input image.}
	\label{fig:gmm_vs_gt}
\end {figure}


	\section{Multi-layer Perceptrons}
\textit{Multi-layer Perceptrons} (MLPs) or \textit{Feedforward Artificial Neuronal Networks} (ANNs) are mathematical functions that try to emulate the way neurons in the brain process information, typically to classify data or perform regression on it. ANNs have recently achieved impressing results on various tasks, such as image classification, sound analysis and regression, typically achieved by very deep networks that used lots of artificial neurons and were fed large datasets exceeding one million samples. In this section, the concepts behind optimization of functions via Gradient Descent, neuronal networks and training of those networks using the Backpropagation algorithm are explained. 



	\subsection {Gradient Descent}
\label{subsec:grad_desc}

\textit{Gradient Descent} or is a standard, analytic first-order technique that iteratively optimizes at least once-differentiable, continuous functions, that is, that calculates a vector of parameters $\Theta$ for a function $f$ so that

\[ \Theta = \argmin \limits_{i} f(i) \]

\noindent To find values for $\Theta$, the approach takes advantage of the fact that the negative gradient vector $\nabla_f = [ \frac{\partial f}{\partial \Theta_0}, \dots, \frac{\partial f}{\partial \Theta_n} ]$, the vector of all first partial derivatives of $f$, points into the direction of the fastest change, or put graphically, the ``steepest slope''. By taking steps into the direction of this gradient, the function value is minimized step by step, while maximization works the same, only that the function is negated and then minimized, which results in parameters that maximize the original function. The parameter $\eta$ depicts the step size and is typically a value in the range $(0.0, 1.0]$. \cite[pp. 40--42]{optimization_book}

The algorithm starts with a guess for the parameters in $\Theta$ and then iteratively evaluates the gradient and updates the parameters accordingly until convergence within arbitrary precision is reached (see algorithm \ref{alg:grad_desc}).

\begin {algorithm}
	\begin {algorithmic}[1]
		\State $\Theta_0$ = random
		\While {$|f(\Theta_i) - f(\Theta_{i - 1})| > \epsilon$}
			\State $\Theta_i = \Theta_{i-1} - \eta \nabla_f$ 
		\EndWhile
	\end{algorithmic}
	\caption{Gradient Descent scheme for optimizing differentiable functions.}
	\label{alg:grad_desc}
\end{algorithm}

\noindent For convex functions, Gradient Descent always finds the global minimum. For non-convex functions however, the algorithm may get stuck in a local minimum or at saddle points, called ``false minima' (see Figure \ref{fig:grad_desc}). Therefore, running the algorithm multiple times with random values or even informed guesses given knowledge about the form of the function to be optimized is advised. The step size $\eta$ has to be chosen depending on the shape of the function to minimize; if $\eta$ is too large, the algorithm will overshoot the minimum and will never converge even if the minimum exists, and if $\eta$ is too small, convergence will take a long time.

\begin {figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.9]{img/fig_grad_desc}
	\end{center}
	\caption{\textbf{Left:} A contour plot of Gradient Descent minimization of $f(a, b) = \sin(a) + \cos^2(b)$. The blue line shows the search path of the algorithm along the negative gradient while estimating the optimal $\Theta$. \textbf{Right}: Effect of the initial guess for $\Theta$ on the optimization. The function $f(a) = \frac{1}{40}a^2 * \frac{1}{2} \cos(a)$ has multiple local minima in the interval [0, 25]. Only the third guess $a=25$ leads to finding the global minimum, while the other two optimization runs get trapped in local minima.}
	\label{fig:grad_desc}
\end {figure}

\noindent However, a more interesting application of the Gradient Descent algorithm exists in the context of machine learning, where the function to optimize often has fixed parameters in addition to $\Theta$, such as data from a dataset that is related to the function is some way. In such cases, one can consider one of the subtypes of Gradient Descent; either, evaluating the average gradient using all available data samples at once, called \textit{Batch Gradient Descent}, or using single data points only - as an approximation of the entire dataset - to perform the updates. This is called \textit{Stochastic Gradient Descent} (SGD) and is preferable if working with datasets that are too large to fit into memory. Alternatively, one can use the average gradient of a subset of data points - this method is often referred to as \textit{Mini-Batch} Gradient Descent. For example, considering the function

\[ f(\Theta) =  \theta_1 x_1 + \theta_2 x_2\]

\noindent with fixed $x_1$, $x_2$ from some dataset $X$. To minimize this function $f$ with respect to the variable parameters $\Theta$ using the dataset and SGD, the algorithm is modified to only take into consideration a single datapoint $x_i = [x_{i1}, x_{i2}]$ at a time chosen at random from $X$. Then, updating $\Theta$ is done by the formula

\[ \Theta_i = \Theta_{i-1} - \eta \nabla_{f_i} \]

\noindent where $f_i$ is the function $f$ evaluated for $x_i$ and $\Theta_{i-1}$, while the number of steps that it takes to consider all datapoints in the dataset is called an \textit{epoch}. Similarily, to use Mini-Batch SGD with a batch size of $n$, an average over the gradients for $n$ datapoints is used:

\[ \Theta_i = \Theta_{i-1} - \eta \left ( \frac{1}{n} \sum \limits_{i=1}^{n} \nabla_{f_i} \right ) \]

\noindent The price that is paid for the relaxed memory cost of SGD is that the traversion towards the minimum becomes noisy for small $n$, as the algorithm approximates the gradient by using only a part of the dataset. For functions with minima in shallow ``valleys'', SGD starts moving in a wild zig-zag pattern near the optimum, further increasing the number of steps the algorithm takes until covergence. To address these problems, a number of optimizations were proposed.


	\subsubsection {Momentum}
\text{Momentum} is an optimization for the Gradient Descent that adds an additional factor to the update step which is dependent on the steepness of the traversed function in previous updates. Using the analogy of a marble rolling down a slope, momentum refers to the inertia of the marble that allows it to jump over saddle points or small holes (local minima) on the bowl's surface rather than stopping abruptly once such a location is reached. This changes the SGD update formula to

\begin {align}
	& v_i = \gamma v_ {i-1} + \eta \nabla_f\\ 
	& \Theta_i = \Theta_{i-1} - v_i
\end {align}

\noindent where $v$ is the momentum vector with one entry per gradient element and $\gamma$ is a modifier that changes the influence of the momentum on the update.

	\begin {figure}[!ht]
		\begin{center}
			\scalebox{0.75}{\input{img/fig_momentum.pgf}}
		\end{center}
		\caption[]{The effect of momentum on SGD, applied to fitting a line to a noisy two-dimensional dataset with $\eta = 0.1$. \textbf{Left:} Standard SGD. Near the optimum, the algorithm begins zig-zagging and takes many steps to converge. \textbf{Right:} SGD with momentum and $\gamma = 0.85$. Although the algorithm overshoots the minimum a few times, the number of steps until convergence is much smaller.}
		\label{fig:momentum}
	\end {figure}


	\subsubsection {Learning Rate Decay}
Another optimization that is often used is \textit{Learning Rate decay}, also called \textit{Learning Rate Annealing}. It exchanges the fixed learning rate $\eta$ with a learning rate that is a function of the iteration number so that based on how long the optimization has been running, the step size becomes smaller. This has the benefit of being able to take large steps towards the minimum initially, but to allow the search to be narrowed down when near the minimum and avoid endless overshooting. The most popular of these is the step decay, which reduces the learning rate by some factor $\zeta$ every $x$ iterations. Let $i$ be a variable counting the number of iterations. The learning rate in an iteration is then given by

\[ \eta_i = \eta_{i-1} * {\zeta}^{\left \lfloor \frac{i}{x} \right \rfloor} \]

\noindent The SGD update then works as normal, only that after each iteration, $\eta$ is calculated anew given the above formula.




	\subsection{The Perceptron Algorithm}
\label{subsec:perceptron_algo}

A single-layer ANN is a network consisting of just one neuron, and is also called ``Perceptron''. The idea for the Perceptron was first proposed by Rosenblatt \cite{rosenblatt_report, rosenblatt_book}, who used it to perform binary classifications, using what is now called the ``Perceptron algorithm''. Given a linearly separable set of $m$ data points of dimension $n$, one can construct a $(n+1) \times m$ input matrix $x$. The last entry in every column is set to $1$ to simplify the notation later on.

The \textit{Perceptron classifier}, the function that decides which class a given sample belongs to, then takes the form of a linear function

\[ d(x) = h(w^{T}x), \label{eq:perc_class} \]

\noindent where $w$ is a vector of dimension $n+1$ and contains the $n$ \textit{weights} and the \textit{bias} value. The function $h$ is the \textit{activation function}, which is defined as 

\[ h(a) =  \begin{cases}
		+1 \text{ if } a \geq 0 \\
	   	 -1 \text{ otherwise}
	    \end{cases}\]

\noindent and maps its input to one of two possible classes.\\

\noindent The task of \textit{training} is defined as finding values for $w_0, \dots, w_{n+1}$ so that each data point is classified correctly by the resulting line, plane or (hyper-)plane. To this account, there needs to be a $m$-dimensional vector $t$ containing the \textit{labels} of each data point to verify the correctness of $w$. These labels are integers that indicate which class each data point belongs to. In the case of the Perceptron algorithm, the labels are either $1$ for the first class $C_1$ or $-1$ for the second class, $C_2$. If all data points are classified correctly by some $w$, then it holds that

\[ w^T x_n \, t_n > 0 \,\,\forall\, x_n \in x \label{eq:perceptron_label} \]

\noindent Using this definition of correctness, a correctness measure can be formulated, which is known as the ``Perceptron criterion''. Functions of this kind are also called \textit{error functions} or \textit{loss functions}. This particular loss function is given by

\[ E_p(w) = - \sum \limits_{n \in \mathcal{M}} w^T x_n\, t_n, \label{eq:perc_error}\]

\noindent where $\mathcal{M}$ is the set of all data points that were misclassfied, meaning that evaluating equation \ref{eq:perceptron_label} for that data yields a value $< 0$. \cite[pp. 192--194]{bishop_pattern}\\

\noindent The correct Perceptron weights can then be determined by some arbitrary learning algorithm, although here, the Gradient Descent scheme (see algorithm \ref{alg:grad_desc}) is applied to the error function \ref{eq:perc_error}. Either (Batch) Gradient Descent or Stochastic Gradient Descent can be used.

Either way, the gradient of $E_p$ has to be calculated. The Perceptron criterion only depends on the weights $w$, so the gradient vector has the shape

\[ \nabla_E = \left[ \frac{\partial E_p}{\partial w_0}, \frac{\partial E_p}{\partial w_1}, \dots, \frac{\partial E_p}{\partial w_{n+1}} \right ] \,. \]

\noindent Taking the partial derivatives in respect to $w$ leads to

\begin{align}
 	\nabla_E &= \frac{\partial E_p}{\partial w} = \frac{\partial}{\partial w} \left (- \sum \limits_{n \in \mathcal{M}} w^T x_n t_n \right ) \\
 	&= -\sum \limits_{n \in \mathcal{M}} x_n t_n
\end{align}

\noindent If SGD is used, this is reduced to simply

\[ \nabla_E = -x_n t_n\]

\noindent for some previously misclassified point $x_n \in \mathcal{M}$ in the dataset. Hence, the update rule for the Perceptron becomes

\[w_i = w_{i - 1} + x_n t_n * \eta\]

\noindent Using this update rule leads to the training algorithm shown in \ref{alg:perceptron_algorithm}, which is applied to an example dataset in figure \ref{fig:perceptron}.

\begin {figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.6]{img/fig_perceptron}
	\end{center}
	\caption{\textbf{Left}: Separating two datasets with the Perceptron algorithm in $\mathbb{R}^2$. The correct classification boundary takes the form of a line, i.e. $0 = w^{T}x = w_0 x + w_1 y + w_2$. The line pictured is given by $w = (-2.285, -2.908, 5.938)$, the weights after 19 iterations. \textbf{Right}: A not-linearly separable data set, on which the algorithm would never converge, because it is based on finding linear classifiers and thus cannot separate the data correctly.}
	\label{fig:perceptron}
\end {figure}

\begin {algorithm}
	\begin {algorithmic}[1]
		\State $w$ = random
		\While {true}
			\For{$x_i$ in $x$}
				\If{$w^T x_i t_i \leq 0$}
					\State $w$ += $x_i t_i * \eta$
				\EndIf

				\If{$w^T x_i t_i > 0$ for all $x_i$ in $x$}
					\State stop
				\EndIf
			\EndFor
		\EndWhile
	\end{algorithmic}
	\caption{Stochastic Gradient Descent applied to the task of finding the Perceptron weights $w$. $x$ is assumed to be linearly separable.}
	\label{alg:perceptron_algorithm}
\end{algorithm}



		\subsection{Multiple Neurons and Backpropagation}
\label{subsec:mlp_backprop}
While the Perceptron algorithm works fine for linear problems, those are only a small subset of real-world problems (see Figure \ref{fig:perceptron}). The solution to this problem is to add more neurons to the model, which form a network that consists of layers that pass the outputs of neurons in the previous layers through non-linear activation functions and then use the results as inputs. These layers are referred to as the ``input layer'', the initial layer, the ``output layer'', the last layer, and ``hidden layers'', which are all the layers inbetween.

	\subsubsection{Multi-Layer Perceptrons}
The resulting networks are called \textit{Multi-Layer Perceptrons} (MLPs), \textit{Feedforward Artificial Neural Networks}(ANNs), or, in the case of a multitude of layers, \textit{Deep Neural Networks}, and are much more powerful than simple Perceptrons, as they can approximate haphazard continuous functions that operate on closed and bounded subsets of $\mathbb{R}^n$ given that the network possesses at least one hidden layer of neurons. This capability makes using MLPs for learning real-life problems feasible. \cite{universal_approx}\cite{universal_approx2}\\

\noindent Again, each neuron in the network computes a weighted sum of its $i$ inputs, just like in Equation \textbf{\ref{eq:perc_class}}, although indexing has been added to identify neurons and weights in the entire network:

\[ a_j^{(l)} = \sum \limits_{i} w^{(l)}_{ji}\,\, z_i^{(l-1)} + b^{(l)}_{j} \]

\noindent Here, $z_i$ are the input values to the current neuron. Note that these either are the direct inputs $x_i$ to the network, or outputs of previous neurons, depending on where in the network the term is evaluated. $w$ is the matrix of weights, and $w^{(l)}_{ji}$ denotes the weight of the connection from the $i$th neuron in the previous layer $l - 1$ to the $j$th neuron in the  layer $l$. The term $b$ is the bias weight associated with the neuron $j$. The bias can also be made part of the weight matrix by introducing an additional input dimension with a value of $1$, like it was done for the Perceptron algorithm in Section \textbf{\ref{subsec:perceptron_algo}}.

Afterwards, the sum $a^{(l)}_j$ is passed into an activation function $h(\cdot)$ to produce the output, also called activation, of the $j$th neuron in the $l$th layer, $z_j^{(l)}$

\[ z_j^{(l)} = h(a^{(l)}_j) \]

\noindent The activations of the input layer are simply the input values $x_1 \dots x_n$. Expanding the term for the 2-layer network shown in Figure \textbf{\ref{fig:mlp}} gives the following formula for the $k$th network output $out_k$:

\[ out_k = \sigma \left ( \sum \limits_{j=0}^{4} w^{(2)}_{kj}\,\, h \left ( \sum \limits_{i=0}^{3} w^{(1)}_{ji} x_i \right ) \right ) \label{eq:mlp_out} \]

\noindent Here, \textbf{$\sigma$} is an output function that is applied in the last layer of the network, and is dependent on the task the network is supposed to fulfill: For regression problems, i.e. fitting a function to a set of data and then predicting new values using that function, the identity function can be used. For binary classification problems, instead of using a step function like in the original Perceptron, one instead uses the \textit{Sigmoid} function and interprets the output as

\[ \sigma_{sig}(x_i) = \begin{cases}
				x_i \in C_1 \text{ if } \frac{1}{1 + e^{-x_i}} \geq 0.5 \\
				x_i \in C_2 \text{ else }\\
			 \end{cases}
\]

\noindent Also, the choice of the activation function $h(w, x)$ is different from the Perceptron: It is not chosen to be the Heaviside step function, but instead, to be a non-linear, continuous and differentiable function, such as the general Sigmoid function and its derivative:

\begin {align}
	 h(x) &= \frac{1}{1 + e^{-x}}\\
	\frac{\partial h}{\partial x} &= h(x)(1 - h(x))
\end {align}

\afterpage{
	\begin {figure}[!ht]
		\begin{center}
			\def\svgwidth{0.8\columnwidth}
			\input{img/mlp.pdf_tex}
		\end{center}
		\caption[]{An MLP with one hidden layer, forming an acyclic directed graph, where connections are color-coded per layer by the receiving neuron. The layer count $L$ starts at the layer after the input layer\protect\footnotemark. Therefore $L = 2$, with three inputs $x_i$ and neuron counts $D^{(1)} = 4$ and $D^{(2)} = 2$. The variables $b^{(1)}$ and $b^{(2)}$ refer to the biases for the neurons in the layers $(1)$ or $(2)$, respectively. Each neuron has its own, changeable bias value, but for the sake of cleanness, the connections from the biases to each neuron of their corresponding layer are not shown.}
		\label{fig:mlp}
	\end {figure}

	\footnotetext{There is no consensus on which method of counting is the most appropriate. Sometimes, only the number of hidden layers or even all layers are counted. \cite[pp. 229]{bishop_pattern}}
}

\noindent The goal of training an MLP is - as previously for the Perceptron - finding specific weights so that all inputs are classified correctly, or at least, so that the network makes only few mistakes. Therefore, a loss function is added to the end of the network, measuring the correctness of the output. Instead of the Perceptron criterion \ref{eq:perc_error}, a more general loss function is employed which compares the difference between the output $\sigma(x)$ of the network for a sample $x$ and the preferred output for $x$, the \textit{label} or \textit{ground truth} of $x$. Typically, in binary classification problems, Sigmoid outputs are used together with the \textit{Cross-Entropy} loss function, which is defined as 

\[ CE(w) =  -t \, \ln (out_x) + (1 - t) \ln (1 - out_x).\footnote{The error is only given as a function of $w$ because both the input $x$ and its correct label $t$ are fixed in the particular case presented here.}  \]

\noindent For multi-class classifications, an adaption of the Cross-Entropy loss can be used, which is further described in Section \ref{subsec:cross_ent}. \cite[pp. 232-236]{bishop_pattern}\\

	\subsubsection{The Backpropagation algorithm}
Training a network with multiple layers is harder than training a single Perceptron because the influence of each of the weights in any layer of the network on the error function has to be computed when using gradient-based optimization methods, while the output of the network no longer is the result of a single function, but instead, a composition of an usually large number of functions (see Equation \textbf{\ref{eq:mlp_out}}). Although it is possible to manually calculate the partial derivatives for the entire network, this becomes more and more cumbersome as the network grows in size.

Luckily, there exists an iterative algorithm called \textit{Backpropagation} \cite{backprop} that finds the gradient of the loss with respect to each weight so that an optimization method can be applied to change every weight. The algorithm makes use of the fact that each neuron can be viewed (almost) independently from the rest of the network. Remembering that the analytic way of obtaining the derivative of a composition of functions 

\[ f \circ g = f(g(x)) \]

\noindent with respect to some $x$ is to apply the chain rule

\[ \frac{\partial f}{\partial x} =\frac{\partial f}{\partial g} \frac{\partial g}{\partial x} \, , \]

\noindent one can see that this concept can also be applied to neurons in a network, because using the output value of a neuron in a previous layer as the input for a node is nothing else than function composition.\\

\noindent Backpropagation thus starts with a \textit{forward pass}, which means that an input vector $x$ is passed through the network and the corresponding sums $a_j$ and activations $z_j$ are calculated and stored. Comparing the final outputs to the ground truth in the loss layer, the network is then traversed in the opposite direction in what is called a \textit{backward pass}, aiming to obtain the influence of each weight in each layer on the final loss, that is, the gradient of the loss function with respect to the weights. After this first phase, the partial derivatives for each weight are known, and the weights are updated in a second phase, in which an optimization algorithm like Gradient Descent is used as described in Section \textbf{\ref{subsec:grad_desc}}.\\

\noindent The central idea of the backward pass is that when traversing the network from the output layer to the input layer, the derivatives of the loss function with respect to the weights of the layer closest to the output of the network depend on outputs of the neurons in the layers that precede that layer. This relationship recursively continues backwards throughout the network and is described by the formula

\begin {align}
	\frac{\partial E}{\partial w_{ji}} &= \frac{\partial E}{\partial a_j} \frac{\partial a_j}{\partial w_{ji}} \label{eq:backprop_recur}
\end {align}

\noindent where $E$ is the loss function of the network.\footnote{Because the formula concerns a particular weight $w_{ji}^{(l)}$ in a particular layer $l$, the $l$-superscript is omitted for convenience.} According to the chain rule, the influence of the weight $w_{ji}$ on the loss is equal to the influence of the weighted sum $a_j$ on the loss function multiplied by the influence of the weight $w_{ji}$ on $a_j$. Taking the partial derivative of the weighted sum $a_j$ with respect to the weight $w_{ji}$ means that

\begin {align}
	\frac{\partial a_j}{\partial w_{ji}} &= \frac{\partial}{\partial w_{ji}} \bigg ( \sum \limits_{k} w_{jk} z_k \bigg )
\end {align}

\noindent where $k$ is the number of neurons in the neurons in the previous layer $l-1$. Because when $k = i$, it holds that

\[ \frac{\partial}{\partial w_{ji}} \bigg ( w_{jk} z_k \bigg ) = z_i \]

\noindent and for $k \neq i$ it holds that 

\[ \frac{\partial}{\partial w_{ji}} \bigg ( w_{jk} z_k \bigg ) = 0 \]

\noindent The derivative of the entire sum is therefore

\[ \frac{\partial}{\partial w_{ji}} \bigg ( \sum \limits_{k} w_{jk} z_k \bigg ) = z_i \]

\noindent In literature, the term $\frac{\partial E}{\partial a_j}$ is often denoted as $\delta_j^{(l)}$, the ``error'' of the neuron $j$ in layer $l$. Using this notation, Equation \textbf{\ref{eq:backprop_recur}} can then be written as

\[ \frac{\partial E}{\partial w_{ji}} = \delta_j^{(l)} z_i \]

\noindent Since $z_i$, the activation from the input end of the connection that is multiplied with $w_{ji}$, is already known for all neurons due to being stored during the forward pass, the only thing that needs to be computed to figure out the loss gradient are the $\delta$-values. The computation of $\delta_j^{(l)}$ depends on the position of the neuron in the network:

\[ \delta_j^{(l)} =  \begin{cases}
				\frac{\partial E}{\partial a_j} \text{ if neuron } j^{(l)} \text{ is an output neuron} \\
			           h'(a_j) \sum \limits_{k} w_{kj} \, \delta_k^{(l+1)} \text{ otherwise}
			     \end{cases}
\]

\noindent In the case that $j$ is an output neuron, the partial derivative is calculated directly. Otherwise, $\delta_j$ depends on the $\delta$-values in deeper layers (see Figure \textbf{\ref{fig:backprop}}). The definition of $\delta$ for hidden neurons is also the reason why a differentiable function must be chosen as the activation $h(\cdot)$. The Backpropagation algorithm continues backwards through the network until the entire weight gradient is known and a single (Stochastic) Gradient Descent step can be taken towards the loss function minimum.

\noindent The main advantage of the algorithm is that it scales linearly with the number of weights, i.e. the algorithm is in $\mathcal{O}(w)$. Furthermore, the fact that all $\delta^{(l)}$-values of a layer have to be calculated to calculate all $\delta^{(l-1)}$-values in the layer that precedes it naturally leads to vectorizing the algorithm layerwise in efficient implementations. \cite{nielsen_book, ng_lecture}\cite[pp. 241-245]{bishop_pattern}


\afterpage{
	\begin {figure}[!ht]
		\begin{center}
			\def\svgwidth{0.8\columnwidth}
			\input{img/backprop.pdf_tex}
		\end{center}
		\caption[]{Backpropagation calculations, applied to a hidden layer $l$.\protect\footnotemark$\,$ Operations during the forward pass are shown in blue, while operations during the backward pass are orange. The central neuron is the only neuron in the layer $l$, while there are two neurons each in the hidden layers $l-1$ and $l+1$. The forward pass calculates the weighted sum $a_1^{(l)}$ and the activation $h(a_1^{(l)})$. Once the backward pass reaches the neuron again, it calculates $\delta_1^{(l)}$ and the partial derivatives of the weights of layer $l$, that is, the weights of the connections that flow into the central neuron in the forward pass, using the backpropagated value $\delta_k^{(l+1)}$.}
		\label{fig:backprop}
	\end {figure}

	\footnotetext{This example is based on a visualization in \cite{karpathy_lecture}.}
}

\noindent Depending on how deep the networks are, i.e. how many hidden layers there are and how many neurons there are in these hidden layers, neural networks can learn very complex classification boundaries. The role of the activation function is akin to the kernel principle of State Vector Machines (SVMs), since it maps inputs to a representation in which they are linearly separable by a hyperplane (see Figure \textbf{\ref{fig:mlp_trick}}).\\

\begin {figure}[!ht]
		\begin{center}
			\textbf{TODO: uncomment in code}
			%\input{img/fig_mlp_trick.pgf}
		\end{center}
		\caption[]{\textbf{Left:} The problem from \ref{fig:perceptron} revisited. An 2-layer MLP with 2 input neurons, 5 hidden neurons and 2 output neurons was trained on the dataset for 10000 iterations using Backpropagation and Gradient Descent and classifies it correctly using a nonlinear classification boundary. \textbf{Right:} Visualization of the transformed data with $(x, y) \Rightarrow (x, y, out(x, y))$. The transformed dataset is then linearly separable by a plane.}
		\label{fig:mlp_trick}
	\end {figure}

\clearpage % workaround for \label in afterpage environment, see https://tex.stackexchange.com/questions/200585/label-is-undefined-when-used-in-afterpage