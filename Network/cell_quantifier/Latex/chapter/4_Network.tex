\chapter {Network Architecture}


	\section {U-Net}
	Since the task of image segmentation is popular, a number of network propositions have been made. Among them is the \textit{U-Net}. The U-Net is a a Convolution Neural Network (CNN) architecture proposed by Ronneberger et al.\cite{unet} which aims to produce segmentation maps for images of cells. The U-Net was implemented using Caffe\cite{caffe}, a CNN library developed by the University of California, Berkeley that allows basic building blocks to be put together to form a network. In the following sections, the principles behind CNNs and the operations in each of the network's layers are detailed.

	% Image of U-Net here

	\section {Network Components and Parameters}
	
		\subsection{Convolutional Layers}
CNNs are a special type of ANNs/MLPs that was popularized by LeCun \cite{lecun98} and, more recently, the successes of Krizhevsky \cite{krizhevsky2012} in the domain of image classification, although CNNs have been applied to other problems such as sound analysis as well. The rationale for their creation was that using fully-connected networks like the ones presented in section \ref{subsec:mlp_backprop} relate all elements of an input vector to every other element, which is generally a good thing to do - however, it ignores the relationship between parts of the input that is given in some datasets and doesn't scale well for very large datasets because of the large number of weights needed. Especially in image data, pixels that are close to each other by some metric such as the Euclidean distance are much more likely to be related, while pixels that lie on the other side of the image hardly are relevant in the context of operations like pixel classification. Consequently, CNNs encode this spatial relationship by using convolution operations (see section \ref{sec:canny}). Concretely, a convolution layer in a CNN typically consists of three sub-components:\\

\noindent First, the data is convoluted by applying one or multiple kernels (also called filter) to all input values and their local neighborhoods.\footnote{Although the convolution in section \ref{sec:canny} describes out-of-bounds handling, the U-Net architecture used in this thesis uses only the naturally valid part of the convolution, i.e. if a $3 \times 3$ filter is used, a 1-value border of the data is lost because the local neighborhood would be partly out of bounds for these border values.} Kernels typically have a size of $k \times k \times d$, where $k$ is the filter size and $d$ is the depth of the dataset, e.g. in an RGB image dataset, the depth is 3 because there are 3 color channels. The result of these convolutions are as many outputs, called \textit{feature maps}, as there are filters in the layer. When viewed as a graph, this means that the hidden neurons represent the results of each convolution step, while all hidden neurons of the same filter share the same weights.

Effectively, this means that the same kind of pattern is searched in the entire dataset, resulting in \textit{translation invariance}, i.e. it doesn't matter where the pattern is because it is still the same pattern at every position in the data.\\

\noindent Then, the feature maps are passed through an non-linear activation function $h(\cdot)$, just like in normal MLPs, to generate activation maps. The U-Net uses the \textit{Rectified Linear Unit} (ReLU) function

\[ ReLU(x) = \max(0, x) \]

\noindent as its activation (see also section \ref{}).\\

\noindent Finally, these activation maps are exposed to a \textit{Pooling} operation such as Maximum Pooling. Maximum pooling is a downsampling method that reduces the complexity of the network by replacing groups of values that are apart by a certain stride in the activation map with the maximum value within that group, resulting in a condensed activation map, while performing this calculation for each depth slice of the data independently. Intuitively, some of the information about where a pattern was detected by the filter is given up in order to make the network easier to train, although the relative positions between patterns are kept intact.\\

\begin {figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.75]{img/fig_convnet}
	\end{center}
	\caption{A Convolutional Neural Network with multiple filters. \textbf{a):} Input layer. The input size is $6 \times 6 \times 3$, while the kernel size is $3 \times 3 \times 3$. Neurons with valid convolution neighborhoods shown in gray. \textbf{b):} Feature maps with size $4 \times 4 \times 3$ that are the result of applying convolution filters to the input, which are then passed into the activation function $h$ to create activation maps. \textbf{c):} Results of a Maximum Pooling operation with sizes $2 \times 2$ and stride $2$ applied to the activation maps. The colors indicate the downsampled position of the maximum activation in the map. \textbf{d):} Fully-connected output layer that calculates the network outputs via applying the output function $\sigma$. The max-pooled values have been rearranged into a column for clarity, and connections are color-coded by the receiving neuron.}
	\label{fig:convnet}
\end {figure}

\noindent The training of such a network then proceeds like in an MLP, only that the weights that the network has to learn by backpropating the loss through the network are the weights of fully-connected layer as well as the filter weights. \cite{deeplearning_book}

		\subsection {Crop Layers}
		\subsection{Softmax}

		\subsection {Dropout}
Dropout layers implement the \textit{Dropout} technique proposed by Srivastava et. al \cite{dropout}. Dropout ``disables'' neurons it is applied to during training with a certain chance according to a user-defined Bernoulli probability. These disabled neurons are not trained on the particular training sample - an effect one could view as training only a sub-network of the original neural network. By applying Dropout repeatedly and scaling the weights by the Dropout probability $p$ during testing, one gains an approximation of averaging the outputs of all possible $2^n$ sub-networks, where $n$ is the number of neurons in the network. This lowers the amount of overfitting because neurons learn not to ``rely'' on other neurons which otherwise would lead to specialized co-adaptions that usually hurt the generalization ability of the network.

The \textit{U-Net} uses $p = 0.5$ in its Dropout layers.



		\subsection {Batch Normalization}




	\section {Loss Functions}

		\subsection{Softmax}
\label{subsec:softmax}

The loss layer that was proposed originally for the U-Net architecture is a \textit{Softmax Loss} layer. The name \textit{Softmax Loss} can lead to misunderstandings, because the layer's internal implementation actually consists of two steps:

First, the input $z$ of the layer is transformed by the softmax function

\[\sigma(z) = \frac{e^{z}}{\sum_{k=0}^{K} e^{z_k}},\]

\noindent where $e$ is the exponential function and $z$ has a dimension of $1 \times K$. The softmax function acts as a normalizer that ``squashes'' the values of its input vector into the range $[0, 1]$ with respect to the differences between the vector values. For example, let $z = [-2.432, 1.832, 0.299]$. Then $\sigma(z) = [0.011, 0.813, 0.176]$ is the softmax output for $z$.

These processed values can be interpreted as probabilities much more easily than the raw scores the loss layer receives, which in turns leads to the second step, in which the actual loss computation is done. The actual loss function the layer uses is the \textit{Cross-Entropy Loss} that compares a true probability distribution $p$ to a model probability distribution $q$. For a vector of calculated softmax probabilities $z_i$, it is defined as follows:

\[CE(p, q) = -\sum \limits_{i = 0}^{K} p(i) \log q(i)\]

\noindent Conveniently, the softmax function forces all input values to be positive, allowing the logarithm in the loss to be used. Even though $\log (0)$ isn't defined, the softmax function should never produce values that are exactly equal to zero or one. \textbf{TODO: WHY?} 

However, the above definition of the softmax function can become numerically unstable when the exponentiation yields large values because dividing by large numbers could potentially produce zero values. To prevent this, the softmax can be made robust by defining it as

\[z' = z - \max \limits_{i = 0}(0, z_i)\]
\[\sigma(z') = \frac{e^{z'}}{\sum_{k=0}^{K} e^{z'_{{k}}}}\]

\noindent Since each pixel of the input image belongs to one class only, this means that for a given pixel, the probability of its ground truth label is $1.0$, while all others are $0.0$. If, without loss of generality, the third class is assumed to be the ground-truth class, $p$ will be $[0.0, 0.0, 1.0, 0.0]$.  Hence, only the predicted probability of the true third class influences the loss, and all the other summands can be omitted, yielding the condensed general formula

\[CE'(q) = - \log(q_{true}),\]

\noindent where $q_{true}$ denotes the predicted probability for the true class. The loss of the entire image is then calculated as the sum of the losses of all pixels divided by the number of pixels.

		\subsection{Cross-Entropy Loss}
\label{subsec:cross_ent}

		\subsection{Weighted Cross-Entropy Loss}

Using a softmax loss function assumes that all pixels in each training sample should carry the same weight in the backpropagation process. As long as the distribution of classification classes is balanced, this works well, but if it is not, classes with lower probability are trained much more slowly.

To combat this problem, Ronneberger et. al\cite{unet} introduce a pixelwise weight map for each training sample whose values depend on the ground truth class and the position of each pixel. For example, the softmax loss is changed to

\[CE(p, q) = -\sum \limits_{i = 0}^{K} p(i) \log q(i) \cdot w(i),\]

\noindent where $w(i)$ is the weight for pixel $i$.

 The weight for each pixel is calculated as follows:

\[ w(i) = w_c(i) + w_0 \cdot \exp \left (- \frac{(d_1(i) + d_2(i))^2}{2\sigma^2} \right ), \]

\noindent where $w_c$ is the class-specific weight for the pixel's ground truth class, $d_1$ and $d_2$ are the euclidean distances to the nearest and second-nearest non-background pixel and $w_0$ and $\sigma$ are additional modifier terms. 

		\subsection{F-Measure}

An alternative to solving the problem of unbalanced classes in the training data is using the F-Measure, also called DICE similarity or $F_1$-Score, as a loss function instead. The F-Measure is a statistical similarity function which has been shown to perform well on unbalanced data\cite{fmeasure1}\cite{fmeasure2}\cite{fmeasure3} defined as

\[F_1 = 2 \cdot \frac{PR \cdot RC}{PR + RC},\]

\noindent where PR denotes the \textit{Precision} and RC denotes the \textit{Recall} over the training sample in comparison to its ground truth. In turn, \textit{Precision} is defined as

\[PR = \frac{TP}{TP + FP}\]

\noindent and \textit{Recall} is defined as

\[RC = \frac{TP}{TP + FN},\]

\noindent where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives. \textbf{TODO}: Wie kann man FP etc auf ein Mehrklassen-Problem anwenden?
