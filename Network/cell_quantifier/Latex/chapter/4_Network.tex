\chapter {Network Architecture}

Since the task of image segmentation is a popular one, a number of architecture propositions have been made that aim to produce segmentations. Among them is the \textit{U-Net}, which is the basis of the network used in this thesis.  In the following sections, the principles behind so-called Convolutional Neural Networks, the differences between them and normal MLPs and the operations in such networks are detailed.


	\section{Convolutional Neural Networks}
\label{sec:CNN}
\textit{Convolutional Neural Networks} (CNNs) are a special subtype of ANNs/MLPs that was popularized by LeCun \cite{lecun98} and, more recently, the successes of Krizhevsky \cite{krizhevsky2012} in the domain of image classification, although CNNs have been applied to other problems such as sound analysis as well. The rationale for their creation was that using fully-connected networks like the ones presented in section \ref{subsec:mlp_backprop} relate all elements of an input vector to every other element, which is generally a good thing to do - however, it ignores the relationship between parts of the input that is given in some datasets and doesn't scale well for very large datasets because of the large number of weights needed. Especially in image data, pixels that are close to each other by some metric such as the Euclidean distance are much more likely to be related, while pixels that lie on the other side of the image hardly are relevant in the context of operations like pixel classification. Consequently, CNNs encode this spatial relationship by using convolution operations. Concretely, a convolution layer in a CNN typically consists of three sub-components:\\

\noindent First, the data is convoluted by applying one or multiple kernels (also called filter) to all input values and their local neighborhoods.\footnote{Although the convolution in section \ref{sec:canny} describes out-of-bounds handling, the architecture used in this thesis uses only the naturally valid part of the convolution, i.e. if a $3 \times 3 \times d$ filter is used, a 1-value border of the data is lost because the local neighborhood would be partly out of bounds for these border values.} Kernels typically have a size of $k \times k \times d$, where $k$ is the filter size and $d$ is the depth of the dataset, e.g. in an RGB image dataset, the depth is 3 because there are 3 color channels. The result of these convolutions are as many outputs, called \textit{feature maps}, as there are filters in the layer. When viewed as a graph, this means that the hidden neurons represent the results of each convolution step, while all hidden neurons of the same filter share the same weights.

Effectively, this means that the same kind of pattern is searched in the entire dataset, resulting in \textit{translation invariance}, i.e. it doesn't matter where the pattern is because it is still the same pattern at every position in the data.

Then, the feature maps are passed through an non-linear activation function $h(\cdot)$, just like in normal MLPs, to generate activation maps.

\noindent Finally, these activation maps are exposed to a pooling operation such as \textit{Maximum Pooling}. Maximum pooling is a downsampling method that reduces the complexity of the network by replacing groups of values that are apart by a certain stride in the activation map with the maximum value within that group, resulting in a condensed activation map, while performing this calculation for each depth slice of the data independently. Intuitively, some of the information about where a pattern was detected by a filter while creating the feature map is given up in order to make the network easier to train, although the relative positions between patterns are kept intact. \cite[pp. 330-345]{deeplearning_book}\\

\noindent The training of such a network then proceeds like in an MLP, only that the weights that the network has to learn by backpropating the loss through the network are the weights of fully-connected layer as well as the filter weights.\\

\noindent See figure \ref{fig:convnet} for an example of a simple CNN architecture.

\begin {figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.75]{img/fig_convnet}
	\end{center}
	\caption{A Convolutional Neural Network with multiple filters. For simplicity, most neuron connections aren't shown explicitly. \textbf{a):} Input layer. The input size is $6 \times 6 \times 3$, while the kernel size is $3 \times 3 \times 3$. Neurons with valid convolution neighborhoods shown in gray. \textbf{b):} Feature maps with size $4 \times 4 \times 3$ that are the result of applying convolution filters to the input, which are then passed into the activation function $h$ to create activation maps. \textbf{c):} Results of a Maximum Pooling operation with sizes $2 \times 2$ and stride $2$ applied to the activation maps. The colors indicate the downsampled position of the maximum activation in a pooling region. \textbf{d):} Fully-connected output layer that calculates the network outputs via applying the output function $\sigma$. The max-pooled values have been rearranged into a column for clarity, and connections are color-coded by the receiving neuron.}
	\label{fig:convnet}
\end {figure}


	\section {Activation Functions}
Even though previously, the Sigmoid function was given as an example of a suitable activation function for MLPs, recent research has shown that better alternatives exist, and it is hardly used anymore. This is mainly because Sigmoids contribute to the problem of ``vanishing gradients'' during Backpropagation which causes earlier layers in a deep network to train much more slowly than layers near the output layer. ``Vanishing gradients'' happen when the magnitude of the gradient is close to zero, which in turn causes the training to change the network weights in affected layers by only a tiny amount, effectively stalling the training process.

The Sigmoid function is especially prone to this because its derivative outputs values in the range $\left [0, \frac{1}{4} \right ]$ only. Combined with weights initialized to small values, e.g. Gaussian-distributed weights with zero mean and unit variance, the repeated application of the chain rule results in many terms in the range $\left [0, 1\right ]$ being multiplied with each other, leading to very small gradients the closer the algorithm gets to the input layer.\cite{glorot} Activation functions for which

\[ \lim \limits_{x \rightarrow -\infty} \frac{\partial}{\partial x} h(x) = 0 \,\,\,\, \land \,\,\,\,  \lim \limits_{x \rightarrow +\infty} \frac{\partial}{\partial x} h(x) = 0 \]

\noindent is true are called \textit{saturating} activation functions and possess the undesirable property of leading to vanishing gradients.\\ 
 \textbf{TODO: zigzagging because not zero-centered, \url{http://cs231n.github.io/neural-networks-1/} + tanh}\\

\noindent A newer, popular alternative to the Sigmoid function is the \textit{Rectified Linear Unit} function (ReLU). It is defined as the piecewise function

\[  ReLU(x) = \begin{cases}
			0 \text{ if } x \leq 0\\
			x \text{ else}
		 \end{cases}
\] 

\noindent or simply

\[ ReLU(x) = \max(0, x) \]

\noindent with its derivative given by 

\[ \frac{\partial}{\partial x} ReLU(x) = \begin{cases}
							1 \text { if } x > 0\\
							0 \text { if } x < 0\\
							 \text{undefined if } x = 0
						        \end{cases},
\]

\noindent although in implementations, the derivative for zero inputs strays from the mathematical definition and is altered to $\frac{\partial ReLU}{\partial x}\rvert_{x=0} = 0$.\footnote{One example is the ``backward\_cpu'' implementation of the Caffe framework's ReLU layer (\url{https://github.com/BVLC/caffe/blob/master/src/caffe/layers/relu_layer.cpp}).}

ReLUs do not saturate and are cheap to compute compared to activation functions containing exponential functions, although when using large learning rates and large weights, the gradients can ``explode'' during Backpropagation, which is the opposite of the ``vanishing gradient'' problem. To deal with this, \textit{gradient clipping} can be performed. A popular form of gradient clipping is L2 norm gradient regularization, given by

\[ \hat{g} = \frac{g\,t}{||g||} \cite{l2clipping} \]

\noindent where $g$ is the gradient vector, $||g||$ is its L2 norm and $\hat{g}$ is the regularized gradient. This regularization is done whenever $||g||$ is larger than some threshold $t$.\\

\noindent Despite this, ReLUs still suffer from the problem that they can ``die'' when the network weights are updated in a way that makes them output activation values of zero forever - for example, when a large negative bias term turns all inputs negative and thus resulting in zero activations. Once a ReLU is dead, the gradient flowing through it will also be zero, and the weights for its neuron won't be updated anymore.

An attempt at fixing this issue is relaxing the strict below-zero activation so it outputs a downscaled version of the input instead. This principle is called \textit{Leaky ReLU} \cite{lrelu} and formally changes the ReLU definition to 

\[  LReLU(x) = \begin{cases}
			\alpha x \text{ if } x \leq 0\\
			x \text{ else}
		 \end{cases}
\]

\noindent for some small scaling parameter $0 < \alpha < 1$. Consequently, its (zero input-altered) derivative becomes

\[ \frac{\partial}{\partial x} LReLU(x) = \begin{cases}
							1 \text { if } x > 0\\
							\alpha \text{ else}
						        \end{cases}
\]

\noindent When the leak parameter $\alpha$ is made an additional weight to be trained by Backpropagation, the resulting activation function is called \textit{Parametric ReLU} (PReLU). Another alternative is the \textit{Exponential ReLU} (ELU) \cite{elu} which aims to combine leakiness with activations close to zero mean to speed up learning. It is defined by

\begin {align}
	ELU(x) &= \begin{cases}
			x \text { if } x \geq 0\\
			\alpha(e^x - 1) \text{ else}
		     \end{cases}\\
	\frac{\partial}{\partial x} ELU(x) &= \begin{cases}
								1 \text { if } x \geq 0\\
								ELU(x) + \alpha \text{ else}
		    				     \end{cases},
\end {align}

\noindent where $\alpha$ is a learnable parameter.


\begin {figure}[!ht]
	\begin{center}
	\begin {subfigure}[{position=b}]{0.3\linewidth}
		\scalebox{0.60}{\input{img/fig_sigmoid.pgf}}
		\caption{}
	\end {subfigure}
		\begin {subfigure}[{position=b}]{0.3\linewidth}
		\scalebox{0.60}{\input{img/fig_tanh.pgf}}
		\caption{}
	\end {subfigure}
	\end{center}

	\begin {center}
	\begin {subfigure}[{position=b}]{0.3\linewidth}
		\scalebox{0.60}{\input{img/fig_relu.pgf}}
		\caption{}
	\end {subfigure}
	\begin {subfigure}[{position=b}]{0.3\linewidth}
		\scalebox{0.60}{\input{img/fig_lrelu.pgf}}
		\caption{}
	\end {subfigure}
	\begin {subfigure}[{position=b}]{0.3\linewidth}
		\scalebox{0.60}{\input{img/fig_elu.pgf}}
		\caption{}
	\end {subfigure}
	\end{center}

		\caption[]{Multiple activation functions (blue) and their derivatives (orange). \textbf{a):} Sigmoid. \textbf{b):} Tanh. \textbf{c):} ReLU. \textbf{d):} LReLU with $\alpha = 0.1$. \textbf{e):} ELU with $\alpha = 0.5$.}
		\label{fig:activation_functions}

\end {figure}



	\section{Weight Initialization}

	\textbf{TODO: Glorot, He Inits, referenz auf tanh und batchnorm kombi? ReLU ELU etc}\\
	symmetry breaking


	\section {Output- and Loss Functions}
In the following, output and loss functions for a CNN as well as the loss function derivatives for Backpropagation are described. Alternatives exist, but the mentioned functions were chosen with the goal of pixel classification in mind and have been found to work well for this kind of problem.


	\subsection{Softmax}
\label{subsec:softmax}

\textit{Softmax} is a vector function that can be used as an output function to transform network scores. The Softmax function for some input vector $z$ is defined as

\[y = Softmax(z) = \frac{e^{z}}{\sum_{c=0}^{C} e^{z_c}},\]

\noindent where $e$ is the exponential function and $z$ has a dimension of $1 \times C$, where in this case, $C$ is the number of classes to be predicted by the network. The Softmax function acts as a normalizer that ``squashes'' the values of its input vector into the range $[0, 1]$ with respect to the differences between the vector values, so that the sum of all values is $1$. For example, let 

\[ z = [-2.432, 1.832, 0.299] \]

\noindent Then 

\[ y = [0.011, 0.813, 0.176] \]

\noindent is the Softmax output for $z$. These processed values can be interpreted as probabilities as opposed to the raw output.



However, the above definition of the Softmax function can become numerically unstable when the exponentiation yields large values because dividing by large numbers could potentially produce zero values due to insufficient floating-point precision. To prevent this, the calculation can be made robust by defining it as

\begin {align}
	z' &= z - \max \limits_{i}(0, z_i)\\
	y &= \frac{e^{c'}}{\sum_{c=0}^{C} e^{z'_{{c}}}}
\end {align}

\noindent which avoids dividing by large numbers but still gives the same result. \cite[p. 81]{deeplearning_book}

The partial derivatives of the Softmax output vector $y$ with respect to the input vector $z$ form a Jacobian matrix and can be notated concisely as

\begin {align}
\label{eq:softmax_deriv}
	\frac{\partial y_i}{\partial z_i} = \bigg \{ \begin {array}{ll}
								y_i(1 - y_i)& \text{ if } i = j \\
								-y_i y_j& \text{ if } i \neq j
							\end{array}
	\,\,\cite[\text{ p. 209}]{bishop_pattern}
\end {align}


		\subsection{Cross-Entropy Loss}
\label{subsec:cross_ent}

A common loss layer in MLPs that perform multi-class classification is the \textit{Cross-Entropy Loss} layer. In many frameworks, the layer's implementation actually consists of two steps: First, the input $z$ of the layer is transformed by the Softmax function, and is then used as an argument for the loss function.

The \textit{Cross-Entropy Loss} function compares a true probability distribution $p$ to a model probability distribution $q$. For a vector of calculated Softmax probabilities $z_i$, it is defined as follows:

\[CE(p, q) = -\sum \limits_{i = 0}^{K} p(i) \ln q(i)\]

\noindent In a pixel classification task such as cell classification, each pixel belongs to one of many mutually exclusive classes. Therefore, the true probability of the $i$th pixel's ground truth label is $1.0$, while all others are $0.0$. For example, if the third class is assumed to be the ground-truth class, the vector of $p$ values for some pixel $i$ will be $[0.0,\, 0.0,\, 1.0,\, 0.0]$. The loss for an entire input image is then calculated as the sum of the losses of all pixels divided by the number of pixels.

\noindent To use the Cross-Entropy loss in combination with a Softmax input during Backpropagation, $\delta$ is needed. As the Cross-Entropy performs the output calculation for the network and therefore directly influences the loss, $\delta$ is defined as simply

\begin {align} \delta &= \frac{\partial CE}{\partial a_j}\\  
			    &= y_i - t_i \hspace{2em}\cite[\text{ p. 209}]{bishop_pattern}
\end {align}

\noindent where $i$ is the index of the Softmax probability for a class $C_i$ and $t_i$ is the ground truth probability. For classification with mutually exclusive classes, i.e. $t_i = 1.0$ when $C_i$ is the correct class and $t_i = 0.0$ otherwise, this means that $\delta$ can be calculated by just subtracting $1$ from the predicted Softmax probability of the true class.


		\subsection{Weighted Cross-Entropy Loss}

Using a Cross-Entropy loss function for pixel classification assumes that all pixels in each input image have the same weight in the Backpropagation process, and therefore, it assumes that the distribution of classes within the samples is balanced. As long as the distribution of classification classes is balanced, this works well, but many datasets do not exhibit this property. Obviously, the assumption is also not true for microscopy images of a few moving on dark background, as most of the image is taken up by the background class, and thus, the performance of training using a normal Cross-Entropy Loss was expected to be poor.

To combat this problem, one can introduce a pixelwise weight map for each training sample whose values depend on the ground truth class and the position of each pixel. Therefore, the Cross-Entropy loss formula changes to

\[ CE(p, q) = -\sum \limits_{i = 0}^{K} w(i) \bigg ( p(i) \ln q(i) \bigg ) \]

\noindent where $w(i)$ is a weight map for each pixel $i$.

One way to construct such a weight map is given in \cite{unet}, using the weight equation

\[ w(i) = w_c(i) + w_0 \left ( \exp \left (- \frac{(d_1(i) + d_2(i))^2}{2\sigma^2} \right ) \right ), \]

\noindent where $w_c(i)$ is the class-specific weight for the $i$th pixel's ground truth class, $d_1$ and $d_2$ are the Euclidean distances to the nearest and second-nearest non-background pixel and $w_0$ and $\sigma$ are additional modifier terms. The class-specific weights $w_c(i)$ can either be deduced by counting all pixels of all classes in all sample labels to perform \textit{average weighting}, in which all sample use the same class weights, or to do the same for each sample, resulting in \textit{individual weighting}. The class weights are then calculated as their inverse probability, i.e.

\[ w_c(i) = 1 - \frac{\#c}{\#all}, \]

\noindent where $\#c$ is the number of pixels that belong to a class $C$ and $\#all$ is the number of pixels in the sample (or in all samples, depending on the weighting mode). This way, classes that occur less frequently are given a harsher penalty when misclassified, making them more important during training. The calculation of $\delta$ for Backpropagation is then defined as follows:\footnote{The usual index $j$ for $\delta$ was omitted here and in the derivative of the Softmax because there is only one neuron in the loss layer anyway. Here, $j$ instead refers to the $j$th element of the vector $a$.} \\


\begin {align}
	\delta &= \frac{\partial CE}{\partial a_j} \\
		&= \frac{\partial}{\partial a_j} \left ( w \left ( - \sum \limits_{j=1}^{C} t_j \ln(y_j) \right ) \right ) \\ \intertext{where $w$ refers to the weight constant, $t_j$ is the ground truth probability and $y_j$ is the Softmax probability for the current pixel. Then}
		&= w \left ( - \sum \limits_{j=1}^{C} t_j \frac{\partial \ln(y_j)}{\partial a_j} \right ) \\
		&= w \left (- \sum \limits_{j=1}^{C} t_j \frac{1}{y_j} \frac{\partial y_j}{\partial a_j} \right ) \\
		&= w \left ( - \frac{t_i}{y_i} \frac{\partial y_i}{\partial a_j} - \sum \limits_{j \neq i}^{C} \frac{t_j}{y_j} \frac{\partial y_j}{\partial a_j} \right ) \\ \intertext{using the derivative of the Softmax from \ref{eq:softmax_deriv}, it follows that}
		&= w \left ( - \frac{t_i}{y_i} y_i (1 - y_i) - \sum \limits_{j \neq i}^{C} \frac{t_j}{y_j} (- y_j y_i) \right ) \\
		&= w \left ( - t_i + t_i y_i - \sum \limits_{j \neq i}^{C} - t_j y_i \right ) \\
		&= w \left ( - t_i - \sum \limits_{j=1}^{C} - t_j y_i  \right ) \\
		&= w \left ( - t_i + y_i \sum \limits_{j=1}^{C} t_j \right ) \\ \intertext{because $t_j = 0$ for all $C_j$ except the ground truth class, this can be simplified to} 
		&= w \left (- t_i + y_i \right ) \\ \tag*{$\blacksquare$} 
\end {align}

\textbf{TODO:} check: $y_i$ = output $i$ of softmax, $a_j$ = input $j$ to softmax?\\
 \textbf{TODO:} which values did we use? did we use class weighting at all? inverse weights..


		\subsection{F-Measure}

An alternative to solving the problem of unbalanced classes in the training data is using the \textit{F-Measure}, also called \textit{DICE similarity}, as a loss function instead. The F-Measure is a statistical similarity function which has been shown to perform well when using SGD-trained CNNs on unbalanced data. \cite{fmeasure3}\cite{fmeasure4}\cite{fmeasure5} It is based on the quantities of \textit{Precision} and \textit{Recall} over the training sample in comparison to its ground truth. The precision is defined as

\[ PR = \frac{TP}{TP + FP} \]

\noindent whereas the recall is defined as

\[ RC = \frac{TP}{TP + FN} \]

\noindent where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives. Traditionally, these values are defined as set theory operations, but to make the F-Measure differentiable, they are redefined as follows:

\begin {align}
TP &= \sum \limits_{i=1}^{n} y_i t_i \\
FP &= \sum  \limits_{i=1}^{n} y_i (1 - t_i) \\
FN &= \sum \limits_{i=1}^{n} (1 - y_i) t_i
\end {align}

\noindent with $y_i$ and $t_i$ being the Softmax probability of the class with the largest probability and the corresponding ground truth probability of a pixel $i$ in an image with $n$ pixels. The $F_1$ score for an image is then defined as

\begin {align}
 	F_1 &= 2 \left ( \frac{PR \cdot RC}{PR + RC} \right ) \\
		&= 2 \left ( \frac{\sum_{i=1}^{n} y_i t_i }{ \sum_{i=1}^{n} (y_i + t_i) } \right )
\end {align}

\noindent and the derivative to use during Backpropagation is

\begin {align}
	\delta &= \frac{\partial F_1}{\partial y_j}\\
	&= 2 \left ( \frac{t_j \sum_{i=1}^{n} \left ( y_i + t_i \right ) - \sum_{i=1}^{n} \left ( y_i t_i \right )  }{\left [ \sum_{i=1}^{n} \left ( y_i + t_i \right ) \right ]^2 } \right )
\end {align}


	\section {The U-Net Architecture}
The U-Net \cite{unet} is a CNN architecture which aims to produce segmentation maps for images of cells. It was implemented using Caffe \cite{caffe}, a CNN library developed by the University of California, Berkeley that allows CNN layers - such as the ones described in the previous section - to be put together to form a network.

% Image of U-Net here 
