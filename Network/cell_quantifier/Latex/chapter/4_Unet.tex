\chapter {U-Net}
Although possible, it wasn't necessary to construct a fitting CNN from scratch. Since the task of image segmentation is popular, a number of propositions have been made; among them is the \textit{U-Net}. The \textit{U-Net} is a CNN architecture proposed by Ronneberger et. al in 2015\cite{unet} which aims to produce binary segmentation maps for images of cells. The network consists of a first, ``contracting'' path which is compromised by a series of convolutions, followed by max-pooling layers, and then a ``widening'' path which performs deconvolutions, giving the network a U-shape. The network also uses \textit{Dropout} layers\cite{dropout} for regularization of its weights.

	\section {Net structure}
	% Image of U-Net here

	\section {Loss layers}

		\subsection{Softmax}

The loss layer that was proposed originally for the U-Net architecture is a \textit{Softmax Loss} layer. The name \textit{Softmax Loss} can lead to misunderstandings, because the layer's internal implementation actually consists of two steps:

First, the input $z$ of the layer is transformed by the softmax function

\[\sigma(z) = \frac{e^{z}}{\sum \limits_{k=0}^{K} e^{z_k}},\]

\noindent where $e$ is the exponential function and $z$ has a dimension of $1 \times K$. The softmax function acts as a normalizer that ``squashes'' the values of its input vector into the range $[0, 1]$ with respect to the differences between the vector values. For example, let $z = [-2.432, 1.832, 0.299]$. Then $\sigma(z) = [0.011, 0.813, 0.176]$ is the softmax output for $z$.

These processed values can be interpreted as probabilities much more easily than the raw scores the loss layer receives, which in turns leads to the second step, in which the actual loss computation is done. The actual loss function the layer uses is the \textit{Cross-Entropy Loss} that compares a true probability distribution $p$ to a model probability distribution $q$. For a vector of calculated softmax probabilities $z_i$, it is defined as follows:

\[CE(p, q) = -\sum \limits_{i = 0}^{K} p(z) \log q(z)\]

\noindent Conveniently, the softmax function forces all input values to be positive, allowing the logarithm in the loss to be used. Even though $\log (0)$ isn't defined, the softmax function should never produce values that are exactly equal to zero or one. \textbf{TODO: WHY?} 

However, the above definition of the softmax function can become numerically unstable when the exponentiation yields large values because dividing by large numbers could potentially produce zero values. To prevent this, the softmax can be made robust by defining it as

\[z_{new} = z - \max \limits_{i = 0}(0, z_i)\]
\[\sigma(z_{new}) = \frac{e^{z_{new}}}{\sum \limits_{k=0}^{K} e^{z_{new_{k}}}}\]

\noindent Since each pixel of the input image belongs to one class only, this means that for a given pixel, the probability of its ground truth label is $1.0$, while all others are $0.0$. If, without loss of generality, the third class is assumed to be the ground-truth class, $p$ will be $[0.0, 0.0, 1.0, 0.0]$.  Hence, only the predicted probability of the true third class influences the loss, and all the other summands can be omitted, yielding the condensed general formula

\[CE'(q) = - \log(q_{true}),\]

\noindent where $q_{true}$ denotes the predicted probability for the true class. The loss of the entire image is then calculated as the sum of the losses of all pixels divided by the number of pixels.



		\subsection{Weighted Softmax}

		\subsection{F-Measure}
