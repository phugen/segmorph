\chapter{Training}
\label{chap:training}

	\section{Dataset}
The dataset used in the following experiments is made up of 90 microscopy images and their label images from three experiments on slightly different types of live \textit{Drosophila} cells. The dimension of all images is $960 \times 608$ pixels ($w \times h$) and the images are given in grayscale. The labelled images are of dimension $3 \times 960 \times 608 \times$ ($d \times w \times h$), where the background class is represented by RGB[0, 0, 0] (black), the cell body proper is marked as RGB[0, 255, 0] (green), the Lamellopodia are RGB[255, 0, 0] (red) and the Filopodia are RGB[0, 0, 255] (blue).

Another ``3-class'' version of the dataset was created in which the classes for Lamellopodia and Filopodia are fused into one, marked as RGB[255, 0, 0] (red). This was done because even for a human, separating Filopodia pixels from Lamellopodia pixels was hard to do because of the varying contrast in the images, and it was deemed valuable to see how a network performs if the ground truth data is more straight-forward to interpret.\\

\begin {figure}[!ht]
	
	\begin {subfigure}[{position=b}]{0.6\linewidth}
		\begin {center}
		\includegraphics[scale=0.65]{img/dataset_ex1.png}
		\end{center}
	\end {subfigure}
	\begin {subfigure}[{position=b}]{0.3\linewidth}
		\begin {center}
		\includegraphics[scale=0.60]{img/dataset_ex2.png}
		\end{center}
	\end {subfigure}

		\caption[]{An example image tile from the 3-class dataset, with corresponding label. The label image is smaller than the input tile because the image tile already includes a mirrored border as padding (see Section \ref{subsec:mirror_tiling}).}
		\label{fig:dataset_example}
	

\end {figure}

	\section {Preprocessing and Data Augmentation}
Usually, neural networks do not work well with input of an arbitrary form, which is why data is preprocessed before training the network on it. The same preprocessing is applied during classification of new images. Hence, in the following sections, the preprocessing operations performed on input data to the U-Net is described. Furthermore, the concept of \textit{data augmentation} is explored, which helps with training when data is scarce.


		\subsection{Normalization}
\label{subsec:normalization}
Neural networks perform consistently better when the input data is normalized, that is, when it is transformed into a representation which has zero mean and unit variance (see Section \textbf{\ref{sec:weight_init}}). This is achieved by subtracting the mean value of the input image per channel from each channel of that image\footnote{Another approach often used is to compute a mean image from all images in the dataset and substract that from each image. However, in the following experiments, individual means were used.} and then dividing by the image's standard deviation (see Figure \ref{fig:norma}).

\begin {figure}[!ht]
	\begin{center}
		\scalebox{0.5}{\input{img/fig_norma.pgf}}
	\end{center}
	\caption[]{Normalization of two-dimensional example data. The blue data points represent the original dataset, while the orange points show the dataset after normalization.}
	\label{fig:norma}
\end {figure}


		\subsection{Mirror Tiling and Rotation}
\label{subsec:mirror_tiling}
CNNs usually need fixed input sizes because the combination of convolutions and pooling shrink the image in a certain way. This has to be accounted for when considering the input size of the image, because some input sizes are invalid in consequence. For example, downsampling with stride $2 \times 2$ shrinks an image by a factor of two, but if the dimensions of the image are not divisable by two at any downsampling stage in the network, the operator cannot be applied without causing interpolation problems. Because the U-Net is supposed to take an input image of some size and output a segmentation map of the same size, the input size additionally has to be chosen correctly so that network outputs an image of the correct size. This is done by adding a padding border to the input images so that in the case of an input size of $244 \times 244$, the padding increases the padded size to $428 \times 428$. This can be done by mirroring the image at its borders so that it matches the needed border size. This input size was chosen as as a compromise between needed memory and context within the image available to the CNN.

However, as the original Drosophila data is of a larger size, each input image is first padded by a mirror border and then tiled into sub-images of the appropriate size to be fed to the network. This has the added benefit of not having to use mirrored data at those tile edges which are not at the edges of the large input image - instead, real data is used. The tiling starts at the top-left corner and continues from left to right and from top to bottom. However, naively tiling would result in ``broken'' tiles at the right and bottom borders when the size of the mirror-padded  image is not evenly divisable by the network's input size, which was the case for the given input data. Consequently, these broken tiles were repaired by starting at the edges of the image and going ``backward'' in both dimensions until a tile of size $428 \times 428$ is created which uses both the broken tile data as well as data that is also part of the previous tile. While this reuses the overlapping parts of the tiles, this way, the data of the broken tiles can also be used for training (see Figure \ref{fig:tile_mirror_rotate}).\\

\noindent Moreover, a major problem with training deep Neural Networks is the immense amount of data required. Common image datasets such as MNIST\footnote{\url{http://yann.lecun.com/exdb/mnist/}}, CIFAR-100\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}} and ImageNet \cite{ILSVRC} contain 10,000 to 14.1 million images. Such numbers are absolutely out of reach for most biomedical applications as the data required is usually not publicly available. Manually labelling the data for this thesis took 30 - 45 minutes per image, making it impossible to create a large training set in a reasonable amount of time.

To nonetheless train a network to segment such images, \textit{data augmentation} is used to increase the number of samples. Data augmentation refers to some way of altering the available data in order to artificially create new data, which, for example, can be used to train a network to be invariant to certain transformations, although translation transformations can be disregarded as CNNs are naturally robust in the face of translated patterns (see Section \ref{sec:CNN}). An easy way to multiply the number of samples eightfold is to flip all images only vertically, followed by rotations of both the original images and their flipped counterparts by $90^{\circ}$, $180^{\circ}$ and $270^{\circ}$, as this yields all possible combinations that are possible by flipping and rotating in $90^{\circ}$ steps.\\


\begin {figure}[!ht]
	\begin {subfigure}[t]{0.5\linewidth}
		\scalebox{0.5}{\input{img/fig_tile_mirror.pdf_tex}}

		\caption*{Mirror tiling. \textbf{a):} Input image. \textbf{b):} Mirroring to obtain border (blue). \textbf{c):} Tiling. Broken tiles are shown darker. \textbf{d):} Repairing broken tiles by using combining broken tile data with data from neighboring tiles by moving back from the image edges ``into'' the image. The total data used for repairing all broken tiles is shown in green, while the data used for repairing the bottom right tile is marked by a dashed square.}
	\end {subfigure}
	\hspace{1cm}
	\begin {subfigure}[t]{0.5\linewidth}
		\scalebox{0.5}{\input{img/fig_rotate.pdf_tex}}

		\caption*{Combinations of flipping and rotating a tile. Taking the original, rotated tiles and the vertical, rotated tiles is enough to obtain all possible variations.}
	\end {subfigure}

		\caption[]{}
		\label{fig:tile_mirror_rotate}

\end {figure}


		\subsection{Elastic Deformation}
Another data augmentation technique is \textit{Elastic Deformation} \cite{elastic}, which calculates a new image from an existing one by sampling random new positions for each pixel in the original image uniformly and multiplies them with a force parameter $\alpha$. Afterwards, the new coordinates are additionally smoothed by a Gaussian filter with standard deviation $\sigma$. The resulting image is a slightly distorted image, and if $\alpha$ and $\sigma$ are chosen appropriately and the same deformation is applied to both an input image and its ground truth image, the resulting images are plausible enough to count as new samples to train on (see Figure \ref{fig:elastic}). This is especially useful for biological and medical use cases because Elastic Deformation produces naturally occuring transformations, such as cells being squashed slightly during microscopy, and helps the network to learn to also classify these correctly. For the work in this thesis, the deformation parameters were set to $\alpha = 200$ and $\sigma = 10$.\\

\noindent After performing data augmentation and removing all tiles that contained only pixels of one class as the result of the tiling (e.g. background pixels only), the dataset then contained 15632 image tiles and their corresponding labels. After, the remaining tiles were split with a ratio of 5:1 into a training and a validation set, containing 13026 images and 2606 images respectively.



\begin {figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.80]{img/fig_elastic.png}
	\end{center}
	\caption[]{\textbf{Left:} Input image with superimposed grid. \textbf{Right:} Elastic deformation with $\alpha = 100$ and $\sigma = 10$.}
	\label{fig:elastic}
\end {figure}


	\section {Regularization and Optimization}
\label{sec:reg_opt}

Choosing the number of hidden layers and the number of hidden neurons within them wisely is important because bad choices can make the network susceptible to underfitting or overfitting. \textit{Underfitting} is the term for the inability of a method to model the data correctly. For example, a linear classifier always underfits on a not-linearly separable dataset because it is not powerful enough to solve the problem. \textit{Overfitting} is the exact opposite and occurs when the model is too complex so that it doesn't model a trend in the data, but instead memorizes the dataset (see Figure \textbf{\ref{fig:overfit}}). This is undesirable because such a model cannot generalize well and will make incorrect classifications once it is asked to classify data it hasn't been presented with during training. Therefore, a balance has to be found to equip a model with both enough power to classify correctly while also allowing leeway for generalizing when making predictions on new data.

	\begin {figure}[!ht]
		\begin{center}
			\textbf{TODO: uncomment in code}
			%\scalebox{0.75}{\input{img/fig_overfit.pgf}}
		\end{center}
		\caption[]{Overfitting on a dataset, using a 2-layer MLP with 100 hidden neurons and 100,000 iterations of training. A better model would classify the three blue outliers as orange.}
		\label{fig:overfit}
	\end {figure}

\noindent The danger of overfitting on data increases with the complexity of the model. As MLPs and CNNs are highly complex, due to their abundance of weights, overfitting is often a problem. Therefore, several techniques to control overfitting were devised, which are called \textit{regularization} techniques.

Also, in addition to optimizations that target the SGD algorithm specifically, such as Momentum, there are also approaches that are applied to enhance the speed or stability of the training when training MLPs and CNNs especially. These are also discussed in the following sections.

	\subsection{Data Shuffling}
Research \cite{shuffling, lecun_norm} has shown that training progresses the fastest when the order in which the samples are examined is changed in each training epoch, but only if the samples are independent. For example, temporal data should not be shuffled because this would destroy the relationship between samples that the network is supposed to learn in the first place. In Caffe, enabling shuffling can be done conveniently by setting the \textit{shuffle} parameter in HDF5 input layers, which is implicitly set to \textit{false}, to \textit{true} instead.


	\subsection{Early Stopping}
\textit{Early Stopping} is based on the assumption that during the validation of the network, i.e. while letting it classify data that hasn't been used for training to test its generalization ability, overfitting is detectable by monitoring the validation loss. The validation loss measures how well the network performs for unseen data, and Early Stopping dictates that once the validation loss indicates overfitting by some stopping criterion, the training should be stopped. For smooth curves without local minima, this would simply mean that once the validation loss starts increasing instead of decreasing, the training should be stopped, although in practice, training and validation losses are unstable and thus harder to handle. \cite{early_stopping} introduces the \textit{generalization loss} value of a training epoch $t$, defined by

\[ GL(t) = 100 \left ( \frac{E_{val}(t)}{E_{min}(t)} - 1 \right ) \]

\noindent where $E_{val}(t)$ and $E_{min}(t)$ are the current validation loss and the minimum validation loss in all previous epochs $\{0, \dots, t\}$. Based on this, several stopping criteria are proposed, such as stopping the training when 

\[ GL(t) > \alpha \]

\noindent for some threshold $\alpha$, i.e. the relative increase of the loss with respect to the current best is exceeded by some threshold $\alpha$. However, this might prematurely stop the training in phases where the validation loss might still recover due to larger weight updates in which the training still progresses fast. Therefore, the training loss $E_{tr}(t)$ over $k$ epochs can be observed by using the training progress measure

\[ P_k(t) = 1000 \left ( \frac{\sum_{t' = t - k + 1}^{t} E_{tr}(t')}{k (\min_{t' = t - k + 1}^{t} E_{tr}(t'))} - 1 \right ) \]

\noindent which measures the ratio of average training error and minimum training error in that interval. In effect, $P_k$ is a measure of the training jitter present in the interval. The stopping criterion then is 

\[ \frac{GL(t)}{P_k(t)} > \beta \]

\noindent meaning that training is stopped when the generalization loss is too high and the training error has become stable, making it unlikely for the validation loss to recover (see Figure \ref{fig:early_stopping}). However, determining correct parameters and mathematically analyzing the benefits of Early Stopping is not easy, making this approach largely heuristical. Additionally, Caffe currently does not implement an easy way to apply these algorithms directly to the training, making it necessary to manually monitor training trends in practice while saving the state of the trained weights frequently so that the training can be interrupted when results stop improving.


\begin {figure}[!ht]
	\begin{center}
		\scalebox{0.7}{\input{img/fig_early_stopping.pgf}}
	\end{center}
	\caption[]{Stopping points calculated by two Early Stopping algorithms applied to artificially created loss data. The training loss is shown in orange, while the validation loss is shown in blue. \textbf{a):} Using $GL(t) > \alpha$ with $\alpha = 30$. \textbf{b):} Using $\frac{GL(t)}{P_k(t)} > \beta$, with $\beta = 10$ and $k = 10$. The first algorithm stops training even though the minimum validation loss is not reached, while the second algorithm gets closer to the global minimum. }
	\label{fig:early_stopping}
\end {figure}


	\subsection {Dropout}
\label{subsec:dropout}
\textit{Dropout} is a regularization technique \cite{dropout} that ``disables'' neurons it is applied to during training with a certain chance according to a user-defined Bernoulli probability. These disabled neurons are not trained on the particular training sample - an effect one could view as training only a sub-network of the original neural network. By applying Dropout repeatedly and scaling the weights by the Dropout probability $p$ during testing, one gains an approximation of averaging the outputs of all possible $2^n$ sub-networks, where $n$ is the number of neurons in the network. This lowers the amount of overfitting because neurons learn not to ``rely'' on other neurons which otherwise would lead to specialized co-adaptions that usually hurt the generalization ability of the network.



	\subsection {Batch Normalization}
\label{subsec:batchnorm}
\textit{Batch Normalization} (BN) \cite{batchnorm, batchnorm_pres} is a method to improve training speed and reduce overfitting while training either general MLPs or CNNs with the SGD algorithm (or other gradient-based algorithms that use mini-batches). It is known that Backpropagation learning works better if the inputs to the network are normalized (see Sections \textbf{\ref{sec:weight_init}} and \textbf{\ref{subsec:normalization}}). However, as the input passes through the network, the repeated transformations change the distribution of the data so that unnormalized values are used as input to hidden neurons, referred to as \textit{internal covariance shift}, which prolongs training as the network has to adapt to unnormalized outputs. One way to try and reduce this shift is by proper initialization of the network weights. BN provides a different way of dealing with covariance shift and claims to diminish the dependence on a good initialization as well as the need for Dropout.

The way BN reduces this shift is by re-normalizing activations $x_i$ within the network so that the following layers once again receive normalized activations. For this purpose, BN first calculates

\[ \mu_B = \frac{1}{m} \sum \limits_{i=1}^{m} x_i\,\, , \]

\noindent the mean of an SGD mini-batch $B$ with size $m$ and

\[ {\sigma^2}_B = \frac{1}{m} \sum \limits_{i=1}^{m} \left ( x_i - \mu_B \right )^2 \,\, , \]

\noindent the variance of $B$. Then, $x_i$ is normalized by applying the formula

\[  \hat{x_i} = \frac{x_i - \mu_B}{\sqrt{{\sigma^2}_B} + \epsilon} \]

\noindent where $\epsilon$ is a small constant that provides numerical stability. These calculations are done for each dimension of the input separately. Additionally, there exist two variables $\gamma$ and $\beta$ which are learned by Backpropagation. These variables perform an affine transformation of the normalized data given by

\[ y_i = \gamma \hat{x_i} + \beta \]

\noindent This transformation simply provides a means to scale and shift the normalized data. In case the best operation is to not transform the data at all, the parameters can be learned to be set to 

\[ \gamma = \sqrt{{\sigma^2}_B} \,\,\text{ and } \,\, \beta = \mu_B ,\]

\noindent in which case the affine transformation simply recovers the original, un-normalized input to the BN layer. For convolutional layers, BN calculates $\gamma$ and $\beta$ per feature map so that all values in the same feature map are normalized the same way. When using the network for classification of some new data sample $x_i$ at test time, the BN layers apply the learned transformation to $x_i$ directly, i.e. the data is transformed using the actual mean and variance of the sample, $E[x]$ and $Var[x]$, not the mini-batch averages:

\[  \hat{x_i} = \frac{x_i - E[x]}{\sqrt{Var[x]} + \epsilon}\,\,  . \]

\noindent This way, inference produces deterministic results that only depend on the input.\\

\noindent BN is also claimed to regularize the network in addition to speeding up training. This is assumed to be the case because the variance and mean values that are are calculated for a sample in a mini-batch are always an average of all samples in the batch, while the batch is chosen randomly from all samples. This means that variance and mean that are used for the BN transformation of a sample are always going to be different, depending on what other samples are included in the mini-batch alongside the sample in question, forcing the network to become robust against this form of ``noise''.


	\subsection {Pseudo-Labelling}
\label{subsec:pseudo_label}
\textit{Pseudo-labelling} \cite{pseudo_label} is a regularization technique that makes use of data for which no ground truth data exists. The network processes $n$ normal and unlabeled samples in each training step, assuming the mini-batch size is $n$.\footnote{\cite{pseudo_label} defines two different mini-batch sizes so that the number of normal and unlabeled samples per training step can be different. For notational simplicity, it is assumed that both sizes are the same.} The loss $L$ for the labelled image is calculated as usual. For unlabelled images, a pseudo-label is generated by passing the unlabelled data into the network using dummy labels, ignoring the resulting loss and replacing the output prediction scores by a one-hot encoding where the class with the maximum output probability is 1. For example, if the network outputs the probabilities 

\[ [0.1,\, 0.1,\, \textbf{0.8},\, 0.1] \]

\noindent for some pixel, the pseudo-probabilties for this pixel would become

\[ [0.0,\, 0.0,\, \textbf{1.0},\, 0.0] \]

\noindent in one-hot encoding. The unlabelled image is then fed to the network again, using the pseudo-label to calculate a pseudo-loss $L_p$. Then, a weighted combined loss $L_c$ is calculated to update the network weights, defined by

\[ L_c = \frac{1}{n} \sum \limits_{m=1}^{n} L(x_m) + \alpha \frac{1}{n} \sum \limits_{m=1}^{n} L_p(u_m)  \]

\noindent where $x$ are labeled samples, $u$ are unlabeled samples and $\alpha$ is a weighting parameter that depends on the number of iterations $t$ that the training has been running. It is defined as

\begin {align}
\alpha(t) = \begin{cases} 0 &\text{ if } t < T_1 \\
				\frac{t - T_1}{T_2 - T_1} \alpha_f &\text{ if } T_1 \leq t < T_2 \\
				\alpha_f &\text { if } T_2 \leq t \\
	        \end{cases}
\end {align}

\noindent with $\alpha_f = 3$, $T_1 = 100$ and $T_2 = 600$. This factor $\alpha$ prevents the unlabelled loss to dominate the weight updates in a stage of training where the network still makes very incorrect predictions. Intuitively, the network confirms its own predictions and updates the weights more decisively by increasing confidence for the update by selecting the dominant class in each pseudo-label.

The reason that pseudo-labelling boosts generalization performance is believed to be that it pushes the decision boundaries of the network towards areas of the data space where the density of data points is low. The ``Cluster Assumption'', the belief that samples that are close to each other are more likely to belong to the same class, holds for many datasets and therefore it is more favorable to have decision boundaries that do not cross these clusters. \textbf{TODO: entropy regularization? more explanation + citations}