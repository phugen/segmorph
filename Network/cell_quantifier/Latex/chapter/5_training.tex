\chapter{Training}
\label{chap:training}

The following chapter presents the properties of the dataset that is later used to train a CNN as well as the necessary preprocessing to feed the data into the network. Additionally, the concept of data augmentation is introduced, which is used for creating a larger dataset to train on. Finally, various regularization techniques are considered that are employed to improve the results of the classification by combating overfitting.

	\section{Dataset}
The dataset used in the following experiments is made up of 90 microscopy images and their label images from three experiments on slightly different types of live \textit{Drosophila} cells. The dimension of all images is $960 \times 608$ pixels ($w \times h$) and the images are given in grayscale. The labeled images are of dimension $3 \times 960 \times 608$ ($d \times w \times h$), where the background class is represented by RGB[0, 0, 0] (black), the cell body is marked as RGB[0, 255, 0] (green), the Lamellopodia are RGB[255, 0, 0] (red) and the Filopodia are RGB[0, 0, 255] (blue).

Another ``3-class'' version of the dataset was created in which the classes for Lamellopodia and Filopodia are fused into one, marked as RGB[255, 0, 0] (red). This was done because even for a human, separating Filopodia pixels from Lamellopodia pixels was hard to do because of the varying contrast in the images, and it was deemed valuable to see how a network performs if the ground truth data is more straight-forward to interpret.\\

\begin {figure}[!htb]
	
	\begin {subfigure}[{position=b}]{0.6\linewidth}
		\begin {center}
		\includegraphics[scale=0.65]{img/dataset_ex1.png}
		\end{center}
	\end {subfigure}
	\begin {subfigure}[{position=b}]{0.3\linewidth}
		\begin {center}
		\includegraphics[scale=0.60]{img/dataset_ex2.png}
		\end{center}
	\end {subfigure}

		\caption[An example image tile from the 3-class dataset.]{An example image tile from the 3-class dataset with corresponding label image. The label image is smaller than the input tile because the image tile already includes a mirrored border as padding (see Section \textbf{\ref{subsec:mirror_tiling}}).}
		\label{fig:dataset_example}
	

\end {figure}

	\section {Preprocessing and Data Augmentation}
Usually, neural networks do not work well with input of an arbitrary form, which is why data is preprocessed before training the network on it. The same preprocessing is applied during classification of new images. Hence, in the following sections, the preprocessing operations performed on input data to the U-Net are described. Furthermore, the concept of \textit{data augmentation} is explored, which helps with training when data is scarce.


		\subsection{Normalization}
\label{subsec:normalization}
Neural networks perform consistently better when the input data is normalized, that is, when it is transformed into a representation which has zero mean and unit variance (see Section \textbf{\ref{sec:weight_init}}). This is achieved by subtracting the mean value of the input image per channel from each channel of that image\footnote{Another approach often used is to compute a mean image from all images in the dataset and substract that from each image. However, in the following experiments, individual means were used.} and then dividing by the image's standard deviation (see Figure \textbf{\ref{fig:norma}}).

\begin {figure}[!htb]
	\begin{center}
		\scalebox{1.0}{\input{img/fig_norma.pgf}}
	\end{center}
	\caption[Normalization of two-dimensional data.]{Normalization of two-dimensional example data. The blue data points represent the original dataset, while the orange points show the dataset after normalization.}
	\label{fig:norma}
\end {figure}


		\subsection{Mirror Tiling and Rotation}
\label{subsec:mirror_tiling}
CNNs usually need fixed input sizes because the combination of convolutions and pooling shrink the image in a certain way. This has to be accounted for when considering the input size of the image, because some input sizes are invalid in consequence. For example, downsampling with stride $2 \times 2$ shrinks an image by a factor of two, but if the dimensions of the image are not divisable by two at any downsampling stage in the network, the operator cannot be applied without causing interpolation problems. Because the U-Net is supposed to take an input image of some size and output a segmentation map of the same size, the input size additionally has to be chosen correctly so that the network outputs an image of the correct size. This is done by adding a padding border to the input images so that in the case of an input size of $244 \times 244$, the padding increases the padded size to $428 \times 428$. This can be done by mirroring the image at its borders so that it matches the needed border size. This input size was chosen as a compromise between needed memory and context within the image available to the CNN.

However, as the original Drosophila data is of a larger size, each input image is first padded by a mirror border and then tiled into sub-images of the appropriate size to be fed to the network. This has the added benefit of not having to use mirrored data at those tile edges which are not at the edges of the large input image - instead, real data is used. The tiling starts at the top-left corner and continues from left to right and from top to bottom. Yet, tiling naively would result in ``broken'' tiles at the right and bottom borders when the size of the mirror-padded  image is not evenly divisable by the network's input size, which was the case for the given input data. Consequently, these broken tiles were repaired by starting at the edges of the image and going ``backward'' in both dimensions until a tile of size $428 \times 428$ is created, which uses both the broken tile data as well as data that is also part of the previous tile. While this reuses the overlapping parts of the tiles, this way, the data of the broken tiles can also be used for training (see Figure \textbf{\ref{fig:tile_mirror_rotate}}).

\noindent Moreover, a major requirement for training deep Neural Networks is an immense amount of data. Common image datasets such as MNIST\footnote{\url{http://yann.lecun.com/exdb/mnist/}}, CIFAR-100\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}} and ImageNet \cite{ILSVRC} contain 10,000 to 14.1 million images. Such numbers are absolutely out of reach for most biomedical applications as the data required is usually not publicly available. Manually labeling the data for this thesis took 30 - 45 minutes per image, making it impossible to create a large training set in a reasonable amount of time.

To nonetheless train a network to segment such images, \textit{data augmentation} is used to increase the number of samples. Data augmentation refers to some way of altering the available data in order to artificially create new data, which, for example, can be used to train a network to be invariant to certain transformations, although translational transformations can be disregarded as CNNs are naturally robust in the face of translated patterns (see Section \textbf{\ref{sec:CNN}}).

An easy way to multiply the number of samples eightfold is to flip all images only vertically, followed by rotations of both the original images and their flipped counterparts by $90^{\circ}$, $180^{\circ}$ and $270^{\circ}$, as this yields all possible combinations that are possible by flipping and rotating in $90^{\circ}$ steps.\\


\begin {figure}[!htb]
	\begin {subfigure}[t]{0.5\linewidth}
		\scalebox{0.5}{\input{img/fig_tile_mirror.pdf_tex}}

		\caption*{Mirror tiling. \textbf{a):} Input image. \textbf{b):} Mirroring to obtain border (blue). \textbf{c):} Tiling. Broken tiles are shown darker. \textbf{d):} Repairing broken tiles by using combining broken tile data with data from neighboring tiles by moving back from the image edges ``into'' the image. The total data used for repairing all broken tiles is shown in green, while the data used for repairing the bottom right tile is marked by a dashed square.}
	\end {subfigure}
	\hspace{1cm}
	\begin {subfigure}[t]{0.5\linewidth}
		\scalebox{0.5}{\input{img/fig_rotate.pdf_tex}}

		\caption*{Combinations of flipping and rotating a tile containing part of a larger image. Taking the original, rotated tiles and the vertical, rotated tiles is enough to obtain all possible variations.}
	\end {subfigure}

		\caption[Mirror tiling and rotating.]{}
		\label{fig:tile_mirror_rotate}

\end {figure}


		\subsection{Elastic Deformation}
Another data augmentation technique is \textit{Elastic Deformation} \cite{elastic}. Elastic Deformation is a way to create new, slightly different images from existing images by introducing a discrete grid of an arbitrary resolution corresponding to the positions of image pixels in the original image and then sampling random new positions for each grid node by adding displacement vectors drawn from a Gaussian distribution with standard deviation $\sigma$ to them. These vectors are additionally multiplied by a force parameter $\alpha$. Using an interpolation method such as bilinear interpolation, the result is a slightly distorted image, and if $\alpha$ and $\sigma$ are chosen appropriately and the same deformation is applied to both an input image and its ground truth image, the resulting images are plausible enough to count as new samples to train on (see Figure \textbf{\ref{fig:elastic}}). This is especially useful for biological and medical use cases because Elastic Deformation produces transformations that can naturally occur, such as cells being squashed slightly during microscopy, which helps the network learn to also classify these correctly. For the work in this thesis, the deformation parameters were set to $\alpha = 200$ and $\sigma = 10$ while using a grid with the same resolution as the image.

Because the image labels were color-coded and simply interpolating would change these colors to meaningless ones, one method to ensure that the labels remain intact is to use nearest-neighbor interpolation for the labels instead of bilinear interpolation.\\

\noindent After tiling and removing all those tiles that contained only pixels of one class (e.g. background pixels only), the dataset was then split into separate training and validation sets with a ratio of 4:1. The data augmentation was performed only for the training images, meaning that the network was tested on real images only. For both the 3- and 4-class datasets, the training set contained 12497 image tiles and their corresponding labels after applying augmentations, while the validation set contained 199 tiles cut from strictly unaltered images.


\begin {figure}[!htb]
	\begin{center}
		\includegraphics[scale=0.80]{img/fig_elastic.png}
	\end{center}
	\caption[Elastic deformation.]{\textbf{Left:} Input image with superimposed lines to show the effect of the deformation more clearly. \textbf{Right:} Effects of Elastic deformation on the first image with $\alpha = 100$ and $\sigma = 10$ and a grid of the same resolution as the image.}
	\label{fig:elastic}
\end {figure}


	\section {Regularization and Optimization}
\label{sec:reg_opt}

Choosing the number of hidden layers and the number of hidden neurons within them wisely is important because bad choices can make the network susceptible to underfitting or overfitting.

\textit{Underfitting} is the term for the inability of a method to model the data correctly. For example, a linear classifier always underfits on a not-linearly separable dataset because it is not powerful enough to solve the problem. \textit{Overfitting} is the exact opposite and occurs when the model is too complex so that it does not model a trend in the data, but instead memorizes the dataset (see Figure \textbf{\ref{fig:overfit}}). This is undesirable because such a model cannot generalize well and will make incorrect classifications once it is asked to classify data it has not been presented with during training. Therefore, a balance has to be found to equip a model with both enough power to classify correctly while also allowing leeway for generalizing when making predictions on new data.\\

%\textbf{TODO: Regularization term for SGD: https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate}\\

	\begin {figure}[!htb]
		\begin{center}
			%\textbf{TODO: uncomment in code}
			\scalebox{0.75}{\input{img/fig_overfit.pgf}}
		\end{center}
		\caption[Overfitting on a dataset.]{Overfitting on a dataset, using a 2-layer neural network with 100 hidden neurons and 100,000 iterations of training. A better model would classify the three blue outliers as orange.}
		\label{fig:overfit}
	\end {figure}

\noindent The danger of overfitting on data increases with the complexity of the model. As neural networks and CNNs are highly complex due to their abundance of weights, overfitting is often a problem. Therefore, several techniques to control overfitting were devised, which are called \textit{regularization} techniques.

Also, in addition to optimizations that target the SGD algorithm specifically, such as Momentum, there are also approaches that are applied to enhance the speed or stability of the training when training neural networks and CNNs especially. These are also discussed in the following sections.

	\subsection{Data Shuffling}
Research \cite{shuffling, lecun_norm} has shown that training progresses the fastest when the order in which the samples are examined is changed in each training epoch, but only if the samples are independent. For example, temporal data should not be shuffled because this would destroy the relationship between samples that the network is supposed to learn in the first place. In Caffe, enabling shuffling can be done conveniently by setting the \textit{shuffle} parameter in HDF5 input layers, which is implicitly set to \textit{false}, to \textit{true} instead, which changes the order of the elements in the dataset in each epoch.


	\subsection{Early Stopping}
\textit{Early Stopping} is based on the assumption that during the validation of the network, i.e. while letting it classify data that has not been used for training to test its generalization ability, overfitting is detectable by monitoring the validation loss. The validation loss measures how well the network performs for unseen data, and Early Stopping dictates that once the validation loss indicates overfitting by some stopping criterion, the training should be stopped. For smooth curves without local minima, this would simply mean that once the validation loss starts increasing instead of decreasing, the training should be stopped, although in practice, training and validation losses are oscillating and thus harder to handle. \cite{early_stopping} introduces the \textit{generalization loss} value of a training epoch $t$, defined by

\[ \text{GL}(t) = 100 \left ( \frac{E_{\text{val}}(t)}{E_{\text{min}}(t)} - 1 \right ) \,, \]

\noindent where $E_{\text{val}}(t)$ and $E_{\text{min}}(t)$ are the current validation loss and the minimum validation loss in all previous epochs $\{0, \dots, t\}$. Based on this, several stopping criteria are proposed, such as stopping the training when 

\[ \text{GL}(t) > \alpha \]

\noindent for some threshold $\alpha$, i.e. the relative increase of the loss with respect to the current best is exceeded by some threshold $\alpha$.\\

However, this might prematurely stop the training in phases where the validation loss might still recover due to larger weight updates in which the training still progresses fast. Therefore, the training loss $E_{\text{tr}}(t)$ over $k$ epochs can be observed by using the training progress measure

\[ P_k(t) = 1000 \left ( \frac{\sum_{t' = t - k + 1}^{t} E_{\text{tr}}(t')}{k (\min_{t' = t - k + 1}^{t} E_{\text{tr}}(t'))} - 1 \right ) \]

\noindent which measures the ratio of average training error and minimum training error in that interval. In effect, $P_k$ is a measure of the training jitter present in the interval. The stopping criterion then is 

\[ \frac{\text{GL}(t)}{P_k(t)} > \beta \,, \]

\noindent meaning that training is stopped when the generalization loss is too high and the training error has become stable, making it unlikely for the validation loss to recover (see Figure \textbf{\ref{fig:early_stopping}}).

However, determining correct parameters is not easy, being a largely heuristical process. Additionally, Caffe currently does not implement an easy way to apply these algorithms to the training directly, making it necessary in practice to manually monitor training trends while saving the state of the trained weights frequently so that the training can be interrupted before the network starts to overfit.


\begin {figure}[!htb]
	\begin{center}
		\scalebox{0.7}{\input{img/fig_early_stopping.pgf}}
	\end{center}
	\caption[Early stopping.]{Stopping points calculated by two Early Stopping algorithms applied to artificially created loss data. The training loss is shown in orange, while the validation loss is shown in blue. \textbf{a):} Using $\text{GL}(t) > \alpha$ with $\alpha = 30$. \textbf{b):} Using $\text{GL}(t)/P_k(t) > \beta$, with $\beta = 10$ and $k = 10$. The first algorithm stops training even though the minimum validation loss is not reached, while the second algorithm gets closer to the global minimum. }
	\label{fig:early_stopping}
\end {figure}


	\subsection {Dropout}
\label{subsec:dropout}
\textit{Dropout} is a regularization technique \cite{dropout} that ``disables'' neurons during training. It is applied to each neuron during training with a certain chance according to a user-defined Bernoulli probability. These disabled neurons are not trained on the particular training sample by setting their gradients to zero - an effect one could view as training only a sub-network of the original neural network. By applying Dropout repeatedly in each iteration and scaling the weights by the Dropout probability $p$ during the testing phase of the network, one gains an approximation of averaging the outputs of all possible $2^n$ sub-networks, where $n$ is the number of neurons in the network. This lowers the amount of overfitting because neurons learn not to ``rely'' on other neurons, which otherwise would lead to specialized co-adaptions that hurt the generalization ability of the network.



	\subsection {Batch Normalization}
\label{subsec:batchnorm}
\textit{Batch Normalization} (BN) \cite{batchnorm, batchnorm_pres} is a method to improve training speed and reduce overfitting while training either general neural networks or CNNs with the SGD algorithm (or other gradient-based algorithms that use mini-batches). It is known that Backpropagation learning works better if the inputs to the network are normalized (see Sections \textbf{\ref{sec:weight_init}} and \textbf{\ref{subsec:normalization}}). However, as the input passes through the network, the repeated transformations change the distribution of the data so that unnormalized values are used as input to hidden neurons, referred to as \textit{internal covariance shift}, which prolongs training as the network has to adapt to unnormalized outputs of earlier layers.

One way to try and reduce this shift is by proper initialization of the network weights. BN provides a different way of dealing with covariance shift and claims to diminish the dependence on a good initialization as well as the need for Dropout.\\

The way BN reduces this shift is by re-normalizing activations $x_i$ within the network before the next activation function is applied, so that the following layers once again receive normalized activations. For this purpose, BN first calculates

\[ \mu_B = \frac{1}{m} \sum \limits_{i=1}^{m} x_i\,\,, \]

\noindent the mean of an SGD mini-batch $B$ with size $m$ and

\[ {\sigma^2}_B = \frac{1}{m} \sum \limits_{i=1}^{m} \left ( x_i - \mu_B \right )^2 \,\,, \]

\noindent the variance of $B$. Then, $x_i$ is normalized by applying the formula

\[  \hat{x_i} = \frac{x_i - \mu_B}{\sqrt{{\sigma^2}_B} + \epsilon} \,, \]

\noindent where $\epsilon$ is a small constant that provides numerical stability. These calculations are done for each dimension of the input separately. Additionally, there exist two variables $\gamma$ and $\beta$ which are learned by Backpropagation. These variables perform an affine transformation of the normalized data given by

\[ y_i = \gamma \hat{x_i} + \beta \,. \]

\noindent This transformation simply provides a means to scale and shift the normalized data. In case the best operation is to not transform the data at all, the parameters can be learned to be set to 

\[ \gamma = \sqrt{{\sigma^2}_B} \,\,\text{ and } \,\, \beta = \mu_B \]

\noindent in which case the affine transformation simply recovers the original, un\--norm\-a\-lized input to the BN layer. For convolutional layers, BN calculates $\gamma$ and $\beta$ per feature map so that all values in the same feature map are normalized the same way. When using the network for classification of some new data sample $x_i$ at test time, the BN layers apply the learned transformation to $x_i$ directly, i.e. the data is transformed using the actual mean and variance of the sample, $\text{E}[x]$ and $\text{Var}[x]$, not the mini-batch averages:

\[  \hat{x_i} = \frac{x_i - \text{E}[x]}{\sqrt{\text{Var}[x]} + \epsilon}\,\,  . \]

\noindent This way, inference produces deterministic results that only depend on the input.\\

\noindent BN is also claimed to regularize the network in addition to speeding up training. This is assumed to be the case because the variance and mean values that are calculated for a sample in a mini-batch are always an average of all samples in the batch, while the batch is chosen randomly from all samples. This means that variance and mean that are used for the BN transformation of a sample are always going to be different, depending on what other samples are included in the mini-batch alongside the sample in question, forcing the network to become robust against this form of noise.
