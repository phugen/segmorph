\chapter {Further research}
\label{chap:futurework}

In the following sections, additional optimizations for and thoughts on the U-Net segmentation network are described. Testing these had to be cut from the experiments conducted in the course of this thesis because of time constraints, although they might improve the current best score further.

\section {Initialization noise}

While an effort was made to find out how large the influence of the random weight initialization of both Xavier and MSRA-style initializations on the results of the network is, an exhaustive test was not possible because this would have taken too long. Ideally, networks with both initialization schemes should be trained multiple times to produce a series of results from which one can determine the minimum and maximum result. This would help determine whether the use of different activation functions or shuffling produces a result that is statistically significant and not merely the result of noise. In the same vein, one could fix the initialization of the network and then train networks with the same initializations but different optimizations to remove the noise jitter and directly compare the networks.

Also, $k$-fold cross validation could be performed for all networks, using different parts of the training set as the validation set and training $k$ times. This is to avoid biased results due to parts of the training set that might be extraordinarily easy or hard to classify.


	\section {Pseudo-labeling}
\label{subsec:pseudo_label}

\textit{Pseudo-labeling} \cite{pseudo_label} is a regularization technique that makes use of data for which no ground truth data exists. The network processes $n$ normal and unlabeled samples in each training step, assuming the mini-batch size is $n$.\footnote{\cite{pseudo_label} defines two different mini-batch sizes so that the number of normal and unlabeled samples per training step can be different. For notational simplicity, it is assumed that both sizes are the same.} The loss $L$ for the labeled image is calculated as usual. For unlabeled images, a pseudo-label is generated by passing the unlabeled data into the network using dummy labels, ignoring the resulting loss and replacing the output prediction scores by a one-hot encoding, where the class with the maximum output probability is 1.0. For example, if the network outputs the probabilities 

\[ [0.1,\, 0.1,\, \textbf{0.8},\, 0.1] \]

\noindent for some pixel, the pseudo-probabilties for this pixel would become

\[ [0.0,\, 0.0,\, \textbf{1.0},\, 0.0] \]

\noindent in one-hot encoding. The unlabeled image is then fed to the network again, using the pseudo-label to calculate a pseudo-loss $L_\text{p}$. Then, a weighted combined loss $L_\text{c}$ is calculated to update the network weights, defined by

\[ L_\text{c} = \frac{1}{n} \sum \limits_{i=1}^{n} L(x_i) + \alpha \frac{1}{n} \sum \limits_{i=1}^{n} L_\text{p}(u_i) \,, \]

\noindent where $x$ are labeled samples, $u$ are unlabeled samples, $L$ is the normal loss for the labeled image and $\alpha$ is a weighting parameter that depends on the number of iterations $t$ the training has been running. It is defined as

\begin {align}
\alpha(t) = \begin{cases} 0 &\text{ if } t < T_1 \\
				\frac{t - T_1}{T_2 - T_1} \alpha_f &\text{ if } T_1 \leq t < T_2 \\
				\alpha_f &\text { if } T_2 \leq t \\
	        \end{cases}
\end {align}

\noindent with $\alpha_f = 3$, $T_1 = 100$ and $T_2 = 600$. This factor $\alpha$ prevents the unlabeled loss to dominate the weight updates in a stage of training where the network still makes very incorrect predictions. Intuitively, the network confirms its own predictions and updates the weights more decisively by increasing confidence for the update by selecting the dominant class in each pseudo-label.\\

\begin {figure}[!htb]
	\begin{center}
		\scalebox{0.45}{\input{img/fig_pseudo_label.pdf_tex}}
	\end{center}

		\caption[Pseudo-Labeling.]{The pseudo-labeling approach. The orange arrows show the path of the labeled images, while the blue arrows show the path of the unlabeled images that receive pseudo-labels. For the sake of simplicity, batch sizes are not taken into consideration in this image.}
		\label{fig:pseudo_label}
\end {figure}

The reason that pseudo-labeling boosts generalization performance is believed to be that it pushes the decision boundaries of the network towards areas of the data space where the density of data points is low. This is considered favorable because of the \textit{cluster assumption}, the belief that samples that are close to each other are more likely to belong to the same class, so decision boundaries that do not cross these clusters are supposed to generalize better. \cite{pseudo_label}

Pseudo-labeling was not used in the training of the networks presented earlier because Caffe, as of the time of writing, does not provide an easy means to change the way the ``Solver'' works, which is the part of the code that governs weight update calculation and application during SGD. Since pseudo-labeling depends on modifying the loss by splitting it into a normal loss and a pseudo-loss, the Solver itself has to be changed, which is not an easy endeavor as the code is highly abstract and uses extensive GPU parallelization. Using Caffe's Python API to replace the Solver is possible, but a naive implementation of the SGD algorithm slows down the training massively when compared to the original implementation, which was inacceptable for the time constraints on the network experiments. However, pseudo-labeling provides a way to make use of unlabeled data which otherwise would not contribute to training, so the idea itself should not be neglected.


	\section{Denoising Auto-Encoders}

\textit{Auto-encoders} are a kind of network structure that performs unsupervised learning by trying to learn to reconstruct its inputs $x$. These networks have an hourglass shape, with the number of hidden neurons first decreasing towards the smallest hidden layer, the ``bottleneck'', in what is called the \textit{encoding} part of the network, after which the number of neurons starts increasing again in the \textit{decoding} half of the network. The output layer has the same number of neurons as the input layer. To train these networks in an unsupervised fashion, the reconstructed output $\hat{x}$ is compared to $x$ using a loss function like the Cross-Entropy, while the weights are updated using Backpropagation, just like the labels are compared to the outputs in a normal neural network during supervised training.

A \textit{denoising auto-encoder} is a network that tries to reconstruct corrupted inputs $\widetilde{x}$ instead of the normal inputs $x$. $\widetilde{x}$ is created from the normal inputs $x$ by statistical means, e.g. by adding salt-and-pepper noise or Gaussian noise to $x$ (see Figure \textbf{\ref{fig:auto_encoder}}). During Backpropagation, the reconstruction $\hat{x}$ is then compared to the \textbf{original} inputs $x$ instead of the corrupted input $\widetilde{x}$. This way, the encoder has to become robust to noise because simply copying $\widetilde{x}$ will yield a high loss. \cite{autoencoder, denoising_autoencoder}\\

\begin {figure}[!htb]
	\begin{center}
		\scalebox{0.70}{\input{img/fig_autoencoder.pdf_tex}}
	\end{center}

		\caption[A denoising auto-encoder.]{A denoising auto-encoder network with a two-neuron bottleneck. The encoder part of the network is shown in blue, while the decoder part is shown in orange.}
		\label{fig:auto_encoder}
\end {figure}

Auto-encoders can be used for pre-training a network in an unsupervised way to improve further ``fine-tuning'' training by training each layer separately, while fixing all weights of layers that have already been trained. Unsupervised pre-training has been reported to lead to better training results \cite{pretraining} and the pseudo-labeling approach described in Section \textbf{\ref{subsec:pseudo_label}} has been reported to benefit from it as well, achieving an overall lower validation loss in the experiments conducted in \cite{pseudo_label}.


	\section{Other optimization algorithms}

Lastly, although many applications use it, SGD is not the only optimization algorithm that can be used for training neural networks. A number of algorithms have been suggested, such as Accelerated Gradient Descent (AGD), which is SGD combined with a different momentum update algorithm called \textit{Nesterov momentum}, which evaluates the gradient at the position where the normal momentum update would take the weights, and then taking another, corrected step, instead of directly computing the gradient at the current position. \cite{nesterov}

Alternatively, there are \textit{adaptive} learning algorithms which do not require tuning of the learning rate because they change it automatically depending on the importance of the current weights, even individually for each weight as opposed to using a global learning rate. For example, weights that receive only small updates are trained with a much higher learning rate compared to those which are changed heavily in each update step. Such algorithms include \textit{Adagrad} \cite{adagrad}, \textit{Adadelta} \cite{adadelta}, \textit{Adam} \cite{adam} and \textit{RMSProp} \cite{rmsprop}, which escape false minima much more easily than SGD. Therefore, testing how the aforementioned algorithms behave when training the U-Net architecture might lead to improved results.