\chapter {Results}
\label{chap:results}

	\section{Hardware}
All variants of the U-Net were trained on a NVIDIA TITAN Xp GPU (12 GB VRAM) using Caffe's CUDA/cuDNN support.

	\section {Segmentation quality}

To evaluate the quality of the segmentations, the average multi-class F-Measure (see Section \ref{subsec:fmeasure}) of the network outputs for all validation images compared to their ground truth was calculated.\\

\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{CNN variant}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		Unet& & \\ \hline
		Unet\_Batchnorm& & \\ \hline
		Unet\_Weighted& & \\ \hline
		Unet\_Weighted\_Batchnorm& & \\ \hline
		Unet\_F1& & \\ \hline
		Unet\_F1\_Batchnorm & & \\ \hline
	\end {tabular}
\caption[]{Multi-class F-Measure scores of 3 and 4-class CNN segmentations.}
\end {table}

\textbf{TODO: Batchnorm: lower learning rate, higher decay rate, no dropout?}
\noindent Among the CNNs, the best network \textbf{TODO} was selected and its ReLU activation functions were replaced by LReLUs, PReLUs and ELUs to evaluate the differences in performance with respect to the activation function used. This was done because an exhaustive search for the best combination of weight initialization, activation functions, hyperparameters (for instance learning rate) and techniques like Dropout and Batch Normalization would have exceeded the time limit of this thesis. Also, at the time of writing, it was not yet known how these different approaches interact precisely. For example, Batch Normalization and weight initialization have the same goal but achieve it in different ways, making it unclear whether one or the other performs better in practice.

\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{Activation}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		ReLU& & \\ \hline
		LReLU& & \\ \hline
		PReLU& & \\ \hline
		ELU& & \\ \hline
	\end {tabular}
\caption[]{Multi-class F-Measure scores of 3 and 4-class segmentations for the \textbf{TODO} network, using different activations functions.}
\end {table}

\noindent Then, the effect of the weight initialization on the best network was compared, using the Xavier and the MSRA initializations.

\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{Init method}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		Xavier& & \\ \hline
		MSRA& & \\ \hline
	\end {tabular}
\caption[]{Multi-class F-Measure scores of 3 and 4-class segmentations for the \textbf{TODO} network, using \textbf{TODO} activations and either Xavier or MSRA weight initialization.}
\end {table}

\textbf{TODO: Evaluation of training progress, i.e. validation loss progession}\\
\textbf{TODO: Enable shuffling in tests https://valserb.wordpress.com/2016/05/15/hdf5-shuffle-caffe/}\\


\noindent To compare the performance of all methods on the validation set with each other, the \textit{Micro} and \textit{Macro} variants of the F-Measure \cite{micromacro} are a suitable way to quantize how well the segmentation works. The Micro F-Measure is defined by the Precision and Recall quantities (see Section \textbf{\ref{subsec:fmeasure}}) of the validation set:

\[ F_{1\mu} = 2 \left ( \frac{PR_\mu \cdot RC_\mu}{PR_\mu + RC_\mu} \right ) \]

\noindent However, here, $PR_\mu$ and $RC_\mu$ denote the micro-average Precision and Recall when comparing the segmented image with the image ground truth. However, $PR_\mu$ and $RC_\mu$ are calculated by calculating the sum of all $TP$, $FP$ and $FN$ values for all images and using these sums to calculate the Precision and Recall over all $n$ validation images, i.e.

\[ PR_\mu = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} (TP_i + FP_i)} \text{ and }  RC_\mu = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} (TP_i + FN_i)} \]

\noindent The Macro F-Measure likewise is defined as

\[ F_{1M} = 2 \left ( \frac{PR_M \cdot RC_M}{PR_M + RC_M} \right ) \]

\noindent where $PR_M$ and $RC_M$ are the macro-average Precision and Recall. These are calculated for each sample independently, summed, and averaged over all $n$ samples:

\[ PR_M = \frac{1}{n} \sum_{i=1}^{n} \frac{TP_i}{TP_i + FP_i} \text { and } RC_M = \frac{1}{n} \sum_{i=1}^{n} \frac{TP_i}{TP_i + FN_i} \] 

\noindent \cite[pp. 317-318]{information_retrieval} highlights that the Micro F-Measure is dominated by ``large'' classes, meaning classes that occur often in the ground truth data. This shifts the focus of the segmentation effectiveness evaluation towards whether the large classes are segmented correctly. As most pixels in the validation images are background pixels and the correct segmentation of the non-background class pixels is of more interest, the Macro F-Measure is therefore chosen for evaluation because it is biased towards smaller classes.\\

\noindent Because Otsu thresholding, K-Means and Gaussian Mixture Models are all unsupervised methods, i.e. they do not depend on ground truth images, the labels they output have no direct relation to the ground truth labels used in the CNN training. Therefore, all combinations of matching the output labels with the ground truth labels are evaluated and for each, a multiclass F-Measure score is calculated. The assignment with the highest score is then assumed to be the correct one, which is then used for the overall evaluation using the Macro F-Measure over all segmentations.


\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{Method}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		Otsu& & \\ \hline
		K-Means& & \\ \hline
		GMM& & \\ \hline
		\textbf{TODO-network}& & \\ \hline
	\end {tabular}
\caption[]{Multi-class F-Measure scores of 3 and 4-class segmentations. The best network is compared to the outputs of unsupervised methods.}
\end {table}