\chapter {Results}
\label{chap:results}

	\section{Hardware}
All variants of the U-Net were trained on a NVIDIA TITAN X GPU (12 GB VRAM) using Caffe's CUDA/cuDNN support. \textbf{TODO: Processor, RAM? What else?}

	\section {Segmentation quality evaluation}

\noindent As there are many possible variations of the U-Net architecture, these variations were tested iteratively, choosing the best network of a number of networks and modifying it further. This was done because an exhaustive search for the best combination of weight initialization, activation functions, hyperparameters like the learning rate and techniques such as Dropout and Batch Normalization would have exceeded the time limit of this thesis. Also, at the time of writing, it was not yet known how these different approaches interact precisely. For example, Batch Normalization and specialized weight initialization schemes have the same goal but achieve it in different ways, making it unclear whether one or the other performs better in practice.\\

\noindent To compare the performance of all methods on the validation set with each other, the \textit{Micro} and \textit{Macro} variants of the F-Measure \cite{micromacro} are a suitable way to quantify how well the segmentation works. The Micro F-Measure is defined by the Precision and Recall quantities (see Section \textbf{\ref{subsec:fmeasure}}) of the validation set:

\[ F_{1\mu} = 2 \left ( \frac{PR_\mu \cdot RC_\mu}{PR_\mu + RC_\mu} \right ) \]

\noindent Here, $PR_\mu$ and $RC_\mu$ denote the micro-average Precision and Recall over the entire validation set. $PR_\mu$ and $RC_\mu$ are calculated by taking the sum of all $TP$, $FP$ and $FN$ values for all images and deriving the Precision and Recall over all $n$ validation images from these sums, i.e.

\[ PR_\mu = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} (TP_i + FP_i)} \text{ and }  RC_\mu = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} (TP_i + FN_i)} \]

\noindent The Macro F-Measure likewise is defined as

\[ F_{1M} = 2 \left ( \frac{PR_M \cdot RC_M}{PR_M + RC_M} \right ) \]

\noindent where $PR_M$ and $RC_M$ are the macro-average Precision and Recall. These are calculated for each sample independently, summed, and averaged over all $n$ samples:

\[ PR_M = \frac{1}{n} \sum_{i=1}^{n} \frac{TP_i}{TP_i + FP_i} \text { and } RC_M = \frac{1}{n} \sum_{i=1}^{n} \frac{TP_i}{TP_i + FN_i} \] 

\noindent \cite[pp. 317-318]{information_retrieval} highlights that the Micro F-Measure is dominated by ``large'' classes, meaning classes that occur often in the ground truth data. This shifts the focus of the segmentation effectiveness evaluation towards whether the large classes are segmented correctly. As most pixels in the validation images are background pixels and the correct segmentation of the non-background class pixels is of more interest, the Macro F-Measure is therefore chosen for assessing which method performs best because it is biased towards smaller classes rather than large ones, but for completeness, both quantities are listed.\\

\noindent The first test run pitted two nearly identical U-Net networks against each other, using ReLU activations and Dropout with $p = 0.5$. The only difference was the choice of the loss function. The \textbf{Unet\_Weighted} networks used the weighted Cross-Entropy Loss, while the \textbf{Unet\_F1} networks employed the multi-class F-Measure. Both networks were trained for 80,000 iterations (or $\approx$ 30 epochs) on both the 3-class and the 4-class training set, while testing the network on the respective validation set every 1,000 iterations.

\textbf{Unet\_Weighted} used an initial learning rate of 0.001, a step learning rate decay of a factor $\zeta = 0.1$ every 20,000 iterations and a momentum modifier $\gamma = 0.99$, while \textbf{Unet\_F1} used an initial learning rate of 0.0001, $\zeta = 0.3$ every 20,000 iterations and $\gamma = 0.99$. Both networks used L2 gradient regularization and a mini-batch size of 5.\\

\noindent The progress of the training is shown in Figure \textbf{\ref{fig:weighted_f1_training}}. The F-Measure scores provide the insight that the proper cells (green) can be discerned from the background much more easily than the Lamellopodia (red) and Filopodia (blue), which mirrors the problems when manually labelling the images. Reducing the dataset to three classes by merging the red and blue classes improves the results for both networks.\\

\begin {figure}[!ht]
	\begin {subfigure}[b]{0.4\linewidth}
		\scalebox{0.685}{\input{img/fig_train_unet_weighted_3.pgf}}
		\caption{\textbf{Unet\_Weighted\_3}}
	\end {subfigure}\hspace{1.3cm}
	\begin {subfigure}[b]{0.4\linewidth}
		\scalebox{0.70}{\input{img/fig_train_unet_weighted_4.pgf}}
		\caption{\textbf{Unet\_Weighted\_4}}
	\end {subfigure}

	\begin {subfigure}[b]{0.4\linewidth}
		\scalebox{0.70}{\input{img/fig_train_unet_f1_3.pgf}}
		\caption{\textbf{Unet\_F1\_3}}
	\end {subfigure}\hspace{1.3cm}
	\begin {subfigure}[b]{0.4\linewidth}
		\scalebox{0.675}{\input{img/fig_train_unet_f1_4.pgf}}
		\caption{\textbf{Unet\_F1\_4}}
	\end {subfigure}

		\caption[Training progress of the first collection of networks.]{Training progress of the first collection of networks. The training loss is shown in muted colors, while the validation loss is denoted by bright colors. For Unet\_F1, the Multi-class F-Measure score of each class is shown in the respective color. Repetitions in the validation progress in later epochs happen because shuffling was not used.}
		\label{fig:weighted_f1_training}
\end {figure}

\noindent The results of testing the networks on the validation set are shown in Table \textbf{\ref{tab:results1}}. They indicate that both networks perform similarly well, although using a Cross-Entropy loss function beats the F-Measure slightly. The Cross-Entropy networks achieved Macro F-Measure scores of $\approx$\textbf{0.877} for 3 classes and $\approx$\textbf{0.746} for 4 classes. Based on these results, for the following tests, Cross-Entropy was used as the loss function.\\ 

\begin {table}
	\begin{flushleft}
		\begin {tabular}[!ht]{|l|c|c|c|c|}
			\hline\multicolumn{5}{|l|}{\textbf{3-class Micro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_3& \cellcolor{green!25}0.936013& \cellcolor{green!25}0.979221& \cellcolor{green!25}0.848447& \cellcolor{green!25}0.959854 \\ \hline
			Unet\_F1\_3& 0.933723& 0.968166& 0.786455&  0.941755\\ \hline
			\multicolumn{5}{|l|}{\textbf{3-class Macro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_3& \cellcolor{green!25}0.841432& \cellcolor{green!25}0.977186& \cellcolor{green!25}0.803589& \cellcolor{green!25}0.876880 \\ \hline
			Unet\_F1\_3& 0.837715& 0.964669& 0.761217& 0.858995 \\ \hline
		\end {tabular}
		\vspace{0.5cm}\\
		\begin {tabular}[!ht]{|l|c|c|c|c|c|}
			\hline\multicolumn{6}{|l|}{\textbf{4-class Micro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Class 4}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_4& \cellcolor{green!25}0.63248& 0.978377& \cellcolor{green!25}0.660174& 0.927164& 0.934388 \\ \hline
			Unet\_F1\_4& 0.632356& \cellcolor{green!25}0.978707& 0.641875& \cellcolor{green!25}0.928546& \cellcolor{green!25}0.935324 \\ \hline
			\multicolumn{6}{|l|}{\textbf{4-class Macro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Class 4}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_4& \cellcolor{green!25}0.59455& 0.975988& 0.565911& \cellcolor{green!25}0.827576& \cellcolor{green!25}0.746051 \\ \hline
			Unet\_F1\_4& 0.592614& \cellcolor{green!25}0.976762& \cellcolor{green!25}0.57009& 0.823417& 0.742489 \\ \hline
		\end {tabular}
	\end {flushleft}

\caption[Micro and Macro F-Measure scores for Unet\_Weighted and Unet\_F1.]{Micro and Macro F-Measure scores achieved by Unet\_Weighted and Unet\_F1 when segmenting the validation set images into 3 and 4 classes. In the 4-class dataset, \textbf{class 1} is the background, \textbf{class 2} is the cell proper, \textbf{class 3} are the Filopodia and \textbf{class 4} are the Lamellopodia, while in the 3-class dataset, \textbf{class 3} represents both Filopodia and Lamellopodia. The best scores in each category, per class, are marked in green.}
\label{tab:results1}
\end {table}

\noindent The second training case tested whether using Batch Normalization provides benefits, either in convergence speed or validation score. Therefore, the \textbf{Unet\_Weighted} networks were modified to perform Batch Normalization before each ReLU activation, implemented in Caffe as a ``Batch Normalization'' layer that normalizes its input according to the mini-batch statistics, followed by a ``Scale'' layer that applies the affine transformation. Also, all Dropout layers were removed, as advised in \cite{batchnorm}.

Again, the network was trained on the 3- and 4-class datasets, this time, as advised in \cite{batchnorm}, with a higher initial learning rate of 0.005, and a faster step learning rate decay that reduces the learning rate by $\zeta = 0.1$ every 7,500 iterations. Momentum was kept at $\gamma = 0.99$. However, the network parameters didn't fit into memory with a mini-batch size of 5 because of the added Batch Normalization layers, and therefore the mini-batch size had to be lowered to 2. To be able to still compare the training to previous results, the number of iterations was raised to 200,000 so that the network was trained for $\approx$30 epochs as before. The results of the training are shown in Table \textbf{\ref{tab:results2}}.


\begin {table}
	\begin{flushleft}
		\begin {tabular}[!ht]{|l|c|c|c|c|}
			\hline\multicolumn{5}{|l|}{\textbf{3-class Micro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_3& 0.936013& 0.979221& 0.848447& 0.959854 \\ \hline
			Unet\_Weighted\_Batchnorm\_3& & & & \\ \hline
			\multicolumn{5}{|l|}{\textbf{3-class Macro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_3& 0.841432& 0.977186&0.803589& 0.876880 \\ \hline
			Unet\_Weighted\_Batchnorm\_3& & & & \\ \hline
		\end {tabular}
		\vspace{0.5cm}\\
		\begin {tabular}[!ht]{|l|c|c|c|c|c|}
			\hline\multicolumn{6}{|l|}{\textbf{4-class Micro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Class 4}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_4& 0.63248& 0.978377& 0.660174& 0.927164& 0.934388 \\ \hline
			Unet\_Weighted\_Batchnorm\_4& & & & & \\ \hline
			\multicolumn{6}{|l|}{\textbf{4-class Macro F-Measure Scores}} \\ \hline
			\textbf{Network}& \textbf{Class 1}& \textbf{Class 2}& \textbf{Class 3}& \textbf{Class 4}& \textbf{Overall} \\ \hline
			Unet\_Weighted\_4& 0.59455& 0.975988& 0.565911& 0.827576& 0.746051 \\ \hline
			Unet\_Weighted\_Batchnorm\_4& & & & & \\ \hline
		\end {tabular}
	\end {flushleft}

\caption[Micro and Macro F-Measure scores for a network with and without Batch Normalization.]{Micro and Macro F-Measure scores of 3 and 4-class segmenations using a weighted Cross-Entropy loss with or without batch normalization.}
\label{tab:results2}
\end {table}

\noindent It is evident that \textbf{TODO}\\


\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{Activation}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		ReLU& & \\ \hline
		LReLU& & \\ \hline
		PReLU& & \\ \hline
		ELU& & \\ \hline
	\end {tabular}
\caption[Multi-Class F-Measure scores for networks with different activation functions.]{Multi-class F-Measure scores of 3 and 4-class segmentations for the \textbf{TODO} network, using different activations functions.}
\end {table}

\noindent Then, the effect of the weight initialization on the best network was compared, using the Xavier and the MSRA initializations.

\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{Init method}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		Xavier& & \\ \hline
		MSRA& & \\ \hline
	\end {tabular}
\caption[Multi-Class F-Measure scores for networks with Xavier and MSRA weight initialization.]{Multi-class F-Measure scores of 3 and 4-class segmentations for the \textbf{TODO} network, using \textbf{TODO} activations and either Xavier or MSRA weight initialization.}
\end {table}



\noindent Because Otsu thresholding, K-Means and Gaussian Mixture Models are all unsupervised methods, i.e. they do not depend on ground truth images, the labels they output have no direct relation to the ground truth labels used in the CNN training. Therefore, all combinations of matching the output labels with the ground truth labels are evaluated and for each, a multiclass F-Measure score is calculated. The assignment with the highest score is then assumed to be the correct one, which is then used for the overall evaluation using the Macro F-Measure over all segmentations.\\

\textbf{TODO: Show activation maps of different layers!}\\
\textbf{TODO: Enable shuffling in tests https://valserb.wordpress.com/2016/05/15/hdf5-shuffle-caffe/}\\


\begin {table}
	\centering
	\begin {tabular}[!ht]{|l|c|c|}
		\hline
		\textbf{Method}& \textbf{3 classes}& \textbf{4 classes}\\ \hline
		Otsu& & \\ \hline
		K-Means& & \\ \hline
		GMM& & \\ \hline
		\textbf{TODO-network}& & \\ \hline
	\end {tabular}
\caption[Multi-class F-Measure scores of the best network in comparison to the unsupervised methods.]{Multi-class F-Measure scores of 3 and 4-class segmentations. The best network is compared to the outputs of unsupervised methods.}
\end {table}