I0428 19:51:20.402153   966 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3456
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 100000
lr_policy: "step"
gamma: 0.1
momentum: 0.99
stepsize: 7500
snapshot: 1000
snapshot_prefix: "./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3"
solver_mode: GPU
net: "./unet_weighted_batchnorm_3/unet_weighted_batchnorm_3.prototxt"
regularization_type: "L2"
test_initialization: true
iter_size: 1
I0428 19:51:20.403334   966 solver.cpp:91] Creating training net from net file: ./unet_weighted_batchnorm_3/unet_weighted_batchnorm_3.prototxt
I0428 19:51:20.404095   966 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: ./unet_weighted_batchnorm_3/unet_weighted_batchnorm_3.prototxt
I0428 19:51:20.404106   966 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0428 19:51:20.404274   966 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer loaddata
I0428 19:51:20.404999   966 net.cpp:58] Initializing net from parameters: 
name: "unet_weighted_batchnorm_3"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "loaddata"
  type: "HDF5Data"
  top: "data"
  top: "label"
  top: "weights"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "caffeHDF5_3.txt"
    batch_size: 2
  }
}
layer {
  name: "conv_d0a-b"
  type: "Convolution"
  bottom: "data"
  top: "d0b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d0b"
  type: "BatchNorm"
  bottom: "d0b"
  top: "d0b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d0b"
  type: "Scale"
  bottom: "d0b"
  top: "d0b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d0b"
  type: "ReLU"
  bottom: "d0b"
  top: "d0b"
}
layer {
  name: "conv_d0b-c"
  type: "Convolution"
  bottom: "d0b"
  top: "d0c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d0c"
  type: "BatchNorm"
  bottom: "d0c"
  top: "d0c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d0c"
  type: "Scale"
  bottom: "d0c"
  top: "d0c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d0c"
  type: "ReLU"
  bottom: "d0c"
  top: "d0c"
}
layer {
  name: "pool_d0c-1a"
  type: "Pooling"
  bottom: "d0c"
  top: "d1a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d1a-b"
  type: "Convolution"
  bottom: "d1a"
  top: "d1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d1b"
  type: "BatchNorm"
  bottom: "d1b"
  top: "d1b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d1b"
  type: "Scale"
  bottom: "d1b"
  top: "d1b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d1b"
  type: "ReLU"
  bottom: "d1b"
  top: "d1b"
}
layer {
  name: "conv_d1b-c"
  type: "Convolution"
  bottom: "d1b"
  top: "d1c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d1c"
  type: "BatchNorm"
  bottom: "d1c"
  top: "d1c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d1c"
  type: "Scale"
  bottom: "d1c"
  top: "d1c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d1c"
  type: "ReLU"
  bottom: "d1c"
  top: "d1c"
}
layer {
  name: "pool_d1c-2a"
  type: "Pooling"
  bottom: "d1c"
  top: "d2a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d2a-b"
  type: "Convolution"
  bottom: "d2a"
  top: "d2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d2b"
  type: "BatchNorm"
  bottom: "d2b"
  top: "d2b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d2b"
  type: "Scale"
  bottom: "d2b"
  top: "d2b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d2b"
  type: "ReLU"
  bottom: "d2b"
  top: "d2b"
}
layer {
  name: "conv_d2b-c"
  type: "Convolution"
  bottom: "d2b"
  top: "d2c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d2c"
  type: "BatchNorm"
  bottom: "d2c"
  top: "d2c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d2c"
  type: "Scale"
  bottom: "d2c"
  top: "d2c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d2c"
  type: "ReLU"
  bottom: "d2c"
  top: "d2c"
}
layer {
  name: "pool_d2c-3a"
  type: "Pooling"
  bottom: "d2c"
  top: "d3a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d3a-b"
  type: "Convolution"
  bottom: "d3a"
  top: "d3b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d3b"
  type: "BatchNorm"
  bottom: "d3b"
  top: "d3b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d3b"
  type: "Scale"
  bottom: "d3b"
  top: "d3b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d3b"
  type: "ReLU"
  bottom: "d3b"
  top: "d3b"
}
layer {
  name: "conv_d3b-c"
  type: "Convolution"
  bottom: "d3b"
  top: "d3c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d3c"
  type: "BatchNorm"
  bottom: "d3c"
  top: "d3c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d3c"
  type: "Scale"
  bottom: "d3c"
  top: "d3c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d3c"
  type: "ReLU"
  bottom: "d3c"
  top: "d3c"
}
layer {
  name: "pool_d3c-4a"
  type: "Pooling"
  bottom: "d3c"
  top: "d4a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d4a-b"
  type: "Convolution"
  bottom: "d4a"
  top: "d4b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d4b"
  type: "BatchNorm"
  bottom: "d4b"
  top: "d4b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d4b"
  type: "Scale"
  bottom: "d4b"
  top: "d4b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d4b"
  type: "ReLU"
  bottom: "d4b"
  top: "d4b"
}
layer {
  name: "conv_d4b-c"
  type: "Convolution"
  bottom: "d4b"
  top: "d4c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d4c"
  type: "BatchNorm"
  bottom: "d4c"
  top: "d4c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d4c"
  type: "Scale"
  bottom: "d4c"
  top: "d4c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d4c"
  type: "ReLU"
  bottom: "d4c"
  top: "d4c"
}
layer {
  name: "upconv_d4c_u3a"
  type: "Deconvolution"
  bottom: "d4c"
  top: "u3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u3a"
  type: "BatchNorm"
  bottom: "u3a"
  top: "u3a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u3a"
  type: "Scale"
  bottom: "u3a"
  top: "u3a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u3a"
  type: "ReLU"
  bottom: "u3a"
  top: "u3a"
}
layer {
  name: "crop_d3c-d3cc"
  type: "Crop"
  bottom: "d3c"
  bottom: "u3a"
  top: "d3cc"
  crop_param {
    axis: 2
    offset: 4
  }
}
layer {
  name: "concat_d3cc_u3a-b"
  type: "Concat"
  bottom: "u3a"
  bottom: "d3cc"
  top: "u3b"
}
layer {
  name: "conv_u3b-c"
  type: "Convolution"
  bottom: "u3b"
  top: "u3c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u3c"
  type: "BatchNorm"
  bottom: "u3c"
  top: "u3c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u3c"
  type: "Scale"
  bottom: "u3c"
  top: "u3c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u3c"
  type: "ReLU"
  bottom: "u3c"
  top: "u3c"
}
layer {
  name: "conv_u3c-d"
  type: "Convolution"
  bottom: "u3c"
  top: "u3d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u3d"
  type: "BatchNorm"
  bottom: "u3d"
  top: "u3d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u3d"
  type: "Scale"
  bottom: "u3d"
  top: "u3d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u3d"
  type: "ReLU"
  bottom: "u3d"
  top: "u3d"
}
layer {
  name: "upconv_u3d_u2a"
  type: "Deconvolution"
  bottom: "u3d"
  top: "u2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u2a"
  type: "BatchNorm"
  bottom: "u2a"
  top: "u2a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u2a"
  type: "Scale"
  bottom: "u2a"
  top: "u2a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u2a"
  type: "ReLU"
  bottom: "u2a"
  top: "u2a"
}
layer {
  name: "crop_d2c-d2cc"
  type: "Crop"
  bottom: "d2c"
  bottom: "u2a"
  top: "d2cc"
  crop_param {
    axis: 2
    offset: 16
  }
}
layer {
  name: "concat_d2cc_u2a-b"
  type: "Concat"
  bottom: "u2a"
  bottom: "d2cc"
  top: "u2b"
}
layer {
  name: "conv_u2b-c"
  type: "Convolution"
  bottom: "u2b"
  top: "u2c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u2c"
  type: "BatchNorm"
  bottom: "u2c"
  top: "u2c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u2c"
  type: "Scale"
  bottom: "u2c"
  top: "u2c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u2c"
  type: "ReLU"
  bottom: "u2c"
  top: "u2c"
}
layer {
  name: "conv_u2c-d"
  type: "Convolution"
  bottom: "u2c"
  top: "u2d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u2d"
  type: "BatchNorm"
  bottom: "u2d"
  top: "u2d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u2d"
  type: "Scale"
  bottom: "u2d"
  top: "u2d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u2d"
  type: "ReLU"
  bottom: "u2d"
  top: "u2d"
}
layer {
  name: "upconv_u2d_u1a"
  type: "Deconvolution"
  bottom: "u2d"
  top: "u1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u1a"
  type: "BatchNorm"
  bottom: "u1a"
  top: "u1a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u1a"
  type: "Scale"
  bottom: "u1a"
  top: "u1a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u1a"
  type: "ReLU"
  bottom: "u1a"
  top: "u1a"
}
layer {
  name: "crop_d1c-d1cc"
  type: "Crop"
  bottom: "d1c"
  bottom: "u1a"
  top: "d1cc"
  crop_param {
    axis: 2
    offset: 40
  }
}
layer {
  name: "concat_d1cc_u1a-b"
  type: "Concat"
  bottom: "u1a"
  bottom: "d1cc"
  top: "u1b"
}
layer {
  name: "conv_u1b-c"
  type: "Convolution"
  bottom: "u1b"
  top: "u1c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u1c"
  type: "BatchNorm"
  bottom: "u1c"
  top: "u1c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u1c"
  type: "Scale"
  bottom: "u1c"
  top: "u1c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u1c"
  type: "ReLU"
  bottom: "u1c"
  top: "u1c"
}
layer {
  name: "conv_u1c-d"
  type: "Convolution"
  bottom: "u1c"
  top: "u1d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u1d"
  type: "BatchNorm"
  bottom: "u1d"
  top: "u1d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u1d"
  type: "Scale"
  bottom: "u1d"
  top: "u1d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u1d"
  type: "ReLU"
  bottom: "u1d"
  top: "u1d"
}
layer {
  name: "upconv_u1d_u0a"
  type: "Deconvolution"
  bottom: "u1d"
  top: "u0a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u0a"
  type: "BatchNorm"
  bottom: "u0a"
  top: "u0a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u0a"
  type: "Scale"
  bottom: "u0a"
  top: "u0a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u0a"
  type: "ReLU"
  bottom: "u0a"
  top: "u0a"
}
layer {
  name: "crop_d0c-d0cc"
  type: "Crop"
  bottom: "d0c"
  bottom: "u0a"
  top: "d0cc"
  crop_param {
    axis: 2
    offset: 88
  }
}
layer {
  name: "concat_d0cc_u0a-b"
  type: "Concat"
  bottom: "u0a"
  bottom: "d0cc"
  top: "u0b"
}
layer {
  name: "conv_u0b-c"
  type: "Convolution"
  bottom: "u0b"
  top: "u0c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u0c"
  type: "BatchNorm"
  bottom: "u0c"
  top: "u0c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u0c"
  type: "Scale"
  bottom: "u0c"
  top: "u0c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u0c"
  type: "ReLU"
  bottom: "u0c"
  top: "u0c"
}
layer {
  name: "conv_u0c-d"
  type: "Convolution"
  bottom: "u0c"
  top: "u0d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u0d"
  type: "BatchNorm"
  bottom: "u0d"
  top: "u0d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u0d"
  type: "Scale"
  bottom: "u0d"
  top: "u0d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u0d"
  type: "ReLU"
  bottom: "u0d"
  top: "u0d"
}
layer {
  name: "conv_u0d-score"
  type: "Convolution"
  bottom: "u0d"
  top: "score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 3
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  bottom: "weights"
  top: "loss"
  loss_weight: 1
}
layer {
  name: "visualize"
  type: "Softmax"
  bottom: "score"
  top: "visualize_out"
  include {
    phase: TRAIN
  }
}
layer {
  name: "fake"
  type: "Silence"
  bottom: "visualize_out"
  include {
    phase: TRAIN
  }
}
I0428 19:51:20.405453   966 layer_factory.hpp:77] Creating layer loaddata
I0428 19:51:20.405464   966 net.cpp:100] Creating Layer loaddata
I0428 19:51:20.405468   966 net.cpp:408] loaddata -> data
I0428 19:51:20.405483   966 net.cpp:408] loaddata -> label
I0428 19:51:20.405488   966 net.cpp:408] loaddata -> weights
I0428 19:51:20.405498   966 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: caffeHDF5_3.txt
I0428 19:51:20.408848   966 hdf5_data_layer.cpp:93] Number of HDF5 files: 20
I0428 19:51:20.442777   966 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0428 19:51:34.296852   966 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0428 19:51:36.180227   966 net.cpp:150] Setting up loaddata
I0428 19:51:36.180274   966 net.cpp:157] Top shape: 2 3 428 428 (1099104)
I0428 19:51:36.180284   966 net.cpp:157] Top shape: 2 244 244 (119072)
I0428 19:51:36.180291   966 net.cpp:157] Top shape: 2 244 244 (119072)
I0428 19:51:36.180296   966 net.cpp:165] Memory required for data: 5348992
I0428 19:51:36.180305   966 layer_factory.hpp:77] Creating layer conv_d0a-b
I0428 19:51:36.180341   966 net.cpp:100] Creating Layer conv_d0a-b
I0428 19:51:36.180348   966 net.cpp:434] conv_d0a-b <- data
I0428 19:51:36.180361   966 net.cpp:408] conv_d0a-b -> d0b
I0428 19:51:36.184736   966 net.cpp:150] Setting up conv_d0a-b
I0428 19:51:36.184777   966 net.cpp:157] Top shape: 2 64 426 426 (23228928)
I0428 19:51:36.184782   966 net.cpp:165] Memory required for data: 98264704
I0428 19:51:36.184800   966 layer_factory.hpp:77] Creating layer bn_d0b
I0428 19:51:36.184814   966 net.cpp:100] Creating Layer bn_d0b
I0428 19:51:36.184823   966 net.cpp:434] bn_d0b <- d0b
I0428 19:51:36.184835   966 net.cpp:395] bn_d0b -> d0b (in-place)
I0428 19:51:36.185498   966 net.cpp:150] Setting up bn_d0b
I0428 19:51:36.185514   966 net.cpp:157] Top shape: 2 64 426 426 (23228928)
I0428 19:51:36.185521   966 net.cpp:165] Memory required for data: 191180416
I0428 19:51:36.185540   966 layer_factory.hpp:77] Creating layer sc_d0b
I0428 19:51:36.185554   966 net.cpp:100] Creating Layer sc_d0b
I0428 19:51:36.185559   966 net.cpp:434] sc_d0b <- d0b
I0428 19:51:36.185571   966 net.cpp:395] sc_d0b -> d0b (in-place)
I0428 19:51:36.185641   966 layer_factory.hpp:77] Creating layer sc_d0b
I0428 19:51:36.187938   966 net.cpp:150] Setting up sc_d0b
I0428 19:51:36.187963   966 net.cpp:157] Top shape: 2 64 426 426 (23228928)
I0428 19:51:36.187974   966 net.cpp:165] Memory required for data: 284096128
I0428 19:51:36.187986   966 layer_factory.hpp:77] Creating layer relu_d0b
I0428 19:51:36.187999   966 net.cpp:100] Creating Layer relu_d0b
I0428 19:51:36.188004   966 net.cpp:434] relu_d0b <- d0b
I0428 19:51:36.188014   966 net.cpp:395] relu_d0b -> d0b (in-place)
I0428 19:51:37.488075   966 net.cpp:150] Setting up relu_d0b
I0428 19:51:37.488098   966 net.cpp:157] Top shape: 2 64 426 426 (23228928)
I0428 19:51:37.488101   966 net.cpp:165] Memory required for data: 377011840
I0428 19:51:37.488106   966 layer_factory.hpp:77] Creating layer conv_d0b-c
I0428 19:51:37.488121   966 net.cpp:100] Creating Layer conv_d0b-c
I0428 19:51:37.488123   966 net.cpp:434] conv_d0b-c <- d0b
I0428 19:51:37.488129   966 net.cpp:408] conv_d0b-c -> d0c
I0428 19:51:37.490363   966 net.cpp:150] Setting up conv_d0b-c
I0428 19:51:37.490376   966 net.cpp:157] Top shape: 2 64 424 424 (23011328)
I0428 19:51:37.490380   966 net.cpp:165] Memory required for data: 469057152
I0428 19:51:37.490388   966 layer_factory.hpp:77] Creating layer bn_d0c
I0428 19:51:37.490396   966 net.cpp:100] Creating Layer bn_d0c
I0428 19:51:37.490417   966 net.cpp:434] bn_d0c <- d0c
I0428 19:51:37.490437   966 net.cpp:395] bn_d0c -> d0c (in-place)
I0428 19:51:37.490833   966 net.cpp:150] Setting up bn_d0c
I0428 19:51:37.490842   966 net.cpp:157] Top shape: 2 64 424 424 (23011328)
I0428 19:51:37.490844   966 net.cpp:165] Memory required for data: 561102464
I0428 19:51:37.490866   966 layer_factory.hpp:77] Creating layer sc_d0c
I0428 19:51:37.490872   966 net.cpp:100] Creating Layer sc_d0c
I0428 19:51:37.490876   966 net.cpp:434] sc_d0c <- d0c
I0428 19:51:37.490880   966 net.cpp:395] sc_d0c -> d0c (in-place)
I0428 19:51:37.490924   966 layer_factory.hpp:77] Creating layer sc_d0c
I0428 19:51:37.492295   966 net.cpp:150] Setting up sc_d0c
I0428 19:51:37.492321   966 net.cpp:157] Top shape: 2 64 424 424 (23011328)
I0428 19:51:37.492323   966 net.cpp:165] Memory required for data: 653147776
I0428 19:51:37.492344   966 layer_factory.hpp:77] Creating layer relu_d0c
I0428 19:51:37.492367   966 net.cpp:100] Creating Layer relu_d0c
I0428 19:51:37.492370   966 net.cpp:434] relu_d0c <- d0c
I0428 19:51:37.492374   966 net.cpp:395] relu_d0c -> d0c (in-place)
I0428 19:51:37.493203   966 net.cpp:150] Setting up relu_d0c
I0428 19:51:37.493214   966 net.cpp:157] Top shape: 2 64 424 424 (23011328)
I0428 19:51:37.493217   966 net.cpp:165] Memory required for data: 745193088
I0428 19:51:37.493235   966 layer_factory.hpp:77] Creating layer d0c_relu_d0c_0_split
I0428 19:51:37.493240   966 net.cpp:100] Creating Layer d0c_relu_d0c_0_split
I0428 19:51:37.493242   966 net.cpp:434] d0c_relu_d0c_0_split <- d0c
I0428 19:51:37.493247   966 net.cpp:408] d0c_relu_d0c_0_split -> d0c_relu_d0c_0_split_0
I0428 19:51:37.493252   966 net.cpp:408] d0c_relu_d0c_0_split -> d0c_relu_d0c_0_split_1
I0428 19:51:37.493320   966 net.cpp:150] Setting up d0c_relu_d0c_0_split
I0428 19:51:37.493326   966 net.cpp:157] Top shape: 2 64 424 424 (23011328)
I0428 19:51:37.493330   966 net.cpp:157] Top shape: 2 64 424 424 (23011328)
I0428 19:51:37.493332   966 net.cpp:165] Memory required for data: 929283712
I0428 19:51:37.493335   966 layer_factory.hpp:77] Creating layer pool_d0c-1a
I0428 19:51:37.493340   966 net.cpp:100] Creating Layer pool_d0c-1a
I0428 19:51:37.493343   966 net.cpp:434] pool_d0c-1a <- d0c_relu_d0c_0_split_0
I0428 19:51:37.493347   966 net.cpp:408] pool_d0c-1a -> d1a
I0428 19:51:37.493396   966 net.cpp:150] Setting up pool_d0c-1a
I0428 19:51:37.493402   966 net.cpp:157] Top shape: 2 64 212 212 (5752832)
I0428 19:51:37.493404   966 net.cpp:165] Memory required for data: 952295040
I0428 19:51:37.493407   966 layer_factory.hpp:77] Creating layer conv_d1a-b
I0428 19:51:37.493413   966 net.cpp:100] Creating Layer conv_d1a-b
I0428 19:51:37.493417   966 net.cpp:434] conv_d1a-b <- d1a
I0428 19:51:37.493422   966 net.cpp:408] conv_d1a-b -> d1b
I0428 19:51:37.494123   966 net.cpp:150] Setting up conv_d1a-b
I0428 19:51:37.494132   966 net.cpp:157] Top shape: 2 128 210 210 (11289600)
I0428 19:51:37.494134   966 net.cpp:165] Memory required for data: 997453440
I0428 19:51:37.494154   966 layer_factory.hpp:77] Creating layer bn_d1b
I0428 19:51:37.494159   966 net.cpp:100] Creating Layer bn_d1b
I0428 19:51:37.494171   966 net.cpp:434] bn_d1b <- d1b
I0428 19:51:37.494175   966 net.cpp:395] bn_d1b -> d1b (in-place)
I0428 19:51:37.494395   966 net.cpp:150] Setting up bn_d1b
I0428 19:51:37.494402   966 net.cpp:157] Top shape: 2 128 210 210 (11289600)
I0428 19:51:37.494405   966 net.cpp:165] Memory required for data: 1042611840
I0428 19:51:37.494415   966 layer_factory.hpp:77] Creating layer sc_d1b
I0428 19:51:37.494421   966 net.cpp:100] Creating Layer sc_d1b
I0428 19:51:37.494426   966 net.cpp:434] sc_d1b <- d1b
I0428 19:51:37.494429   966 net.cpp:395] sc_d1b -> d1b (in-place)
I0428 19:51:37.494462   966 layer_factory.hpp:77] Creating layer sc_d1b
I0428 19:51:37.494611   966 net.cpp:150] Setting up sc_d1b
I0428 19:51:37.494619   966 net.cpp:157] Top shape: 2 128 210 210 (11289600)
I0428 19:51:37.494621   966 net.cpp:165] Memory required for data: 1087770240
I0428 19:51:37.494626   966 layer_factory.hpp:77] Creating layer relu_d1b
I0428 19:51:37.494647   966 net.cpp:100] Creating Layer relu_d1b
I0428 19:51:37.494668   966 net.cpp:434] relu_d1b <- d1b
I0428 19:51:37.494673   966 net.cpp:395] relu_d1b -> d1b (in-place)
I0428 19:51:37.494858   966 net.cpp:150] Setting up relu_d1b
I0428 19:51:37.494868   966 net.cpp:157] Top shape: 2 128 210 210 (11289600)
I0428 19:51:37.494869   966 net.cpp:165] Memory required for data: 1132928640
I0428 19:51:37.494873   966 layer_factory.hpp:77] Creating layer conv_d1b-c
I0428 19:51:37.494880   966 net.cpp:100] Creating Layer conv_d1b-c
I0428 19:51:37.494884   966 net.cpp:434] conv_d1b-c <- d1b
I0428 19:51:37.494889   966 net.cpp:408] conv_d1b-c -> d1c
I0428 19:51:37.495978   966 net.cpp:150] Setting up conv_d1b-c
I0428 19:51:37.495986   966 net.cpp:157] Top shape: 2 128 208 208 (11075584)
I0428 19:51:37.495990   966 net.cpp:165] Memory required for data: 1177230976
I0428 19:51:37.495995   966 layer_factory.hpp:77] Creating layer bn_d1c
I0428 19:51:37.496003   966 net.cpp:100] Creating Layer bn_d1c
I0428 19:51:37.496006   966 net.cpp:434] bn_d1c <- d1c
I0428 19:51:37.496011   966 net.cpp:395] bn_d1c -> d1c (in-place)
I0428 19:51:37.496204   966 net.cpp:150] Setting up bn_d1c
I0428 19:51:37.496212   966 net.cpp:157] Top shape: 2 128 208 208 (11075584)
I0428 19:51:37.496215   966 net.cpp:165] Memory required for data: 1221533312
I0428 19:51:37.496222   966 layer_factory.hpp:77] Creating layer sc_d1c
I0428 19:51:37.496225   966 net.cpp:100] Creating Layer sc_d1c
I0428 19:51:37.496227   966 net.cpp:434] sc_d1c <- d1c
I0428 19:51:37.496232   966 net.cpp:395] sc_d1c -> d1c (in-place)
I0428 19:51:37.496263   966 layer_factory.hpp:77] Creating layer sc_d1c
I0428 19:51:37.496400   966 net.cpp:150] Setting up sc_d1c
I0428 19:51:37.496407   966 net.cpp:157] Top shape: 2 128 208 208 (11075584)
I0428 19:51:37.496409   966 net.cpp:165] Memory required for data: 1265835648
I0428 19:51:37.496414   966 layer_factory.hpp:77] Creating layer relu_d1c
I0428 19:51:37.496417   966 net.cpp:100] Creating Layer relu_d1c
I0428 19:51:37.496420   966 net.cpp:434] relu_d1c <- d1c
I0428 19:51:37.496425   966 net.cpp:395] relu_d1c -> d1c (in-place)
I0428 19:51:37.496580   966 net.cpp:150] Setting up relu_d1c
I0428 19:51:37.496589   966 net.cpp:157] Top shape: 2 128 208 208 (11075584)
I0428 19:51:37.496592   966 net.cpp:165] Memory required for data: 1310137984
I0428 19:51:37.496594   966 layer_factory.hpp:77] Creating layer d1c_relu_d1c_0_split
I0428 19:51:37.496599   966 net.cpp:100] Creating Layer d1c_relu_d1c_0_split
I0428 19:51:37.496601   966 net.cpp:434] d1c_relu_d1c_0_split <- d1c
I0428 19:51:37.496606   966 net.cpp:408] d1c_relu_d1c_0_split -> d1c_relu_d1c_0_split_0
I0428 19:51:37.496611   966 net.cpp:408] d1c_relu_d1c_0_split -> d1c_relu_d1c_0_split_1
I0428 19:51:37.496645   966 net.cpp:150] Setting up d1c_relu_d1c_0_split
I0428 19:51:37.496652   966 net.cpp:157] Top shape: 2 128 208 208 (11075584)
I0428 19:51:37.496655   966 net.cpp:157] Top shape: 2 128 208 208 (11075584)
I0428 19:51:37.496657   966 net.cpp:165] Memory required for data: 1398742656
I0428 19:51:37.496660   966 layer_factory.hpp:77] Creating layer pool_d1c-2a
I0428 19:51:37.496665   966 net.cpp:100] Creating Layer pool_d1c-2a
I0428 19:51:37.496666   966 net.cpp:434] pool_d1c-2a <- d1c_relu_d1c_0_split_0
I0428 19:51:37.496670   966 net.cpp:408] pool_d1c-2a -> d2a
I0428 19:51:37.496701   966 net.cpp:150] Setting up pool_d1c-2a
I0428 19:51:37.496706   966 net.cpp:157] Top shape: 2 128 104 104 (2768896)
I0428 19:51:37.496709   966 net.cpp:165] Memory required for data: 1409818240
I0428 19:51:37.496711   966 layer_factory.hpp:77] Creating layer conv_d2a-b
I0428 19:51:37.496719   966 net.cpp:100] Creating Layer conv_d2a-b
I0428 19:51:37.496721   966 net.cpp:434] conv_d2a-b <- d2a
I0428 19:51:37.496726   966 net.cpp:408] conv_d2a-b -> d2b
I0428 19:51:37.500146   966 net.cpp:150] Setting up conv_d2a-b
I0428 19:51:37.500161   966 net.cpp:157] Top shape: 2 256 102 102 (5326848)
I0428 19:51:37.500180   966 net.cpp:165] Memory required for data: 1431125632
I0428 19:51:37.500198   966 layer_factory.hpp:77] Creating layer bn_d2b
I0428 19:51:37.500221   966 net.cpp:100] Creating Layer bn_d2b
I0428 19:51:37.500224   966 net.cpp:434] bn_d2b <- d2b
I0428 19:51:37.500231   966 net.cpp:395] bn_d2b -> d2b (in-place)
I0428 19:51:37.500416   966 net.cpp:150] Setting up bn_d2b
I0428 19:51:37.500425   966 net.cpp:157] Top shape: 2 256 102 102 (5326848)
I0428 19:51:37.500428   966 net.cpp:165] Memory required for data: 1452433024
I0428 19:51:37.500440   966 layer_factory.hpp:77] Creating layer sc_d2b
I0428 19:51:37.500447   966 net.cpp:100] Creating Layer sc_d2b
I0428 19:51:37.500449   966 net.cpp:434] sc_d2b <- d2b
I0428 19:51:37.500455   966 net.cpp:395] sc_d2b -> d2b (in-place)
I0428 19:51:37.500494   966 layer_factory.hpp:77] Creating layer sc_d2b
I0428 19:51:37.500607   966 net.cpp:150] Setting up sc_d2b
I0428 19:51:37.500615   966 net.cpp:157] Top shape: 2 256 102 102 (5326848)
I0428 19:51:37.500634   966 net.cpp:165] Memory required for data: 1473740416
I0428 19:51:37.500639   966 layer_factory.hpp:77] Creating layer relu_d2b
I0428 19:51:37.500643   966 net.cpp:100] Creating Layer relu_d2b
I0428 19:51:37.500646   966 net.cpp:434] relu_d2b <- d2b
I0428 19:51:37.500650   966 net.cpp:395] relu_d2b -> d2b (in-place)
I0428 19:51:37.500831   966 net.cpp:150] Setting up relu_d2b
I0428 19:51:37.500840   966 net.cpp:157] Top shape: 2 256 102 102 (5326848)
I0428 19:51:37.500844   966 net.cpp:165] Memory required for data: 1495047808
I0428 19:51:37.500846   966 layer_factory.hpp:77] Creating layer conv_d2b-c
I0428 19:51:37.500852   966 net.cpp:100] Creating Layer conv_d2b-c
I0428 19:51:37.500855   966 net.cpp:434] conv_d2b-c <- d2b
I0428 19:51:37.500860   966 net.cpp:408] conv_d2b-c -> d2c
I0428 19:51:37.504961   966 net.cpp:150] Setting up conv_d2b-c
I0428 19:51:37.504972   966 net.cpp:157] Top shape: 2 256 100 100 (5120000)
I0428 19:51:37.504974   966 net.cpp:165] Memory required for data: 1515527808
I0428 19:51:37.504995   966 layer_factory.hpp:77] Creating layer bn_d2c
I0428 19:51:37.505000   966 net.cpp:100] Creating Layer bn_d2c
I0428 19:51:37.505004   966 net.cpp:434] bn_d2c <- d2c
I0428 19:51:37.505008   966 net.cpp:395] bn_d2c -> d2c (in-place)
I0428 19:51:37.505172   966 net.cpp:150] Setting up bn_d2c
I0428 19:51:37.505180   966 net.cpp:157] Top shape: 2 256 100 100 (5120000)
I0428 19:51:37.505182   966 net.cpp:165] Memory required for data: 1536007808
I0428 19:51:37.505188   966 layer_factory.hpp:77] Creating layer sc_d2c
I0428 19:51:37.505193   966 net.cpp:100] Creating Layer sc_d2c
I0428 19:51:37.505197   966 net.cpp:434] sc_d2c <- d2c
I0428 19:51:37.505200   966 net.cpp:395] sc_d2c -> d2c (in-place)
I0428 19:51:37.505234   966 layer_factory.hpp:77] Creating layer sc_d2c
I0428 19:51:37.505338   966 net.cpp:150] Setting up sc_d2c
I0428 19:51:37.505347   966 net.cpp:157] Top shape: 2 256 100 100 (5120000)
I0428 19:51:37.505348   966 net.cpp:165] Memory required for data: 1556487808
I0428 19:51:37.505352   966 layer_factory.hpp:77] Creating layer relu_d2c
I0428 19:51:37.505357   966 net.cpp:100] Creating Layer relu_d2c
I0428 19:51:37.505360   966 net.cpp:434] relu_d2c <- d2c
I0428 19:51:37.505363   966 net.cpp:395] relu_d2c -> d2c (in-place)
I0428 19:51:37.506201   966 net.cpp:150] Setting up relu_d2c
I0428 19:51:37.506214   966 net.cpp:157] Top shape: 2 256 100 100 (5120000)
I0428 19:51:37.506217   966 net.cpp:165] Memory required for data: 1576967808
I0428 19:51:37.506234   966 layer_factory.hpp:77] Creating layer d2c_relu_d2c_0_split
I0428 19:51:37.506239   966 net.cpp:100] Creating Layer d2c_relu_d2c_0_split
I0428 19:51:37.506242   966 net.cpp:434] d2c_relu_d2c_0_split <- d2c
I0428 19:51:37.506248   966 net.cpp:408] d2c_relu_d2c_0_split -> d2c_relu_d2c_0_split_0
I0428 19:51:37.506256   966 net.cpp:408] d2c_relu_d2c_0_split -> d2c_relu_d2c_0_split_1
I0428 19:51:37.506299   966 net.cpp:150] Setting up d2c_relu_d2c_0_split
I0428 19:51:37.506304   966 net.cpp:157] Top shape: 2 256 100 100 (5120000)
I0428 19:51:37.506307   966 net.cpp:157] Top shape: 2 256 100 100 (5120000)
I0428 19:51:37.506325   966 net.cpp:165] Memory required for data: 1617927808
I0428 19:51:37.506327   966 layer_factory.hpp:77] Creating layer pool_d2c-3a
I0428 19:51:37.506348   966 net.cpp:100] Creating Layer pool_d2c-3a
I0428 19:51:37.506352   966 net.cpp:434] pool_d2c-3a <- d2c_relu_d2c_0_split_0
I0428 19:51:37.506358   966 net.cpp:408] pool_d2c-3a -> d3a
I0428 19:51:37.506413   966 net.cpp:150] Setting up pool_d2c-3a
I0428 19:51:37.506422   966 net.cpp:157] Top shape: 2 256 50 50 (1280000)
I0428 19:51:37.506425   966 net.cpp:165] Memory required for data: 1623047808
I0428 19:51:37.506428   966 layer_factory.hpp:77] Creating layer conv_d3a-b
I0428 19:51:37.506434   966 net.cpp:100] Creating Layer conv_d3a-b
I0428 19:51:37.506436   966 net.cpp:434] conv_d3a-b <- d3a
I0428 19:51:37.506443   966 net.cpp:408] conv_d3a-b -> d3b
I0428 19:51:37.514961   966 net.cpp:150] Setting up conv_d3a-b
I0428 19:51:37.514972   966 net.cpp:157] Top shape: 2 512 48 48 (2359296)
I0428 19:51:37.514991   966 net.cpp:165] Memory required for data: 1632484992
I0428 19:51:37.514997   966 layer_factory.hpp:77] Creating layer bn_d3b
I0428 19:51:37.515009   966 net.cpp:100] Creating Layer bn_d3b
I0428 19:51:37.515012   966 net.cpp:434] bn_d3b <- d3b
I0428 19:51:37.515017   966 net.cpp:395] bn_d3b -> d3b (in-place)
I0428 19:51:37.515221   966 net.cpp:150] Setting up bn_d3b
I0428 19:51:37.515229   966 net.cpp:157] Top shape: 2 512 48 48 (2359296)
I0428 19:51:37.515231   966 net.cpp:165] Memory required for data: 1641922176
I0428 19:51:37.515238   966 layer_factory.hpp:77] Creating layer sc_d3b
I0428 19:51:37.515244   966 net.cpp:100] Creating Layer sc_d3b
I0428 19:51:37.515247   966 net.cpp:434] sc_d3b <- d3b
I0428 19:51:37.515250   966 net.cpp:395] sc_d3b -> d3b (in-place)
I0428 19:51:37.515300   966 layer_factory.hpp:77] Creating layer sc_d3b
I0428 19:51:37.515400   966 net.cpp:150] Setting up sc_d3b
I0428 19:51:37.515409   966 net.cpp:157] Top shape: 2 512 48 48 (2359296)
I0428 19:51:37.515411   966 net.cpp:165] Memory required for data: 1651359360
I0428 19:51:37.515416   966 layer_factory.hpp:77] Creating layer relu_d3b
I0428 19:51:37.515421   966 net.cpp:100] Creating Layer relu_d3b
I0428 19:51:37.515424   966 net.cpp:434] relu_d3b <- d3b
I0428 19:51:37.515427   966 net.cpp:395] relu_d3b -> d3b (in-place)
I0428 19:51:37.515633   966 net.cpp:150] Setting up relu_d3b
I0428 19:51:37.515643   966 net.cpp:157] Top shape: 2 512 48 48 (2359296)
I0428 19:51:37.515645   966 net.cpp:165] Memory required for data: 1660796544
I0428 19:51:37.515648   966 layer_factory.hpp:77] Creating layer conv_d3b-c
I0428 19:51:37.515671   966 net.cpp:100] Creating Layer conv_d3b-c
I0428 19:51:37.515674   966 net.cpp:434] conv_d3b-c <- d3b
I0428 19:51:37.515678   966 net.cpp:408] conv_d3b-c -> d3c
I0428 19:51:37.530714   966 net.cpp:150] Setting up conv_d3b-c
I0428 19:51:37.530725   966 net.cpp:157] Top shape: 2 512 46 46 (2166784)
I0428 19:51:37.530743   966 net.cpp:165] Memory required for data: 1669463680
I0428 19:51:37.530748   966 layer_factory.hpp:77] Creating layer bn_d3c
I0428 19:51:37.530753   966 net.cpp:100] Creating Layer bn_d3c
I0428 19:51:37.530756   966 net.cpp:434] bn_d3c <- d3c
I0428 19:51:37.530762   966 net.cpp:395] bn_d3c -> d3c (in-place)
I0428 19:51:37.530968   966 net.cpp:150] Setting up bn_d3c
I0428 19:51:37.530977   966 net.cpp:157] Top shape: 2 512 46 46 (2166784)
I0428 19:51:37.530978   966 net.cpp:165] Memory required for data: 1678130816
I0428 19:51:37.530985   966 layer_factory.hpp:77] Creating layer sc_d3c
I0428 19:51:37.530992   966 net.cpp:100] Creating Layer sc_d3c
I0428 19:51:37.530993   966 net.cpp:434] sc_d3c <- d3c
I0428 19:51:37.530997   966 net.cpp:395] sc_d3c -> d3c (in-place)
I0428 19:51:37.531056   966 layer_factory.hpp:77] Creating layer sc_d3c
I0428 19:51:37.531173   966 net.cpp:150] Setting up sc_d3c
I0428 19:51:37.531180   966 net.cpp:157] Top shape: 2 512 46 46 (2166784)
I0428 19:51:37.531183   966 net.cpp:165] Memory required for data: 1686797952
I0428 19:51:37.531188   966 layer_factory.hpp:77] Creating layer relu_d3c
I0428 19:51:37.531208   966 net.cpp:100] Creating Layer relu_d3c
I0428 19:51:37.531211   966 net.cpp:434] relu_d3c <- d3c
I0428 19:51:37.531216   966 net.cpp:395] relu_d3c -> d3c (in-place)
I0428 19:51:37.531419   966 net.cpp:150] Setting up relu_d3c
I0428 19:51:37.531427   966 net.cpp:157] Top shape: 2 512 46 46 (2166784)
I0428 19:51:37.531445   966 net.cpp:165] Memory required for data: 1695465088
I0428 19:51:37.531448   966 layer_factory.hpp:77] Creating layer d3c_relu_d3c_0_split
I0428 19:51:37.531453   966 net.cpp:100] Creating Layer d3c_relu_d3c_0_split
I0428 19:51:37.531455   966 net.cpp:434] d3c_relu_d3c_0_split <- d3c
I0428 19:51:37.531460   966 net.cpp:408] d3c_relu_d3c_0_split -> d3c_relu_d3c_0_split_0
I0428 19:51:37.531466   966 net.cpp:408] d3c_relu_d3c_0_split -> d3c_relu_d3c_0_split_1
I0428 19:51:37.531507   966 net.cpp:150] Setting up d3c_relu_d3c_0_split
I0428 19:51:37.531513   966 net.cpp:157] Top shape: 2 512 46 46 (2166784)
I0428 19:51:37.531517   966 net.cpp:157] Top shape: 2 512 46 46 (2166784)
I0428 19:51:37.531518   966 net.cpp:165] Memory required for data: 1712799360
I0428 19:51:37.531522   966 layer_factory.hpp:77] Creating layer pool_d3c-4a
I0428 19:51:37.531529   966 net.cpp:100] Creating Layer pool_d3c-4a
I0428 19:51:37.531532   966 net.cpp:434] pool_d3c-4a <- d3c_relu_d3c_0_split_0
I0428 19:51:37.531535   966 net.cpp:408] pool_d3c-4a -> d4a
I0428 19:51:37.531569   966 net.cpp:150] Setting up pool_d3c-4a
I0428 19:51:37.531575   966 net.cpp:157] Top shape: 2 512 23 23 (541696)
I0428 19:51:37.531577   966 net.cpp:165] Memory required for data: 1714966144
I0428 19:51:37.531579   966 layer_factory.hpp:77] Creating layer conv_d4a-b
I0428 19:51:37.531587   966 net.cpp:100] Creating Layer conv_d4a-b
I0428 19:51:37.531590   966 net.cpp:434] conv_d4a-b <- d4a
I0428 19:51:37.531594   966 net.cpp:408] conv_d4a-b -> d4b
I0428 19:51:37.560276   966 net.cpp:150] Setting up conv_d4a-b
I0428 19:51:37.560288   966 net.cpp:157] Top shape: 2 1024 21 21 (903168)
I0428 19:51:37.560290   966 net.cpp:165] Memory required for data: 1718578816
I0428 19:51:37.560295   966 layer_factory.hpp:77] Creating layer bn_d4b
I0428 19:51:37.560302   966 net.cpp:100] Creating Layer bn_d4b
I0428 19:51:37.560322   966 net.cpp:434] bn_d4b <- d4b
I0428 19:51:37.560325   966 net.cpp:395] bn_d4b -> d4b (in-place)
I0428 19:51:37.560508   966 net.cpp:150] Setting up bn_d4b
I0428 19:51:37.560515   966 net.cpp:157] Top shape: 2 1024 21 21 (903168)
I0428 19:51:37.560518   966 net.cpp:165] Memory required for data: 1722191488
I0428 19:51:37.560524   966 layer_factory.hpp:77] Creating layer sc_d4b
I0428 19:51:37.560528   966 net.cpp:100] Creating Layer sc_d4b
I0428 19:51:37.560531   966 net.cpp:434] sc_d4b <- d4b
I0428 19:51:37.560536   966 net.cpp:395] sc_d4b -> d4b (in-place)
I0428 19:51:37.560587   966 layer_factory.hpp:77] Creating layer sc_d4b
I0428 19:51:37.560698   966 net.cpp:150] Setting up sc_d4b
I0428 19:51:37.560705   966 net.cpp:157] Top shape: 2 1024 21 21 (903168)
I0428 19:51:37.560708   966 net.cpp:165] Memory required for data: 1725804160
I0428 19:51:37.560712   966 layer_factory.hpp:77] Creating layer relu_d4b
I0428 19:51:37.560717   966 net.cpp:100] Creating Layer relu_d4b
I0428 19:51:37.560719   966 net.cpp:434] relu_d4b <- d4b
I0428 19:51:37.560724   966 net.cpp:395] relu_d4b -> d4b (in-place)
I0428 19:51:37.561563   966 net.cpp:150] Setting up relu_d4b
I0428 19:51:37.561574   966 net.cpp:157] Top shape: 2 1024 21 21 (903168)
I0428 19:51:37.561591   966 net.cpp:165] Memory required for data: 1729416832
I0428 19:51:37.561595   966 layer_factory.hpp:77] Creating layer conv_d4b-c
I0428 19:51:37.561604   966 net.cpp:100] Creating Layer conv_d4b-c
I0428 19:51:37.561609   966 net.cpp:434] conv_d4b-c <- d4b
I0428 19:51:37.561614   966 net.cpp:408] conv_d4b-c -> d4c
I0428 19:51:37.619989   966 net.cpp:150] Setting up conv_d4b-c
I0428 19:51:37.620010   966 net.cpp:157] Top shape: 2 1024 19 19 (739328)
I0428 19:51:37.620028   966 net.cpp:165] Memory required for data: 1732374144
I0428 19:51:37.620045   966 layer_factory.hpp:77] Creating layer bn_d4c
I0428 19:51:37.620090   966 net.cpp:100] Creating Layer bn_d4c
I0428 19:51:37.620095   966 net.cpp:434] bn_d4c <- d4c
I0428 19:51:37.620115   966 net.cpp:395] bn_d4c -> d4c (in-place)
I0428 19:51:37.620316   966 net.cpp:150] Setting up bn_d4c
I0428 19:51:37.620323   966 net.cpp:157] Top shape: 2 1024 19 19 (739328)
I0428 19:51:37.620326   966 net.cpp:165] Memory required for data: 1735331456
I0428 19:51:37.620332   966 layer_factory.hpp:77] Creating layer sc_d4c
I0428 19:51:37.620338   966 net.cpp:100] Creating Layer sc_d4c
I0428 19:51:37.620342   966 net.cpp:434] sc_d4c <- d4c
I0428 19:51:37.620347   966 net.cpp:395] sc_d4c -> d4c (in-place)
I0428 19:51:37.620419   966 layer_factory.hpp:77] Creating layer sc_d4c
I0428 19:51:37.620522   966 net.cpp:150] Setting up sc_d4c
I0428 19:51:37.620529   966 net.cpp:157] Top shape: 2 1024 19 19 (739328)
I0428 19:51:37.620532   966 net.cpp:165] Memory required for data: 1738288768
I0428 19:51:37.620537   966 layer_factory.hpp:77] Creating layer relu_d4c
I0428 19:51:37.620540   966 net.cpp:100] Creating Layer relu_d4c
I0428 19:51:37.620544   966 net.cpp:434] relu_d4c <- d4c
I0428 19:51:37.620548   966 net.cpp:395] relu_d4c -> d4c (in-place)
I0428 19:51:37.620756   966 net.cpp:150] Setting up relu_d4c
I0428 19:51:37.620764   966 net.cpp:157] Top shape: 2 1024 19 19 (739328)
I0428 19:51:37.620782   966 net.cpp:165] Memory required for data: 1741246080
I0428 19:51:37.620784   966 layer_factory.hpp:77] Creating layer upconv_d4c_u3a
I0428 19:51:37.620806   966 net.cpp:100] Creating Layer upconv_d4c_u3a
I0428 19:51:37.620810   966 net.cpp:434] upconv_d4c_u3a <- d4c
I0428 19:51:37.620815   966 net.cpp:408] upconv_d4c_u3a -> u3a
I0428 19:51:37.633846   966 net.cpp:150] Setting up upconv_d4c_u3a
I0428 19:51:37.633858   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.633860   966 net.cpp:165] Memory required for data: 1747160704
I0428 19:51:37.633882   966 layer_factory.hpp:77] Creating layer bn_u3a
I0428 19:51:37.633888   966 net.cpp:100] Creating Layer bn_u3a
I0428 19:51:37.633890   966 net.cpp:434] bn_u3a <- u3a
I0428 19:51:37.633894   966 net.cpp:395] bn_u3a -> u3a (in-place)
I0428 19:51:37.634119   966 net.cpp:150] Setting up bn_u3a
I0428 19:51:37.634129   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.634131   966 net.cpp:165] Memory required for data: 1753075328
I0428 19:51:37.634136   966 layer_factory.hpp:77] Creating layer sc_u3a
I0428 19:51:37.634142   966 net.cpp:100] Creating Layer sc_u3a
I0428 19:51:37.634160   966 net.cpp:434] sc_u3a <- u3a
I0428 19:51:37.634166   966 net.cpp:395] sc_u3a -> u3a (in-place)
I0428 19:51:37.634202   966 layer_factory.hpp:77] Creating layer sc_u3a
I0428 19:51:37.634315   966 net.cpp:150] Setting up sc_u3a
I0428 19:51:37.634323   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.634325   966 net.cpp:165] Memory required for data: 1758989952
I0428 19:51:37.634330   966 layer_factory.hpp:77] Creating layer relu_u3a
I0428 19:51:37.634335   966 net.cpp:100] Creating Layer relu_u3a
I0428 19:51:37.634338   966 net.cpp:434] relu_u3a <- u3a
I0428 19:51:37.634342   966 net.cpp:395] relu_u3a -> u3a (in-place)
I0428 19:51:37.634534   966 net.cpp:150] Setting up relu_u3a
I0428 19:51:37.634543   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.634546   966 net.cpp:165] Memory required for data: 1764904576
I0428 19:51:37.634548   966 layer_factory.hpp:77] Creating layer u3a_relu_u3a_0_split
I0428 19:51:37.634553   966 net.cpp:100] Creating Layer u3a_relu_u3a_0_split
I0428 19:51:37.634572   966 net.cpp:434] u3a_relu_u3a_0_split <- u3a
I0428 19:51:37.634582   966 net.cpp:408] u3a_relu_u3a_0_split -> u3a_relu_u3a_0_split_0
I0428 19:51:37.634588   966 net.cpp:408] u3a_relu_u3a_0_split -> u3a_relu_u3a_0_split_1
I0428 19:51:37.634627   966 net.cpp:150] Setting up u3a_relu_u3a_0_split
I0428 19:51:37.634635   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.634639   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.634641   966 net.cpp:165] Memory required for data: 1776733824
I0428 19:51:37.634656   966 layer_factory.hpp:77] Creating layer crop_d3c-d3cc
I0428 19:51:37.634663   966 net.cpp:100] Creating Layer crop_d3c-d3cc
I0428 19:51:37.634666   966 net.cpp:434] crop_d3c-d3cc <- d3c_relu_d3c_0_split_1
I0428 19:51:37.634670   966 net.cpp:434] crop_d3c-d3cc <- u3a_relu_u3a_0_split_0
I0428 19:51:37.634675   966 net.cpp:408] crop_d3c-d3cc -> d3cc
I0428 19:51:37.634699   966 net.cpp:150] Setting up crop_d3c-d3cc
I0428 19:51:37.634707   966 net.cpp:157] Top shape: 2 512 38 38 (1478656)
I0428 19:51:37.634709   966 net.cpp:165] Memory required for data: 1782648448
I0428 19:51:37.634711   966 layer_factory.hpp:77] Creating layer concat_d3cc_u3a-b
I0428 19:51:37.634717   966 net.cpp:100] Creating Layer concat_d3cc_u3a-b
I0428 19:51:37.634719   966 net.cpp:434] concat_d3cc_u3a-b <- u3a_relu_u3a_0_split_1
I0428 19:51:37.634722   966 net.cpp:434] concat_d3cc_u3a-b <- d3cc
I0428 19:51:37.634727   966 net.cpp:408] concat_d3cc_u3a-b -> u3b
I0428 19:51:37.634748   966 net.cpp:150] Setting up concat_d3cc_u3a-b
I0428 19:51:37.634769   966 net.cpp:157] Top shape: 2 1024 38 38 (2957312)
I0428 19:51:37.634771   966 net.cpp:165] Memory required for data: 1794477696
I0428 19:51:37.634773   966 layer_factory.hpp:77] Creating layer conv_u3b-c
I0428 19:51:37.634783   966 net.cpp:100] Creating Layer conv_u3b-c
I0428 19:51:37.634785   966 net.cpp:434] conv_u3b-c <- u3b
I0428 19:51:37.634790   966 net.cpp:408] conv_u3b-c -> u3c
I0428 19:51:37.665745   966 net.cpp:150] Setting up conv_u3b-c
I0428 19:51:37.665761   966 net.cpp:157] Top shape: 2 512 36 36 (1327104)
I0428 19:51:37.665763   966 net.cpp:165] Memory required for data: 1799786112
I0428 19:51:37.665769   966 layer_factory.hpp:77] Creating layer bn_u3c
I0428 19:51:37.665777   966 net.cpp:100] Creating Layer bn_u3c
I0428 19:51:37.665781   966 net.cpp:434] bn_u3c <- u3c
I0428 19:51:37.665786   966 net.cpp:395] bn_u3c -> u3c (in-place)
I0428 19:51:37.665993   966 net.cpp:150] Setting up bn_u3c
I0428 19:51:37.666002   966 net.cpp:157] Top shape: 2 512 36 36 (1327104)
I0428 19:51:37.666004   966 net.cpp:165] Memory required for data: 1805094528
I0428 19:51:37.666010   966 layer_factory.hpp:77] Creating layer sc_u3c
I0428 19:51:37.666015   966 net.cpp:100] Creating Layer sc_u3c
I0428 19:51:37.666018   966 net.cpp:434] sc_u3c <- u3c
I0428 19:51:37.666024   966 net.cpp:395] sc_u3c -> u3c (in-place)
I0428 19:51:37.666057   966 layer_factory.hpp:77] Creating layer sc_u3c
I0428 19:51:37.666198   966 net.cpp:150] Setting up sc_u3c
I0428 19:51:37.666205   966 net.cpp:157] Top shape: 2 512 36 36 (1327104)
I0428 19:51:37.666208   966 net.cpp:165] Memory required for data: 1810402944
I0428 19:51:37.666213   966 layer_factory.hpp:77] Creating layer relu_u3c
I0428 19:51:37.666218   966 net.cpp:100] Creating Layer relu_u3c
I0428 19:51:37.666220   966 net.cpp:434] relu_u3c <- u3c
I0428 19:51:37.666224   966 net.cpp:395] relu_u3c -> u3c (in-place)
I0428 19:51:37.666416   966 net.cpp:150] Setting up relu_u3c
I0428 19:51:37.666426   966 net.cpp:157] Top shape: 2 512 36 36 (1327104)
I0428 19:51:37.666429   966 net.cpp:165] Memory required for data: 1815711360
I0428 19:51:37.666431   966 layer_factory.hpp:77] Creating layer conv_u3c-d
I0428 19:51:37.666458   966 net.cpp:100] Creating Layer conv_u3c-d
I0428 19:51:37.666463   966 net.cpp:434] conv_u3c-d <- u3c
I0428 19:51:37.666484   966 net.cpp:408] conv_u3c-d -> u3d
I0428 19:51:37.681401   966 net.cpp:150] Setting up conv_u3c-d
I0428 19:51:37.681414   966 net.cpp:157] Top shape: 2 512 34 34 (1183744)
I0428 19:51:37.681416   966 net.cpp:165] Memory required for data: 1820446336
I0428 19:51:37.681421   966 layer_factory.hpp:77] Creating layer bn_u3d
I0428 19:51:37.681432   966 net.cpp:100] Creating Layer bn_u3d
I0428 19:51:37.681438   966 net.cpp:434] bn_u3d <- u3d
I0428 19:51:37.681459   966 net.cpp:395] bn_u3d -> u3d (in-place)
I0428 19:51:37.681663   966 net.cpp:150] Setting up bn_u3d
I0428 19:51:37.681671   966 net.cpp:157] Top shape: 2 512 34 34 (1183744)
I0428 19:51:37.681674   966 net.cpp:165] Memory required for data: 1825181312
I0428 19:51:37.681696   966 layer_factory.hpp:77] Creating layer sc_u3d
I0428 19:51:37.681704   966 net.cpp:100] Creating Layer sc_u3d
I0428 19:51:37.681709   966 net.cpp:434] sc_u3d <- u3d
I0428 19:51:37.681713   966 net.cpp:395] sc_u3d -> u3d (in-place)
I0428 19:51:37.681749   966 layer_factory.hpp:77] Creating layer sc_u3d
I0428 19:51:37.681874   966 net.cpp:150] Setting up sc_u3d
I0428 19:51:37.681881   966 net.cpp:157] Top shape: 2 512 34 34 (1183744)
I0428 19:51:37.681884   966 net.cpp:165] Memory required for data: 1829916288
I0428 19:51:37.681888   966 layer_factory.hpp:77] Creating layer relu_u3d
I0428 19:51:37.681895   966 net.cpp:100] Creating Layer relu_u3d
I0428 19:51:37.681897   966 net.cpp:434] relu_u3d <- u3d
I0428 19:51:37.681901   966 net.cpp:395] relu_u3d -> u3d (in-place)
I0428 19:51:37.682777   966 net.cpp:150] Setting up relu_u3d
I0428 19:51:37.682791   966 net.cpp:157] Top shape: 2 512 34 34 (1183744)
I0428 19:51:37.682795   966 net.cpp:165] Memory required for data: 1834651264
I0428 19:51:37.682797   966 layer_factory.hpp:77] Creating layer upconv_u3d_u2a
I0428 19:51:37.682816   966 net.cpp:100] Creating Layer upconv_u3d_u2a
I0428 19:51:37.682818   966 net.cpp:434] upconv_u3d_u2a <- u3d
I0428 19:51:37.682824   966 net.cpp:408] upconv_u3d_u2a -> u2a
I0428 19:51:37.686913   966 net.cpp:150] Setting up upconv_u3d_u2a
I0428 19:51:37.686926   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.686928   966 net.cpp:165] Memory required for data: 1844121216
I0428 19:51:37.686950   966 layer_factory.hpp:77] Creating layer bn_u2a
I0428 19:51:37.686956   966 net.cpp:100] Creating Layer bn_u2a
I0428 19:51:37.686959   966 net.cpp:434] bn_u2a <- u2a
I0428 19:51:37.686964   966 net.cpp:395] bn_u2a -> u2a (in-place)
I0428 19:51:37.687186   966 net.cpp:150] Setting up bn_u2a
I0428 19:51:37.687196   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.687198   966 net.cpp:165] Memory required for data: 1853591168
I0428 19:51:37.687203   966 layer_factory.hpp:77] Creating layer sc_u2a
I0428 19:51:37.687209   966 net.cpp:100] Creating Layer sc_u2a
I0428 19:51:37.687212   966 net.cpp:434] sc_u2a <- u2a
I0428 19:51:37.687216   966 net.cpp:395] sc_u2a -> u2a (in-place)
I0428 19:51:37.687268   966 layer_factory.hpp:77] Creating layer sc_u2a
I0428 19:51:37.687384   966 net.cpp:150] Setting up sc_u2a
I0428 19:51:37.687391   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.687393   966 net.cpp:165] Memory required for data: 1863061120
I0428 19:51:37.687398   966 layer_factory.hpp:77] Creating layer relu_u2a
I0428 19:51:37.687403   966 net.cpp:100] Creating Layer relu_u2a
I0428 19:51:37.687405   966 net.cpp:434] relu_u2a <- u2a
I0428 19:51:37.687410   966 net.cpp:395] relu_u2a -> u2a (in-place)
I0428 19:51:37.687589   966 net.cpp:150] Setting up relu_u2a
I0428 19:51:37.687599   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.687602   966 net.cpp:165] Memory required for data: 1872531072
I0428 19:51:37.687604   966 layer_factory.hpp:77] Creating layer u2a_relu_u2a_0_split
I0428 19:51:37.687609   966 net.cpp:100] Creating Layer u2a_relu_u2a_0_split
I0428 19:51:37.687628   966 net.cpp:434] u2a_relu_u2a_0_split <- u2a
I0428 19:51:37.687633   966 net.cpp:408] u2a_relu_u2a_0_split -> u2a_relu_u2a_0_split_0
I0428 19:51:37.687654   966 net.cpp:408] u2a_relu_u2a_0_split -> u2a_relu_u2a_0_split_1
I0428 19:51:37.687693   966 net.cpp:150] Setting up u2a_relu_u2a_0_split
I0428 19:51:37.687700   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.687705   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.687706   966 net.cpp:165] Memory required for data: 1891470976
I0428 19:51:37.687708   966 layer_factory.hpp:77] Creating layer crop_d2c-d2cc
I0428 19:51:37.687716   966 net.cpp:100] Creating Layer crop_d2c-d2cc
I0428 19:51:37.687717   966 net.cpp:434] crop_d2c-d2cc <- d2c_relu_d2c_0_split_1
I0428 19:51:37.687721   966 net.cpp:434] crop_d2c-d2cc <- u2a_relu_u2a_0_split_0
I0428 19:51:37.687726   966 net.cpp:408] crop_d2c-d2cc -> d2cc
I0428 19:51:37.687762   966 net.cpp:150] Setting up crop_d2c-d2cc
I0428 19:51:37.687768   966 net.cpp:157] Top shape: 2 256 68 68 (2367488)
I0428 19:51:37.687770   966 net.cpp:165] Memory required for data: 1900940928
I0428 19:51:37.687772   966 layer_factory.hpp:77] Creating layer concat_d2cc_u2a-b
I0428 19:51:37.687777   966 net.cpp:100] Creating Layer concat_d2cc_u2a-b
I0428 19:51:37.687779   966 net.cpp:434] concat_d2cc_u2a-b <- u2a_relu_u2a_0_split_1
I0428 19:51:37.687783   966 net.cpp:434] concat_d2cc_u2a-b <- d2cc
I0428 19:51:37.687786   966 net.cpp:408] concat_d2cc_u2a-b -> u2b
I0428 19:51:37.687806   966 net.cpp:150] Setting up concat_d2cc_u2a-b
I0428 19:51:37.687810   966 net.cpp:157] Top shape: 2 512 68 68 (4734976)
I0428 19:51:37.687813   966 net.cpp:165] Memory required for data: 1919880832
I0428 19:51:37.687814   966 layer_factory.hpp:77] Creating layer conv_u2b-c
I0428 19:51:37.687824   966 net.cpp:100] Creating Layer conv_u2b-c
I0428 19:51:37.687826   966 net.cpp:434] conv_u2b-c <- u2b
I0428 19:51:37.687831   966 net.cpp:408] conv_u2b-c -> u2c
I0428 19:51:37.695958   966 net.cpp:150] Setting up conv_u2b-c
I0428 19:51:37.695971   966 net.cpp:157] Top shape: 2 256 66 66 (2230272)
I0428 19:51:37.695973   966 net.cpp:165] Memory required for data: 1928801920
I0428 19:51:37.695994   966 layer_factory.hpp:77] Creating layer bn_u2c
I0428 19:51:37.696002   966 net.cpp:100] Creating Layer bn_u2c
I0428 19:51:37.696007   966 net.cpp:434] bn_u2c <- u2c
I0428 19:51:37.696010   966 net.cpp:395] bn_u2c -> u2c (in-place)
I0428 19:51:37.696229   966 net.cpp:150] Setting up bn_u2c
I0428 19:51:37.696238   966 net.cpp:157] Top shape: 2 256 66 66 (2230272)
I0428 19:51:37.696240   966 net.cpp:165] Memory required for data: 1937723008
I0428 19:51:37.696246   966 layer_factory.hpp:77] Creating layer sc_u2c
I0428 19:51:37.696252   966 net.cpp:100] Creating Layer sc_u2c
I0428 19:51:37.696255   966 net.cpp:434] sc_u2c <- u2c
I0428 19:51:37.696260   966 net.cpp:395] sc_u2c -> u2c (in-place)
I0428 19:51:37.696297   966 layer_factory.hpp:77] Creating layer sc_u2c
I0428 19:51:37.696416   966 net.cpp:150] Setting up sc_u2c
I0428 19:51:37.696424   966 net.cpp:157] Top shape: 2 256 66 66 (2230272)
I0428 19:51:37.696426   966 net.cpp:165] Memory required for data: 1946644096
I0428 19:51:37.696430   966 layer_factory.hpp:77] Creating layer relu_u2c
I0428 19:51:37.696435   966 net.cpp:100] Creating Layer relu_u2c
I0428 19:51:37.696437   966 net.cpp:434] relu_u2c <- u2c
I0428 19:51:37.696441   966 net.cpp:395] relu_u2c -> u2c (in-place)
I0428 19:51:37.696630   966 net.cpp:150] Setting up relu_u2c
I0428 19:51:37.696638   966 net.cpp:157] Top shape: 2 256 66 66 (2230272)
I0428 19:51:37.696640   966 net.cpp:165] Memory required for data: 1955565184
I0428 19:51:37.696645   966 layer_factory.hpp:77] Creating layer conv_u2c-d
I0428 19:51:37.696667   966 net.cpp:100] Creating Layer conv_u2c-d
I0428 19:51:37.696686   966 net.cpp:434] conv_u2c-d <- u2c
I0428 19:51:37.696691   966 net.cpp:408] conv_u2c-d -> u2d
I0428 19:51:37.701123   966 net.cpp:150] Setting up conv_u2c-d
I0428 19:51:37.701134   966 net.cpp:157] Top shape: 2 256 64 64 (2097152)
I0428 19:51:37.701138   966 net.cpp:165] Memory required for data: 1963953792
I0428 19:51:37.701143   966 layer_factory.hpp:77] Creating layer bn_u2d
I0428 19:51:37.701148   966 net.cpp:100] Creating Layer bn_u2d
I0428 19:51:37.701151   966 net.cpp:434] bn_u2d <- u2d
I0428 19:51:37.701155   966 net.cpp:395] bn_u2d -> u2d (in-place)
I0428 19:51:37.701361   966 net.cpp:150] Setting up bn_u2d
I0428 19:51:37.701370   966 net.cpp:157] Top shape: 2 256 64 64 (2097152)
I0428 19:51:37.701371   966 net.cpp:165] Memory required for data: 1972342400
I0428 19:51:37.701377   966 layer_factory.hpp:77] Creating layer sc_u2d
I0428 19:51:37.701385   966 net.cpp:100] Creating Layer sc_u2d
I0428 19:51:37.701387   966 net.cpp:434] sc_u2d <- u2d
I0428 19:51:37.701390   966 net.cpp:395] sc_u2d -> u2d (in-place)
I0428 19:51:37.701449   966 layer_factory.hpp:77] Creating layer sc_u2d
I0428 19:51:37.701580   966 net.cpp:150] Setting up sc_u2d
I0428 19:51:37.701608   966 net.cpp:157] Top shape: 2 256 64 64 (2097152)
I0428 19:51:37.701611   966 net.cpp:165] Memory required for data: 1980731008
I0428 19:51:37.701617   966 layer_factory.hpp:77] Creating layer relu_u2d
I0428 19:51:37.701622   966 net.cpp:100] Creating Layer relu_u2d
I0428 19:51:37.701624   966 net.cpp:434] relu_u2d <- u2d
I0428 19:51:37.701629   966 net.cpp:395] relu_u2d -> u2d (in-place)
I0428 19:51:37.701818   966 net.cpp:150] Setting up relu_u2d
I0428 19:51:37.701828   966 net.cpp:157] Top shape: 2 256 64 64 (2097152)
I0428 19:51:37.701831   966 net.cpp:165] Memory required for data: 1989119616
I0428 19:51:37.701833   966 layer_factory.hpp:77] Creating layer upconv_u2d_u1a
I0428 19:51:37.701855   966 net.cpp:100] Creating Layer upconv_u2d_u1a
I0428 19:51:37.701860   966 net.cpp:434] upconv_u2d_u1a <- u2d
I0428 19:51:37.701869   966 net.cpp:408] upconv_u2d_u1a -> u1a
I0428 19:51:37.702926   966 net.cpp:150] Setting up upconv_u2d_u1a
I0428 19:51:37.702950   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.702952   966 net.cpp:165] Memory required for data: 2005896832
I0428 19:51:37.702973   966 layer_factory.hpp:77] Creating layer bn_u1a
I0428 19:51:37.702980   966 net.cpp:100] Creating Layer bn_u1a
I0428 19:51:37.702982   966 net.cpp:434] bn_u1a <- u1a
I0428 19:51:37.702986   966 net.cpp:395] bn_u1a -> u1a (in-place)
I0428 19:51:37.703178   966 net.cpp:150] Setting up bn_u1a
I0428 19:51:37.703186   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.703188   966 net.cpp:165] Memory required for data: 2022674048
I0428 19:51:37.703194   966 layer_factory.hpp:77] Creating layer sc_u1a
I0428 19:51:37.703200   966 net.cpp:100] Creating Layer sc_u1a
I0428 19:51:37.703202   966 net.cpp:434] sc_u1a <- u1a
I0428 19:51:37.703207   966 net.cpp:395] sc_u1a -> u1a (in-place)
I0428 19:51:37.703243   966 layer_factory.hpp:77] Creating layer sc_u1a
I0428 19:51:37.703371   966 net.cpp:150] Setting up sc_u1a
I0428 19:51:37.703377   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.703379   966 net.cpp:165] Memory required for data: 2039451264
I0428 19:51:37.703384   966 layer_factory.hpp:77] Creating layer relu_u1a
I0428 19:51:37.703389   966 net.cpp:100] Creating Layer relu_u1a
I0428 19:51:37.703392   966 net.cpp:434] relu_u1a <- u1a
I0428 19:51:37.703397   966 net.cpp:395] relu_u1a -> u1a (in-place)
I0428 19:51:37.704294   966 net.cpp:150] Setting up relu_u1a
I0428 19:51:37.704306   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.704309   966 net.cpp:165] Memory required for data: 2056228480
I0428 19:51:37.704326   966 layer_factory.hpp:77] Creating layer u1a_relu_u1a_0_split
I0428 19:51:37.704332   966 net.cpp:100] Creating Layer u1a_relu_u1a_0_split
I0428 19:51:37.704335   966 net.cpp:434] u1a_relu_u1a_0_split <- u1a
I0428 19:51:37.704342   966 net.cpp:408] u1a_relu_u1a_0_split -> u1a_relu_u1a_0_split_0
I0428 19:51:37.704349   966 net.cpp:408] u1a_relu_u1a_0_split -> u1a_relu_u1a_0_split_1
I0428 19:51:37.704409   966 net.cpp:150] Setting up u1a_relu_u1a_0_split
I0428 19:51:37.704417   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.704421   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.704422   966 net.cpp:165] Memory required for data: 2089782912
I0428 19:51:37.704426   966 layer_factory.hpp:77] Creating layer crop_d1c-d1cc
I0428 19:51:37.704432   966 net.cpp:100] Creating Layer crop_d1c-d1cc
I0428 19:51:37.704434   966 net.cpp:434] crop_d1c-d1cc <- d1c_relu_d1c_0_split_1
I0428 19:51:37.704437   966 net.cpp:434] crop_d1c-d1cc <- u1a_relu_u1a_0_split_0
I0428 19:51:37.704442   966 net.cpp:408] crop_d1c-d1cc -> d1cc
I0428 19:51:37.704465   966 net.cpp:150] Setting up crop_d1c-d1cc
I0428 19:51:37.704469   966 net.cpp:157] Top shape: 2 128 128 128 (4194304)
I0428 19:51:37.704471   966 net.cpp:165] Memory required for data: 2106560128
I0428 19:51:37.704484   966 layer_factory.hpp:77] Creating layer concat_d1cc_u1a-b
I0428 19:51:37.704490   966 net.cpp:100] Creating Layer concat_d1cc_u1a-b
I0428 19:51:37.704493   966 net.cpp:434] concat_d1cc_u1a-b <- u1a_relu_u1a_0_split_1
I0428 19:51:37.704509   966 net.cpp:434] concat_d1cc_u1a-b <- d1cc
I0428 19:51:37.704516   966 net.cpp:408] concat_d1cc_u1a-b -> u1b
I0428 19:51:37.704540   966 net.cpp:150] Setting up concat_d1cc_u1a-b
I0428 19:51:37.704547   966 net.cpp:157] Top shape: 2 256 128 128 (8388608)
I0428 19:51:37.704550   966 net.cpp:165] Memory required for data: 2140114560
I0428 19:51:37.704552   966 layer_factory.hpp:77] Creating layer conv_u1b-c
I0428 19:51:37.704561   966 net.cpp:100] Creating Layer conv_u1b-c
I0428 19:51:37.704565   966 net.cpp:434] conv_u1b-c <- u1b
I0428 19:51:37.704569   966 net.cpp:408] conv_u1b-c -> u1c
I0428 19:51:37.707396   966 net.cpp:150] Setting up conv_u1b-c
I0428 19:51:37.707411   966 net.cpp:157] Top shape: 2 128 126 126 (4064256)
I0428 19:51:37.707412   966 net.cpp:165] Memory required for data: 2156371584
I0428 19:51:37.707418   966 layer_factory.hpp:77] Creating layer bn_u1c
I0428 19:51:37.707423   966 net.cpp:100] Creating Layer bn_u1c
I0428 19:51:37.707427   966 net.cpp:434] bn_u1c <- u1c
I0428 19:51:37.707430   966 net.cpp:395] bn_u1c -> u1c (in-place)
I0428 19:51:37.707664   966 net.cpp:150] Setting up bn_u1c
I0428 19:51:37.707671   966 net.cpp:157] Top shape: 2 128 126 126 (4064256)
I0428 19:51:37.707674   966 net.cpp:165] Memory required for data: 2172628608
I0428 19:51:37.707680   966 layer_factory.hpp:77] Creating layer sc_u1c
I0428 19:51:37.707685   966 net.cpp:100] Creating Layer sc_u1c
I0428 19:51:37.707689   966 net.cpp:434] sc_u1c <- u1c
I0428 19:51:37.707693   966 net.cpp:395] sc_u1c -> u1c (in-place)
I0428 19:51:37.707733   966 layer_factory.hpp:77] Creating layer sc_u1c
I0428 19:51:37.707882   966 net.cpp:150] Setting up sc_u1c
I0428 19:51:37.707890   966 net.cpp:157] Top shape: 2 128 126 126 (4064256)
I0428 19:51:37.707893   966 net.cpp:165] Memory required for data: 2188885632
I0428 19:51:37.707898   966 layer_factory.hpp:77] Creating layer relu_u1c
I0428 19:51:37.707902   966 net.cpp:100] Creating Layer relu_u1c
I0428 19:51:37.707906   966 net.cpp:434] relu_u1c <- u1c
I0428 19:51:37.707911   966 net.cpp:395] relu_u1c -> u1c (in-place)
I0428 19:51:37.708104   966 net.cpp:150] Setting up relu_u1c
I0428 19:51:37.708113   966 net.cpp:157] Top shape: 2 128 126 126 (4064256)
I0428 19:51:37.708115   966 net.cpp:165] Memory required for data: 2205142656
I0428 19:51:37.708119   966 layer_factory.hpp:77] Creating layer conv_u1c-d
I0428 19:51:37.708127   966 net.cpp:100] Creating Layer conv_u1c-d
I0428 19:51:37.708130   966 net.cpp:434] conv_u1c-d <- u1c
I0428 19:51:37.708135   966 net.cpp:408] conv_u1c-d -> u1d
I0428 19:51:37.709348   966 net.cpp:150] Setting up conv_u1c-d
I0428 19:51:37.709357   966 net.cpp:157] Top shape: 2 128 124 124 (3936256)
I0428 19:51:37.709358   966 net.cpp:165] Memory required for data: 2220887680
I0428 19:51:37.709363   966 layer_factory.hpp:77] Creating layer bn_u1d
I0428 19:51:37.709368   966 net.cpp:100] Creating Layer bn_u1d
I0428 19:51:37.709372   966 net.cpp:434] bn_u1d <- u1d
I0428 19:51:37.709375   966 net.cpp:395] bn_u1d -> u1d (in-place)
I0428 19:51:37.709594   966 net.cpp:150] Setting up bn_u1d
I0428 19:51:37.709605   966 net.cpp:157] Top shape: 2 128 124 124 (3936256)
I0428 19:51:37.709607   966 net.cpp:165] Memory required for data: 2236632704
I0428 19:51:37.709631   966 layer_factory.hpp:77] Creating layer sc_u1d
I0428 19:51:37.709640   966 net.cpp:100] Creating Layer sc_u1d
I0428 19:51:37.709642   966 net.cpp:434] sc_u1d <- u1d
I0428 19:51:37.709648   966 net.cpp:395] sc_u1d -> u1d (in-place)
I0428 19:51:37.709689   966 layer_factory.hpp:77] Creating layer sc_u1d
I0428 19:51:37.709820   966 net.cpp:150] Setting up sc_u1d
I0428 19:51:37.709843   966 net.cpp:157] Top shape: 2 128 124 124 (3936256)
I0428 19:51:37.709846   966 net.cpp:165] Memory required for data: 2252377728
I0428 19:51:37.709851   966 layer_factory.hpp:77] Creating layer relu_u1d
I0428 19:51:37.709854   966 net.cpp:100] Creating Layer relu_u1d
I0428 19:51:37.709857   966 net.cpp:434] relu_u1d <- u1d
I0428 19:51:37.709861   966 net.cpp:395] relu_u1d -> u1d (in-place)
I0428 19:51:37.710073   966 net.cpp:150] Setting up relu_u1d
I0428 19:51:37.710083   966 net.cpp:157] Top shape: 2 128 124 124 (3936256)
I0428 19:51:37.710086   966 net.cpp:165] Memory required for data: 2268122752
I0428 19:51:37.710088   966 layer_factory.hpp:77] Creating layer upconv_u1d_u0a
I0428 19:51:37.710096   966 net.cpp:100] Creating Layer upconv_u1d_u0a
I0428 19:51:37.710098   966 net.cpp:434] upconv_u1d_u0a <- u1d
I0428 19:51:37.710105   966 net.cpp:408] upconv_u1d_u0a -> u0a
I0428 19:51:37.710819   966 net.cpp:150] Setting up upconv_u1d_u0a
I0428 19:51:37.710826   966 net.cpp:157] Top shape: 2 128 248 248 (15745024)
I0428 19:51:37.710829   966 net.cpp:165] Memory required for data: 2331102848
I0428 19:51:37.710834   966 layer_factory.hpp:77] Creating layer bn_u0a
I0428 19:51:37.710839   966 net.cpp:100] Creating Layer bn_u0a
I0428 19:51:37.710841   966 net.cpp:434] bn_u0a <- u0a
I0428 19:51:37.710846   966 net.cpp:395] bn_u0a -> u0a (in-place)
I0428 19:51:37.711916   966 net.cpp:150] Setting up bn_u0a
I0428 19:51:37.711928   966 net.cpp:157] Top shape: 2 128 248 248 (15745024)
I0428 19:51:37.711946   966 net.cpp:165] Memory required for data: 2394082944
I0428 19:51:37.711953   966 layer_factory.hpp:77] Creating layer sc_u0a
I0428 19:51:37.711962   966 net.cpp:100] Creating Layer sc_u0a
I0428 19:51:37.711966   966 net.cpp:434] sc_u0a <- u0a
I0428 19:51:37.711969   966 net.cpp:395] sc_u0a -> u0a (in-place)
I0428 19:51:37.712013   966 layer_factory.hpp:77] Creating layer sc_u0a
I0428 19:51:37.712210   966 net.cpp:150] Setting up sc_u0a
I0428 19:51:37.712218   966 net.cpp:157] Top shape: 2 128 248 248 (15745024)
I0428 19:51:37.712220   966 net.cpp:165] Memory required for data: 2457063040
I0428 19:51:37.712224   966 layer_factory.hpp:77] Creating layer relu_u0a
I0428 19:51:37.712230   966 net.cpp:100] Creating Layer relu_u0a
I0428 19:51:37.712232   966 net.cpp:434] relu_u0a <- u0a
I0428 19:51:37.712237   966 net.cpp:395] relu_u0a -> u0a (in-place)
I0428 19:51:37.712435   966 net.cpp:150] Setting up relu_u0a
I0428 19:51:37.712443   966 net.cpp:157] Top shape: 2 128 248 248 (15745024)
I0428 19:51:37.712446   966 net.cpp:165] Memory required for data: 2520043136
I0428 19:51:37.712448   966 layer_factory.hpp:77] Creating layer u0a_relu_u0a_0_split
I0428 19:51:37.712455   966 net.cpp:100] Creating Layer u0a_relu_u0a_0_split
I0428 19:51:37.712457   966 net.cpp:434] u0a_relu_u0a_0_split <- u0a
I0428 19:51:37.712461   966 net.cpp:408] u0a_relu_u0a_0_split -> u0a_relu_u0a_0_split_0
I0428 19:51:37.712468   966 net.cpp:408] u0a_relu_u0a_0_split -> u0a_relu_u0a_0_split_1
I0428 19:51:37.712525   966 net.cpp:150] Setting up u0a_relu_u0a_0_split
I0428 19:51:37.712533   966 net.cpp:157] Top shape: 2 128 248 248 (15745024)
I0428 19:51:37.712537   966 net.cpp:157] Top shape: 2 128 248 248 (15745024)
I0428 19:51:37.712538   966 net.cpp:165] Memory required for data: 2646003328
I0428 19:51:37.712541   966 layer_factory.hpp:77] Creating layer crop_d0c-d0cc
I0428 19:51:37.712549   966 net.cpp:100] Creating Layer crop_d0c-d0cc
I0428 19:51:37.712553   966 net.cpp:434] crop_d0c-d0cc <- d0c_relu_d0c_0_split_1
I0428 19:51:37.712555   966 net.cpp:434] crop_d0c-d0cc <- u0a_relu_u0a_0_split_0
I0428 19:51:37.712559   966 net.cpp:408] crop_d0c-d0cc -> d0cc
I0428 19:51:37.712584   966 net.cpp:150] Setting up crop_d0c-d0cc
I0428 19:51:37.712591   966 net.cpp:157] Top shape: 2 64 248 248 (7872512)
I0428 19:51:37.712594   966 net.cpp:165] Memory required for data: 2677493376
I0428 19:51:37.712595   966 layer_factory.hpp:77] Creating layer concat_d0cc_u0a-b
I0428 19:51:37.712601   966 net.cpp:100] Creating Layer concat_d0cc_u0a-b
I0428 19:51:37.712604   966 net.cpp:434] concat_d0cc_u0a-b <- u0a_relu_u0a_0_split_1
I0428 19:51:37.712607   966 net.cpp:434] concat_d0cc_u0a-b <- d0cc
I0428 19:51:37.712612   966 net.cpp:408] concat_d0cc_u0a-b -> u0b
I0428 19:51:37.712633   966 net.cpp:150] Setting up concat_d0cc_u0a-b
I0428 19:51:37.712637   966 net.cpp:157] Top shape: 2 192 248 248 (23617536)
I0428 19:51:37.712656   966 net.cpp:165] Memory required for data: 2771963520
I0428 19:51:37.712658   966 layer_factory.hpp:77] Creating layer conv_u0b-c
I0428 19:51:37.712669   966 net.cpp:100] Creating Layer conv_u0b-c
I0428 19:51:37.712671   966 net.cpp:434] conv_u0b-c <- u0b
I0428 19:51:37.712677   966 net.cpp:408] conv_u0b-c -> u0c
I0428 19:51:37.713624   966 net.cpp:150] Setting up conv_u0b-c
I0428 19:51:37.713635   966 net.cpp:157] Top shape: 2 64 246 246 (7746048)
I0428 19:51:37.713639   966 net.cpp:165] Memory required for data: 2802947712
I0428 19:51:37.713644   966 layer_factory.hpp:77] Creating layer bn_u0c
I0428 19:51:37.713647   966 net.cpp:100] Creating Layer bn_u0c
I0428 19:51:37.713651   966 net.cpp:434] bn_u0c <- u0c
I0428 19:51:37.713655   966 net.cpp:395] bn_u0c -> u0c (in-place)
I0428 19:51:37.713892   966 net.cpp:150] Setting up bn_u0c
I0428 19:51:37.713901   966 net.cpp:157] Top shape: 2 64 246 246 (7746048)
I0428 19:51:37.713903   966 net.cpp:165] Memory required for data: 2833931904
I0428 19:51:37.713909   966 layer_factory.hpp:77] Creating layer sc_u0c
I0428 19:51:37.713914   966 net.cpp:100] Creating Layer sc_u0c
I0428 19:51:37.713925   966 net.cpp:434] sc_u0c <- u0c
I0428 19:51:37.713932   966 net.cpp:395] sc_u0c -> u0c (in-place)
I0428 19:51:37.713971   966 layer_factory.hpp:77] Creating layer sc_u0c
I0428 19:51:37.714181   966 net.cpp:150] Setting up sc_u0c
I0428 19:51:37.714188   966 net.cpp:157] Top shape: 2 64 246 246 (7746048)
I0428 19:51:37.714191   966 net.cpp:165] Memory required for data: 2864916096
I0428 19:51:37.714196   966 layer_factory.hpp:77] Creating layer relu_u0c
I0428 19:51:37.714200   966 net.cpp:100] Creating Layer relu_u0c
I0428 19:51:37.714202   966 net.cpp:434] relu_u0c <- u0c
I0428 19:51:37.714208   966 net.cpp:395] relu_u0c -> u0c (in-place)
I0428 19:51:37.715000   966 net.cpp:150] Setting up relu_u0c
I0428 19:51:37.715013   966 net.cpp:157] Top shape: 2 64 246 246 (7746048)
I0428 19:51:37.715030   966 net.cpp:165] Memory required for data: 2895900288
I0428 19:51:37.715034   966 layer_factory.hpp:77] Creating layer conv_u0c-d
I0428 19:51:37.715044   966 net.cpp:100] Creating Layer conv_u0c-d
I0428 19:51:37.715057   966 net.cpp:434] conv_u0c-d <- u0c
I0428 19:51:37.715064   966 net.cpp:408] conv_u0c-d -> u0d
I0428 19:51:37.715576   966 net.cpp:150] Setting up conv_u0c-d
I0428 19:51:37.715585   966 net.cpp:157] Top shape: 2 64 244 244 (7620608)
I0428 19:51:37.715587   966 net.cpp:165] Memory required for data: 2926382720
I0428 19:51:37.715603   966 layer_factory.hpp:77] Creating layer bn_u0d
I0428 19:51:37.715625   966 net.cpp:100] Creating Layer bn_u0d
I0428 19:51:37.715627   966 net.cpp:434] bn_u0d <- u0d
I0428 19:51:37.715631   966 net.cpp:395] bn_u0d -> u0d (in-place)
I0428 19:51:37.716673   966 net.cpp:150] Setting up bn_u0d
I0428 19:51:37.716686   966 net.cpp:157] Top shape: 2 64 244 244 (7620608)
I0428 19:51:37.716688   966 net.cpp:165] Memory required for data: 2956865152
I0428 19:51:37.716711   966 layer_factory.hpp:77] Creating layer sc_u0d
I0428 19:51:37.716718   966 net.cpp:100] Creating Layer sc_u0d
I0428 19:51:37.716722   966 net.cpp:434] sc_u0d <- u0d
I0428 19:51:37.716727   966 net.cpp:395] sc_u0d -> u0d (in-place)
I0428 19:51:37.716799   966 layer_factory.hpp:77] Creating layer sc_u0d
I0428 19:51:37.717000   966 net.cpp:150] Setting up sc_u0d
I0428 19:51:37.717007   966 net.cpp:157] Top shape: 2 64 244 244 (7620608)
I0428 19:51:37.717010   966 net.cpp:165] Memory required for data: 2987347584
I0428 19:51:37.717015   966 layer_factory.hpp:77] Creating layer relu_u0d
I0428 19:51:37.717020   966 net.cpp:100] Creating Layer relu_u0d
I0428 19:51:37.717022   966 net.cpp:434] relu_u0d <- u0d
I0428 19:51:37.717027   966 net.cpp:395] relu_u0d -> u0d (in-place)
I0428 19:51:37.717211   966 net.cpp:150] Setting up relu_u0d
I0428 19:51:37.717221   966 net.cpp:157] Top shape: 2 64 244 244 (7620608)
I0428 19:51:37.717223   966 net.cpp:165] Memory required for data: 3017830016
I0428 19:51:37.717226   966 layer_factory.hpp:77] Creating layer conv_u0d-score
I0428 19:51:37.717234   966 net.cpp:100] Creating Layer conv_u0d-score
I0428 19:51:37.717252   966 net.cpp:434] conv_u0d-score <- u0d
I0428 19:51:37.717259   966 net.cpp:408] conv_u0d-score -> score
I0428 19:51:37.717615   966 net.cpp:150] Setting up conv_u0d-score
I0428 19:51:37.717625   966 net.cpp:157] Top shape: 2 3 244 244 (357216)
I0428 19:51:37.717628   966 net.cpp:165] Memory required for data: 3019258880
I0428 19:51:37.717633   966 layer_factory.hpp:77] Creating layer score_conv_u0d-score_0_split
I0428 19:51:37.717638   966 net.cpp:100] Creating Layer score_conv_u0d-score_0_split
I0428 19:51:37.717640   966 net.cpp:434] score_conv_u0d-score_0_split <- score
I0428 19:51:37.717646   966 net.cpp:408] score_conv_u0d-score_0_split -> score_conv_u0d-score_0_split_0
I0428 19:51:37.717653   966 net.cpp:408] score_conv_u0d-score_0_split -> score_conv_u0d-score_0_split_1
I0428 19:51:37.717694   966 net.cpp:150] Setting up score_conv_u0d-score_0_split
I0428 19:51:37.717700   966 net.cpp:157] Top shape: 2 3 244 244 (357216)
I0428 19:51:37.717703   966 net.cpp:157] Top shape: 2 3 244 244 (357216)
I0428 19:51:37.717706   966 net.cpp:165] Memory required for data: 3022116608
I0428 19:51:37.717708   966 layer_factory.hpp:77] Creating layer loss
I0428 19:51:37.717713   966 net.cpp:100] Creating Layer loss
I0428 19:51:37.717715   966 net.cpp:434] loss <- score_conv_u0d-score_0_split_0
I0428 19:51:37.717720   966 net.cpp:434] loss <- label
I0428 19:51:37.717723   966 net.cpp:434] loss <- weights
I0428 19:51:37.717727   966 net.cpp:408] loss -> loss
I0428 19:51:37.717734   966 layer_factory.hpp:77] Creating layer loss
I0428 19:51:37.719089   966 net.cpp:150] Setting up loss
I0428 19:51:37.719101   966 net.cpp:157] Top shape: (1)
I0428 19:51:37.719104   966 net.cpp:160]     with loss weight 1
I0428 19:51:37.719130   966 net.cpp:165] Memory required for data: 3022116612
I0428 19:51:37.719133   966 layer_factory.hpp:77] Creating layer visualize
I0428 19:51:37.719139   966 net.cpp:100] Creating Layer visualize
I0428 19:51:37.719142   966 net.cpp:434] visualize <- score_conv_u0d-score_0_split_1
I0428 19:51:37.719149   966 net.cpp:408] visualize -> visualize_out
I0428 19:51:37.720727   966 net.cpp:150] Setting up visualize
I0428 19:51:37.720742   966 net.cpp:157] Top shape: 2 3 244 244 (357216)
I0428 19:51:37.720760   966 net.cpp:165] Memory required for data: 3023545476
I0428 19:51:37.720763   966 layer_factory.hpp:77] Creating layer fake
I0428 19:51:37.720770   966 net.cpp:100] Creating Layer fake
I0428 19:51:37.720773   966 net.cpp:434] fake <- visualize_out
I0428 19:51:37.720777   966 net.cpp:150] Setting up fake
I0428 19:51:37.720779   966 net.cpp:165] Memory required for data: 3023545476
I0428 19:51:37.720782   966 net.cpp:228] fake does not need backward computation.
I0428 19:51:37.720785   966 net.cpp:228] visualize does not need backward computation.
I0428 19:51:37.720788   966 net.cpp:226] loss needs backward computation.
I0428 19:51:37.720793   966 net.cpp:226] score_conv_u0d-score_0_split needs backward computation.
I0428 19:51:37.720795   966 net.cpp:226] conv_u0d-score needs backward computation.
I0428 19:51:37.720798   966 net.cpp:226] relu_u0d needs backward computation.
I0428 19:51:37.720816   966 net.cpp:226] sc_u0d needs backward computation.
I0428 19:51:37.720818   966 net.cpp:226] bn_u0d needs backward computation.
I0428 19:51:37.720820   966 net.cpp:226] conv_u0c-d needs backward computation.
I0428 19:51:37.720824   966 net.cpp:226] relu_u0c needs backward computation.
I0428 19:51:37.720825   966 net.cpp:226] sc_u0c needs backward computation.
I0428 19:51:37.720827   966 net.cpp:226] bn_u0c needs backward computation.
I0428 19:51:37.720830   966 net.cpp:226] conv_u0b-c needs backward computation.
I0428 19:51:37.720832   966 net.cpp:226] concat_d0cc_u0a-b needs backward computation.
I0428 19:51:37.720835   966 net.cpp:226] crop_d0c-d0cc needs backward computation.
I0428 19:51:37.720839   966 net.cpp:226] u0a_relu_u0a_0_split needs backward computation.
I0428 19:51:37.720841   966 net.cpp:226] relu_u0a needs backward computation.
I0428 19:51:37.720856   966 net.cpp:226] sc_u0a needs backward computation.
I0428 19:51:37.720859   966 net.cpp:226] bn_u0a needs backward computation.
I0428 19:51:37.720860   966 net.cpp:226] upconv_u1d_u0a needs backward computation.
I0428 19:51:37.720863   966 net.cpp:226] relu_u1d needs backward computation.
I0428 19:51:37.720865   966 net.cpp:226] sc_u1d needs backward computation.
I0428 19:51:37.720867   966 net.cpp:226] bn_u1d needs backward computation.
I0428 19:51:37.720870   966 net.cpp:226] conv_u1c-d needs backward computation.
I0428 19:51:37.720872   966 net.cpp:226] relu_u1c needs backward computation.
I0428 19:51:37.720875   966 net.cpp:226] sc_u1c needs backward computation.
I0428 19:51:37.720876   966 net.cpp:226] bn_u1c needs backward computation.
I0428 19:51:37.720878   966 net.cpp:226] conv_u1b-c needs backward computation.
I0428 19:51:37.720881   966 net.cpp:226] concat_d1cc_u1a-b needs backward computation.
I0428 19:51:37.720885   966 net.cpp:226] crop_d1c-d1cc needs backward computation.
I0428 19:51:37.720887   966 net.cpp:226] u1a_relu_u1a_0_split needs backward computation.
I0428 19:51:37.720890   966 net.cpp:226] relu_u1a needs backward computation.
I0428 19:51:37.720893   966 net.cpp:226] sc_u1a needs backward computation.
I0428 19:51:37.720896   966 net.cpp:226] bn_u1a needs backward computation.
I0428 19:51:37.720897   966 net.cpp:226] upconv_u2d_u1a needs backward computation.
I0428 19:51:37.720899   966 net.cpp:226] relu_u2d needs backward computation.
I0428 19:51:37.720902   966 net.cpp:226] sc_u2d needs backward computation.
I0428 19:51:37.720904   966 net.cpp:226] bn_u2d needs backward computation.
I0428 19:51:37.720906   966 net.cpp:226] conv_u2c-d needs backward computation.
I0428 19:51:37.720908   966 net.cpp:226] relu_u2c needs backward computation.
I0428 19:51:37.720911   966 net.cpp:226] sc_u2c needs backward computation.
I0428 19:51:37.720913   966 net.cpp:226] bn_u2c needs backward computation.
I0428 19:51:37.720916   966 net.cpp:226] conv_u2b-c needs backward computation.
I0428 19:51:37.720917   966 net.cpp:226] concat_d2cc_u2a-b needs backward computation.
I0428 19:51:37.720921   966 net.cpp:226] crop_d2c-d2cc needs backward computation.
I0428 19:51:37.720924   966 net.cpp:226] u2a_relu_u2a_0_split needs backward computation.
I0428 19:51:37.720927   966 net.cpp:226] relu_u2a needs backward computation.
I0428 19:51:37.720929   966 net.cpp:226] sc_u2a needs backward computation.
I0428 19:51:37.720932   966 net.cpp:226] bn_u2a needs backward computation.
I0428 19:51:37.720933   966 net.cpp:226] upconv_u3d_u2a needs backward computation.
I0428 19:51:37.720937   966 net.cpp:226] relu_u3d needs backward computation.
I0428 19:51:37.720938   966 net.cpp:226] sc_u3d needs backward computation.
I0428 19:51:37.720940   966 net.cpp:226] bn_u3d needs backward computation.
I0428 19:51:37.720942   966 net.cpp:226] conv_u3c-d needs backward computation.
I0428 19:51:37.720945   966 net.cpp:226] relu_u3c needs backward computation.
I0428 19:51:37.720947   966 net.cpp:226] sc_u3c needs backward computation.
I0428 19:51:37.720950   966 net.cpp:226] bn_u3c needs backward computation.
I0428 19:51:37.720952   966 net.cpp:226] conv_u3b-c needs backward computation.
I0428 19:51:37.720954   966 net.cpp:226] concat_d3cc_u3a-b needs backward computation.
I0428 19:51:37.720957   966 net.cpp:226] crop_d3c-d3cc needs backward computation.
I0428 19:51:37.720962   966 net.cpp:226] u3a_relu_u3a_0_split needs backward computation.
I0428 19:51:37.720963   966 net.cpp:226] relu_u3a needs backward computation.
I0428 19:51:37.720965   966 net.cpp:226] sc_u3a needs backward computation.
I0428 19:51:37.720968   966 net.cpp:226] bn_u3a needs backward computation.
I0428 19:51:37.720970   966 net.cpp:226] upconv_d4c_u3a needs backward computation.
I0428 19:51:37.720973   966 net.cpp:226] relu_d4c needs backward computation.
I0428 19:51:37.720975   966 net.cpp:226] sc_d4c needs backward computation.
I0428 19:51:37.720978   966 net.cpp:226] bn_d4c needs backward computation.
I0428 19:51:37.720980   966 net.cpp:226] conv_d4b-c needs backward computation.
I0428 19:51:37.720988   966 net.cpp:226] relu_d4b needs backward computation.
I0428 19:51:37.720990   966 net.cpp:226] sc_d4b needs backward computation.
I0428 19:51:37.720993   966 net.cpp:226] bn_d4b needs backward computation.
I0428 19:51:37.720995   966 net.cpp:226] conv_d4a-b needs backward computation.
I0428 19:51:37.720999   966 net.cpp:226] pool_d3c-4a needs backward computation.
I0428 19:51:37.721000   966 net.cpp:226] d3c_relu_d3c_0_split needs backward computation.
I0428 19:51:37.721004   966 net.cpp:226] relu_d3c needs backward computation.
I0428 19:51:37.721006   966 net.cpp:226] sc_d3c needs backward computation.
I0428 19:51:37.721009   966 net.cpp:226] bn_d3c needs backward computation.
I0428 19:51:37.721011   966 net.cpp:226] conv_d3b-c needs backward computation.
I0428 19:51:37.721014   966 net.cpp:226] relu_d3b needs backward computation.
I0428 19:51:37.721016   966 net.cpp:226] sc_d3b needs backward computation.
I0428 19:51:37.721019   966 net.cpp:226] bn_d3b needs backward computation.
I0428 19:51:37.721021   966 net.cpp:226] conv_d3a-b needs backward computation.
I0428 19:51:37.721024   966 net.cpp:226] pool_d2c-3a needs backward computation.
I0428 19:51:37.721026   966 net.cpp:226] d2c_relu_d2c_0_split needs backward computation.
I0428 19:51:37.721029   966 net.cpp:226] relu_d2c needs backward computation.
I0428 19:51:37.721031   966 net.cpp:226] sc_d2c needs backward computation.
I0428 19:51:37.721034   966 net.cpp:226] bn_d2c needs backward computation.
I0428 19:51:37.721036   966 net.cpp:226] conv_d2b-c needs backward computation.
I0428 19:51:37.721038   966 net.cpp:226] relu_d2b needs backward computation.
I0428 19:51:37.721042   966 net.cpp:226] sc_d2b needs backward computation.
I0428 19:51:37.721045   966 net.cpp:226] bn_d2b needs backward computation.
I0428 19:51:37.721047   966 net.cpp:226] conv_d2a-b needs backward computation.
I0428 19:51:37.721050   966 net.cpp:226] pool_d1c-2a needs backward computation.
I0428 19:51:37.721052   966 net.cpp:226] d1c_relu_d1c_0_split needs backward computation.
I0428 19:51:37.721055   966 net.cpp:226] relu_d1c needs backward computation.
I0428 19:51:37.721057   966 net.cpp:226] sc_d1c needs backward computation.
I0428 19:51:37.721060   966 net.cpp:226] bn_d1c needs backward computation.
I0428 19:51:37.721061   966 net.cpp:226] conv_d1b-c needs backward computation.
I0428 19:51:37.721065   966 net.cpp:226] relu_d1b needs backward computation.
I0428 19:51:37.721067   966 net.cpp:226] sc_d1b needs backward computation.
I0428 19:51:37.721070   966 net.cpp:226] bn_d1b needs backward computation.
I0428 19:51:37.721071   966 net.cpp:226] conv_d1a-b needs backward computation.
I0428 19:51:37.721073   966 net.cpp:226] pool_d0c-1a needs backward computation.
I0428 19:51:37.721076   966 net.cpp:226] d0c_relu_d0c_0_split needs backward computation.
I0428 19:51:37.721079   966 net.cpp:226] relu_d0c needs backward computation.
I0428 19:51:37.721081   966 net.cpp:226] sc_d0c needs backward computation.
I0428 19:51:37.721084   966 net.cpp:226] bn_d0c needs backward computation.
I0428 19:51:37.721087   966 net.cpp:226] conv_d0b-c needs backward computation.
I0428 19:51:37.721089   966 net.cpp:226] relu_d0b needs backward computation.
I0428 19:51:37.721091   966 net.cpp:226] sc_d0b needs backward computation.
I0428 19:51:37.721094   966 net.cpp:226] bn_d0b needs backward computation.
I0428 19:51:37.721097   966 net.cpp:226] conv_d0a-b needs backward computation.
I0428 19:51:37.721101   966 net.cpp:228] loaddata does not need backward computation.
I0428 19:51:37.721109   966 net.cpp:270] This network produces output loss
I0428 19:51:37.721173   966 net.cpp:283] Network initialization done.
I0428 19:51:37.722096   966 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: ./unet_weighted_batchnorm_3/unet_weighted_batchnorm_3.prototxt
I0428 19:51:37.722108   966 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0428 19:51:37.722112   966 solver.cpp:181] Creating test net (#0) specified by net file: ./unet_weighted_batchnorm_3/unet_weighted_batchnorm_3.prototxt
I0428 19:51:37.722210   966 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer loaddata
I0428 19:51:37.722249   966 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer visualize
I0428 19:51:37.722254   966 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer fake
I0428 19:51:37.722725   966 net.cpp:58] Initializing net from parameters: 
name: "unet_weighted_batchnorm_3"
force_backward: true
state {
  phase: TEST
}
layer {
  name: "loaddata"
  type: "HDF5Data"
  top: "data"
  top: "label"
  top: "weights"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "caffeHDF5_validation_3.txt"
    batch_size: 1
  }
}
layer {
  name: "conv_d0a-b"
  type: "Convolution"
  bottom: "data"
  top: "d0b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d0b"
  type: "BatchNorm"
  bottom: "d0b"
  top: "d0b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d0b"
  type: "Scale"
  bottom: "d0b"
  top: "d0b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d0b"
  type: "ReLU"
  bottom: "d0b"
  top: "d0b"
}
layer {
  name: "conv_d0b-c"
  type: "Convolution"
  bottom: "d0b"
  top: "d0c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d0c"
  type: "BatchNorm"
  bottom: "d0c"
  top: "d0c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d0c"
  type: "Scale"
  bottom: "d0c"
  top: "d0c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d0c"
  type: "ReLU"
  bottom: "d0c"
  top: "d0c"
}
layer {
  name: "pool_d0c-1a"
  type: "Pooling"
  bottom: "d0c"
  top: "d1a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d1a-b"
  type: "Convolution"
  bottom: "d1a"
  top: "d1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d1b"
  type: "BatchNorm"
  bottom: "d1b"
  top: "d1b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d1b"
  type: "Scale"
  bottom: "d1b"
  top: "d1b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d1b"
  type: "ReLU"
  bottom: "d1b"
  top: "d1b"
}
layer {
  name: "conv_d1b-c"
  type: "Convolution"
  bottom: "d1b"
  top: "d1c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d1c"
  type: "BatchNorm"
  bottom: "d1c"
  top: "d1c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d1c"
  type: "Scale"
  bottom: "d1c"
  top: "d1c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d1c"
  type: "ReLU"
  bottom: "d1c"
  top: "d1c"
}
layer {
  name: "pool_d1c-2a"
  type: "Pooling"
  bottom: "d1c"
  top: "d2a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d2a-b"
  type: "Convolution"
  bottom: "d2a"
  top: "d2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d2b"
  type: "BatchNorm"
  bottom: "d2b"
  top: "d2b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d2b"
  type: "Scale"
  bottom: "d2b"
  top: "d2b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d2b"
  type: "ReLU"
  bottom: "d2b"
  top: "d2b"
}
layer {
  name: "conv_d2b-c"
  type: "Convolution"
  bottom: "d2b"
  top: "d2c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d2c"
  type: "BatchNorm"
  bottom: "d2c"
  top: "d2c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d2c"
  type: "Scale"
  bottom: "d2c"
  top: "d2c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d2c"
  type: "ReLU"
  bottom: "d2c"
  top: "d2c"
}
layer {
  name: "pool_d2c-3a"
  type: "Pooling"
  bottom: "d2c"
  top: "d3a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d3a-b"
  type: "Convolution"
  bottom: "d3a"
  top: "d3b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d3b"
  type: "BatchNorm"
  bottom: "d3b"
  top: "d3b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d3b"
  type: "Scale"
  bottom: "d3b"
  top: "d3b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d3b"
  type: "ReLU"
  bottom: "d3b"
  top: "d3b"
}
layer {
  name: "conv_d3b-c"
  type: "Convolution"
  bottom: "d3b"
  top: "d3c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d3c"
  type: "BatchNorm"
  bottom: "d3c"
  top: "d3c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d3c"
  type: "Scale"
  bottom: "d3c"
  top: "d3c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d3c"
  type: "ReLU"
  bottom: "d3c"
  top: "d3c"
}
layer {
  name: "pool_d3c-4a"
  type: "Pooling"
  bottom: "d3c"
  top: "d4a"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv_d4a-b"
  type: "Convolution"
  bottom: "d4a"
  top: "d4b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d4b"
  type: "BatchNorm"
  bottom: "d4b"
  top: "d4b"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d4b"
  type: "Scale"
  bottom: "d4b"
  top: "d4b"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d4b"
  type: "ReLU"
  bottom: "d4b"
  top: "d4b"
}
layer {
  name: "conv_d4b-c"
  type: "Convolution"
  bottom: "d4b"
  top: "d4c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_d4c"
  type: "BatchNorm"
  bottom: "d4c"
  top: "d4c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_d4c"
  type: "Scale"
  bottom: "d4c"
  top: "d4c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d4c"
  type: "ReLU"
  bottom: "d4c"
  top: "d4c"
}
layer {
  name: "upconv_d4c_u3a"
  type: "Deconvolution"
  bottom: "d4c"
  top: "u3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u3a"
  type: "BatchNorm"
  bottom: "u3a"
  top: "u3a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u3a"
  type: "Scale"
  bottom: "u3a"
  top: "u3a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u3a"
  type: "ReLU"
  bottom: "u3a"
  top: "u3a"
}
layer {
  name: "crop_d3c-d3cc"
  type: "Crop"
  bottom: "d3c"
  bottom: "u3a"
  top: "d3cc"
  crop_param {
    axis: 2
    offset: 4
  }
}
layer {
  name: "concat_d3cc_u3a-b"
  type: "Concat"
  bottom: "u3a"
  bottom: "d3cc"
  top: "u3b"
}
layer {
  name: "conv_u3b-c"
  type: "Convolution"
  bottom: "u3b"
  top: "u3c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u3c"
  type: "BatchNorm"
  bottom: "u3c"
  top: "u3c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u3c"
  type: "Scale"
  bottom: "u3c"
  top: "u3c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u3c"
  type: "ReLU"
  bottom: "u3c"
  top: "u3c"
}
layer {
  name: "conv_u3c-d"
  type: "Convolution"
  bottom: "u3c"
  top: "u3d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u3d"
  type: "BatchNorm"
  bottom: "u3d"
  top: "u3d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u3d"
  type: "Scale"
  bottom: "u3d"
  top: "u3d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u3d"
  type: "ReLU"
  bottom: "u3d"
  top: "u3d"
}
layer {
  name: "upconv_u3d_u2a"
  type: "Deconvolution"
  bottom: "u3d"
  top: "u2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u2a"
  type: "BatchNorm"
  bottom: "u2a"
  top: "u2a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u2a"
  type: "Scale"
  bottom: "u2a"
  top: "u2a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u2a"
  type: "ReLU"
  bottom: "u2a"
  top: "u2a"
}
layer {
  name: "crop_d2c-d2cc"
  type: "Crop"
  bottom: "d2c"
  bottom: "u2a"
  top: "d2cc"
  crop_param {
    axis: 2
    offset: 16
  }
}
layer {
  name: "concat_d2cc_u2a-b"
  type: "Concat"
  bottom: "u2a"
  bottom: "d2cc"
  top: "u2b"
}
layer {
  name: "conv_u2b-c"
  type: "Convolution"
  bottom: "u2b"
  top: "u2c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u2c"
  type: "BatchNorm"
  bottom: "u2c"
  top: "u2c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u2c"
  type: "Scale"
  bottom: "u2c"
  top: "u2c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u2c"
  type: "ReLU"
  bottom: "u2c"
  top: "u2c"
}
layer {
  name: "conv_u2c-d"
  type: "Convolution"
  bottom: "u2c"
  top: "u2d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u2d"
  type: "BatchNorm"
  bottom: "u2d"
  top: "u2d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u2d"
  type: "Scale"
  bottom: "u2d"
  top: "u2d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u2d"
  type: "ReLU"
  bottom: "u2d"
  top: "u2d"
}
layer {
  name: "upconv_u2d_u1a"
  type: "Deconvolution"
  bottom: "u2d"
  top: "u1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u1a"
  type: "BatchNorm"
  bottom: "u1a"
  top: "u1a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u1a"
  type: "Scale"
  bottom: "u1a"
  top: "u1a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u1a"
  type: "ReLU"
  bottom: "u1a"
  top: "u1a"
}
layer {
  name: "crop_d1c-d1cc"
  type: "Crop"
  bottom: "d1c"
  bottom: "u1a"
  top: "d1cc"
  crop_param {
    axis: 2
    offset: 40
  }
}
layer {
  name: "concat_d1cc_u1a-b"
  type: "Concat"
  bottom: "u1a"
  bottom: "d1cc"
  top: "u1b"
}
layer {
  name: "conv_u1b-c"
  type: "Convolution"
  bottom: "u1b"
  top: "u1c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u1c"
  type: "BatchNorm"
  bottom: "u1c"
  top: "u1c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u1c"
  type: "Scale"
  bottom: "u1c"
  top: "u1c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u1c"
  type: "ReLU"
  bottom: "u1c"
  top: "u1c"
}
layer {
  name: "conv_u1c-d"
  type: "Convolution"
  bottom: "u1c"
  top: "u1d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u1d"
  type: "BatchNorm"
  bottom: "u1d"
  top: "u1d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u1d"
  type: "Scale"
  bottom: "u1d"
  top: "u1d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u1d"
  type: "ReLU"
  bottom: "u1d"
  top: "u1d"
}
layer {
  name: "upconv_u1d_u0a"
  type: "Deconvolution"
  bottom: "u1d"
  top: "u0a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_u0a"
  type: "BatchNorm"
  bottom: "u0a"
  top: "u0a"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u0a"
  type: "Scale"
  bottom: "u0a"
  top: "u0a"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u0a"
  type: "ReLU"
  bottom: "u0a"
  top: "u0a"
}
layer {
  name: "crop_d0c-d0cc"
  type: "Crop"
  bottom: "d0c"
  bottom: "u0a"
  top: "d0cc"
  crop_param {
    axis: 2
    offset: 88
  }
}
layer {
  name: "concat_d0cc_u0a-b"
  type: "Concat"
  bottom: "u0a"
  bottom: "d0cc"
  top: "u0b"
}
layer {
  name: "conv_u0b-c"
  type: "Convolution"
  bottom: "u0b"
  top: "u0c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u0c"
  type: "BatchNorm"
  bottom: "u0c"
  top: "u0c"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u0c"
  type: "Scale"
  bottom: "u0c"
  top: "u0c"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u0c"
  type: "ReLU"
  bottom: "u0c"
  top: "u0c"
}
layer {
  name: "conv_u0c-d"
  type: "Convolution"
  bottom: "u0c"
  top: "u0d"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "bn_u0d"
  type: "BatchNorm"
  bottom: "u0d"
  top: "u0d"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.999
    eps: 1e-05
  }
}
layer {
  name: "sc_u0d"
  type: "Scale"
  bottom: "u0d"
  top: "u0d"
  scale_param {
    axis: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_u0d"
  type: "ReLU"
  bottom: "u0d"
  top: "u0d"
}
layer {
  name: "conv_u0d-score"
  type: "Convolution"
  bottom: "u0d"
  top: "score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 3
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  bottom: "weights"
  top: "loss"
  loss_weight: 1
}
I0428 19:51:37.723050   966 layer_factory.hpp:77] Creating layer loaddata
I0428 19:51:37.723060   966 net.cpp:100] Creating Layer loaddata
I0428 19:51:37.723063   966 net.cpp:408] loaddata -> data
I0428 19:51:37.723070   966 net.cpp:408] loaddata -> label
I0428 19:51:37.723075   966 net.cpp:408] loaddata -> weights
I0428 19:51:37.723080   966 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: caffeHDF5_validation_3.txt
I0428 19:51:37.738271   966 hdf5_data_layer.cpp:93] Number of HDF5 files: 10
I0428 19:51:40.797610   966 net.cpp:150] Setting up loaddata
I0428 19:51:40.797654   966 net.cpp:157] Top shape: 1 3 428 428 (549552)
I0428 19:51:40.797665   966 net.cpp:157] Top shape: 1 244 244 (59536)
I0428 19:51:40.797672   966 net.cpp:157] Top shape: 1 244 244 (59536)
I0428 19:51:40.797677   966 net.cpp:165] Memory required for data: 2674496
I0428 19:51:40.797685   966 layer_factory.hpp:77] Creating layer conv_d0a-b
I0428 19:51:40.797722   966 net.cpp:100] Creating Layer conv_d0a-b
I0428 19:51:40.797729   966 net.cpp:434] conv_d0a-b <- data
I0428 19:51:40.797740   966 net.cpp:408] conv_d0a-b -> d0b
I0428 19:51:40.800138   966 net.cpp:150] Setting up conv_d0a-b
I0428 19:51:40.800164   966 net.cpp:157] Top shape: 1 64 426 426 (11614464)
I0428 19:51:40.800169   966 net.cpp:165] Memory required for data: 49132352
I0428 19:51:40.800189   966 layer_factory.hpp:77] Creating layer bn_d0b
I0428 19:51:40.800201   966 net.cpp:100] Creating Layer bn_d0b
I0428 19:51:40.800209   966 net.cpp:434] bn_d0b <- d0b
I0428 19:51:40.800216   966 net.cpp:395] bn_d0b -> d0b (in-place)
I0428 19:51:40.800873   966 net.cpp:150] Setting up bn_d0b
I0428 19:51:40.800887   966 net.cpp:157] Top shape: 1 64 426 426 (11614464)
I0428 19:51:40.800899   966 net.cpp:165] Memory required for data: 95590208
I0428 19:51:40.800915   966 layer_factory.hpp:77] Creating layer sc_d0b
I0428 19:51:40.800930   966 net.cpp:100] Creating Layer sc_d0b
I0428 19:51:40.800935   966 net.cpp:434] sc_d0b <- d0b
I0428 19:51:40.800943   966 net.cpp:395] sc_d0b -> d0b (in-place)
I0428 19:51:40.801023   966 layer_factory.hpp:77] Creating layer sc_d0b
I0428 19:51:40.803225   966 net.cpp:150] Setting up sc_d0b
I0428 19:51:40.803248   966 net.cpp:157] Top shape: 1 64 426 426 (11614464)
I0428 19:51:40.803256   966 net.cpp:165] Memory required for data: 142048064
I0428 19:51:40.803267   966 layer_factory.hpp:77] Creating layer relu_d0b
I0428 19:51:40.803278   966 net.cpp:100] Creating Layer relu_d0b
I0428 19:51:40.803283   966 net.cpp:434] relu_d0b <- d0b
I0428 19:51:40.803293   966 net.cpp:395] relu_d0b -> d0b (in-place)
I0428 19:51:40.803722   966 net.cpp:150] Setting up relu_d0b
I0428 19:51:40.803737   966 net.cpp:157] Top shape: 1 64 426 426 (11614464)
I0428 19:51:40.803742   966 net.cpp:165] Memory required for data: 188505920
I0428 19:51:40.803748   966 layer_factory.hpp:77] Creating layer conv_d0b-c
I0428 19:51:40.803761   966 net.cpp:100] Creating Layer conv_d0b-c
I0428 19:51:40.803766   966 net.cpp:434] conv_d0b-c <- d0b
I0428 19:51:40.803776   966 net.cpp:408] conv_d0b-c -> d0c
I0428 19:51:40.806931   966 net.cpp:150] Setting up conv_d0b-c
I0428 19:51:40.806970   966 net.cpp:157] Top shape: 1 64 424 424 (11505664)
I0428 19:51:40.806977   966 net.cpp:165] Memory required for data: 234528576
I0428 19:51:40.806993   966 layer_factory.hpp:77] Creating layer bn_d0c
I0428 19:51:40.807006   966 net.cpp:100] Creating Layer bn_d0c
I0428 19:51:40.807011   966 net.cpp:434] bn_d0c <- d0c
I0428 19:51:40.807027   966 net.cpp:395] bn_d0c -> d0c (in-place)
I0428 19:51:40.807704   966 net.cpp:150] Setting up bn_d0c
I0428 19:51:40.807720   966 net.cpp:157] Top shape: 1 64 424 424 (11505664)
I0428 19:51:40.807725   966 net.cpp:165] Memory required for data: 280551232
I0428 19:51:40.807740   966 layer_factory.hpp:77] Creating layer sc_d0c
I0428 19:51:40.807751   966 net.cpp:100] Creating Layer sc_d0c
I0428 19:51:40.807811   966 net.cpp:434] sc_d0c <- d0c
I0428 19:51:40.807829   966 net.cpp:395] sc_d0c -> d0c (in-place)
I0428 19:51:40.807926   966 layer_factory.hpp:77] Creating layer sc_d0c
I0428 19:51:40.810289   966 net.cpp:150] Setting up sc_d0c
I0428 19:51:40.810313   966 net.cpp:157] Top shape: 1 64 424 424 (11505664)
I0428 19:51:40.810318   966 net.cpp:165] Memory required for data: 326573888
I0428 19:51:40.810333   966 layer_factory.hpp:77] Creating layer relu_d0c
I0428 19:51:40.810345   966 net.cpp:100] Creating Layer relu_d0c
I0428 19:51:40.810351   966 net.cpp:434] relu_d0c <- d0c
I0428 19:51:40.810359   966 net.cpp:395] relu_d0c -> d0c (in-place)
I0428 19:51:40.810744   966 net.cpp:150] Setting up relu_d0c
I0428 19:51:40.810757   966 net.cpp:157] Top shape: 1 64 424 424 (11505664)
I0428 19:51:40.810762   966 net.cpp:165] Memory required for data: 372596544
I0428 19:51:40.810766   966 layer_factory.hpp:77] Creating layer d0c_relu_d0c_0_split
I0428 19:51:40.810775   966 net.cpp:100] Creating Layer d0c_relu_d0c_0_split
I0428 19:51:40.810780   966 net.cpp:434] d0c_relu_d0c_0_split <- d0c
I0428 19:51:40.810788   966 net.cpp:408] d0c_relu_d0c_0_split -> d0c_relu_d0c_0_split_0
I0428 19:51:40.810799   966 net.cpp:408] d0c_relu_d0c_0_split -> d0c_relu_d0c_0_split_1
I0428 19:51:40.810874   966 net.cpp:150] Setting up d0c_relu_d0c_0_split
I0428 19:51:40.810885   966 net.cpp:157] Top shape: 1 64 424 424 (11505664)
I0428 19:51:40.810892   966 net.cpp:157] Top shape: 1 64 424 424 (11505664)
I0428 19:51:40.810897   966 net.cpp:165] Memory required for data: 464641856
I0428 19:51:40.810901   966 layer_factory.hpp:77] Creating layer pool_d0c-1a
I0428 19:51:40.810910   966 net.cpp:100] Creating Layer pool_d0c-1a
I0428 19:51:40.810916   966 net.cpp:434] pool_d0c-1a <- d0c_relu_d0c_0_split_0
I0428 19:51:40.810925   966 net.cpp:408] pool_d0c-1a -> d1a
I0428 19:51:40.810988   966 net.cpp:150] Setting up pool_d0c-1a
I0428 19:51:40.810998   966 net.cpp:157] Top shape: 1 64 212 212 (2876416)
I0428 19:51:40.811004   966 net.cpp:165] Memory required for data: 476147520
I0428 19:51:40.811008   966 layer_factory.hpp:77] Creating layer conv_d1a-b
I0428 19:51:40.811020   966 net.cpp:100] Creating Layer conv_d1a-b
I0428 19:51:40.811024   966 net.cpp:434] conv_d1a-b <- d1a
I0428 19:51:40.811033   966 net.cpp:408] conv_d1a-b -> d1b
I0428 19:51:40.812309   966 net.cpp:150] Setting up conv_d1a-b
I0428 19:51:40.812333   966 net.cpp:157] Top shape: 1 128 210 210 (5644800)
I0428 19:51:40.812338   966 net.cpp:165] Memory required for data: 498726720
I0428 19:51:40.812356   966 layer_factory.hpp:77] Creating layer bn_d1b
I0428 19:51:40.812364   966 net.cpp:100] Creating Layer bn_d1b
I0428 19:51:40.812379   966 net.cpp:434] bn_d1b <- d1b
I0428 19:51:40.812386   966 net.cpp:395] bn_d1b -> d1b (in-place)
I0428 19:51:40.812798   966 net.cpp:150] Setting up bn_d1b
I0428 19:51:40.812808   966 net.cpp:157] Top shape: 1 128 210 210 (5644800)
I0428 19:51:40.812813   966 net.cpp:165] Memory required for data: 521305920
I0428 19:51:40.812829   966 layer_factory.hpp:77] Creating layer sc_d1b
I0428 19:51:40.812839   966 net.cpp:100] Creating Layer sc_d1b
I0428 19:51:40.812844   966 net.cpp:434] sc_d1b <- d1b
I0428 19:51:40.812851   966 net.cpp:395] sc_d1b -> d1b (in-place)
I0428 19:51:40.812932   966 layer_factory.hpp:77] Creating layer sc_d1b
I0428 19:51:40.813247   966 net.cpp:150] Setting up sc_d1b
I0428 19:51:40.813259   966 net.cpp:157] Top shape: 1 128 210 210 (5644800)
I0428 19:51:40.813263   966 net.cpp:165] Memory required for data: 543885120
I0428 19:51:40.813279   966 layer_factory.hpp:77] Creating layer relu_d1b
I0428 19:51:40.813287   966 net.cpp:100] Creating Layer relu_d1b
I0428 19:51:40.813292   966 net.cpp:434] relu_d1b <- d1b
I0428 19:51:40.813298   966 net.cpp:395] relu_d1b -> d1b (in-place)
I0428 19:51:40.813642   966 net.cpp:150] Setting up relu_d1b
I0428 19:51:40.813658   966 net.cpp:157] Top shape: 1 128 210 210 (5644800)
I0428 19:51:40.813663   966 net.cpp:165] Memory required for data: 566464320
I0428 19:51:40.813702   966 layer_factory.hpp:77] Creating layer conv_d1b-c
I0428 19:51:40.813715   966 net.cpp:100] Creating Layer conv_d1b-c
I0428 19:51:40.813721   966 net.cpp:434] conv_d1b-c <- d1b
I0428 19:51:40.813730   966 net.cpp:408] conv_d1b-c -> d1c
I0428 19:51:40.815793   966 net.cpp:150] Setting up conv_d1b-c
I0428 19:51:40.815806   966 net.cpp:157] Top shape: 1 128 208 208 (5537792)
I0428 19:51:40.815811   966 net.cpp:165] Memory required for data: 588615488
I0428 19:51:40.815827   966 layer_factory.hpp:77] Creating layer bn_d1c
I0428 19:51:40.815838   966 net.cpp:100] Creating Layer bn_d1c
I0428 19:51:40.815843   966 net.cpp:434] bn_d1c <- d1c
I0428 19:51:40.815851   966 net.cpp:395] bn_d1c -> d1c (in-place)
I0428 19:51:40.817711   966 net.cpp:150] Setting up bn_d1c
I0428 19:51:40.817733   966 net.cpp:157] Top shape: 1 128 208 208 (5537792)
I0428 19:51:40.817736   966 net.cpp:165] Memory required for data: 610766656
I0428 19:51:40.817751   966 layer_factory.hpp:77] Creating layer sc_d1c
I0428 19:51:40.817761   966 net.cpp:100] Creating Layer sc_d1c
I0428 19:51:40.817766   966 net.cpp:434] sc_d1c <- d1c
I0428 19:51:40.817775   966 net.cpp:395] sc_d1c -> d1c (in-place)
I0428 19:51:40.817847   966 layer_factory.hpp:77] Creating layer sc_d1c
I0428 19:51:40.818127   966 net.cpp:150] Setting up sc_d1c
I0428 19:51:40.818140   966 net.cpp:157] Top shape: 1 128 208 208 (5537792)
I0428 19:51:40.818143   966 net.cpp:165] Memory required for data: 632917824
I0428 19:51:40.818153   966 layer_factory.hpp:77] Creating layer relu_d1c
I0428 19:51:40.818161   966 net.cpp:100] Creating Layer relu_d1c
I0428 19:51:40.818164   966 net.cpp:434] relu_d1c <- d1c
I0428 19:51:40.818171   966 net.cpp:395] relu_d1c -> d1c (in-place)
I0428 19:51:40.819661   966 net.cpp:150] Setting up relu_d1c
I0428 19:51:40.819682   966 net.cpp:157] Top shape: 1 128 208 208 (5537792)
I0428 19:51:40.819694   966 net.cpp:165] Memory required for data: 655068992
I0428 19:51:40.819700   966 layer_factory.hpp:77] Creating layer d1c_relu_d1c_0_split
I0428 19:51:40.819710   966 net.cpp:100] Creating Layer d1c_relu_d1c_0_split
I0428 19:51:40.819715   966 net.cpp:434] d1c_relu_d1c_0_split <- d1c
I0428 19:51:40.819725   966 net.cpp:408] d1c_relu_d1c_0_split -> d1c_relu_d1c_0_split_0
I0428 19:51:40.819736   966 net.cpp:408] d1c_relu_d1c_0_split -> d1c_relu_d1c_0_split_1
I0428 19:51:40.819818   966 net.cpp:150] Setting up d1c_relu_d1c_0_split
I0428 19:51:40.819830   966 net.cpp:157] Top shape: 1 128 208 208 (5537792)
I0428 19:51:40.819836   966 net.cpp:157] Top shape: 1 128 208 208 (5537792)
I0428 19:51:40.819840   966 net.cpp:165] Memory required for data: 699371328
I0428 19:51:40.819845   966 layer_factory.hpp:77] Creating layer pool_d1c-2a
I0428 19:51:40.819854   966 net.cpp:100] Creating Layer pool_d1c-2a
I0428 19:51:40.819859   966 net.cpp:434] pool_d1c-2a <- d1c_relu_d1c_0_split_0
I0428 19:51:40.819866   966 net.cpp:408] pool_d1c-2a -> d2a
I0428 19:51:40.819938   966 net.cpp:150] Setting up pool_d1c-2a
I0428 19:51:40.819949   966 net.cpp:157] Top shape: 1 128 104 104 (1384448)
I0428 19:51:40.819953   966 net.cpp:165] Memory required for data: 704909120
I0428 19:51:40.819957   966 layer_factory.hpp:77] Creating layer conv_d2a-b
I0428 19:51:40.819970   966 net.cpp:100] Creating Layer conv_d2a-b
I0428 19:51:40.819974   966 net.cpp:434] conv_d2a-b <- d2a
I0428 19:51:40.819983   966 net.cpp:408] conv_d2a-b -> d2b
I0428 19:51:40.824787   966 net.cpp:150] Setting up conv_d2a-b
I0428 19:51:40.824808   966 net.cpp:157] Top shape: 1 256 102 102 (2663424)
I0428 19:51:40.824812   966 net.cpp:165] Memory required for data: 715562816
I0428 19:51:40.824822   966 layer_factory.hpp:77] Creating layer bn_d2b
I0428 19:51:40.824831   966 net.cpp:100] Creating Layer bn_d2b
I0428 19:51:40.824837   966 net.cpp:434] bn_d2b <- d2b
I0428 19:51:40.824846   966 net.cpp:395] bn_d2b -> d2b (in-place)
I0428 19:51:40.825204   966 net.cpp:150] Setting up bn_d2b
I0428 19:51:40.825215   966 net.cpp:157] Top shape: 1 256 102 102 (2663424)
I0428 19:51:40.825220   966 net.cpp:165] Memory required for data: 726216512
I0428 19:51:40.825260   966 layer_factory.hpp:77] Creating layer sc_d2b
I0428 19:51:40.825269   966 net.cpp:100] Creating Layer sc_d2b
I0428 19:51:40.825274   966 net.cpp:434] sc_d2b <- d2b
I0428 19:51:40.825284   966 net.cpp:395] sc_d2b -> d2b (in-place)
I0428 19:51:40.825350   966 layer_factory.hpp:77] Creating layer sc_d2b
I0428 19:51:40.825573   966 net.cpp:150] Setting up sc_d2b
I0428 19:51:40.825585   966 net.cpp:157] Top shape: 1 256 102 102 (2663424)
I0428 19:51:40.825590   966 net.cpp:165] Memory required for data: 736870208
I0428 19:51:40.825598   966 layer_factory.hpp:77] Creating layer relu_d2b
I0428 19:51:40.825605   966 net.cpp:100] Creating Layer relu_d2b
I0428 19:51:40.825609   966 net.cpp:434] relu_d2b <- d2b
I0428 19:51:40.825618   966 net.cpp:395] relu_d2b -> d2b (in-place)
I0428 19:51:40.825914   966 net.cpp:150] Setting up relu_d2b
I0428 19:51:40.825927   966 net.cpp:157] Top shape: 1 256 102 102 (2663424)
I0428 19:51:40.825932   966 net.cpp:165] Memory required for data: 747523904
I0428 19:51:40.825937   966 layer_factory.hpp:77] Creating layer conv_d2b-c
I0428 19:51:40.825947   966 net.cpp:100] Creating Layer conv_d2b-c
I0428 19:51:40.825953   966 net.cpp:434] conv_d2b-c <- d2b
I0428 19:51:40.825959   966 net.cpp:408] conv_d2b-c -> d2c
I0428 19:51:40.833626   966 net.cpp:150] Setting up conv_d2b-c
I0428 19:51:40.833647   966 net.cpp:157] Top shape: 1 256 100 100 (2560000)
I0428 19:51:40.833650   966 net.cpp:165] Memory required for data: 757763904
I0428 19:51:40.833660   966 layer_factory.hpp:77] Creating layer bn_d2c
I0428 19:51:40.833669   966 net.cpp:100] Creating Layer bn_d2c
I0428 19:51:40.833674   966 net.cpp:434] bn_d2c <- d2c
I0428 19:51:40.833684   966 net.cpp:395] bn_d2c -> d2c (in-place)
I0428 19:51:40.834043   966 net.cpp:150] Setting up bn_d2c
I0428 19:51:40.834055   966 net.cpp:157] Top shape: 1 256 100 100 (2560000)
I0428 19:51:40.834059   966 net.cpp:165] Memory required for data: 768003904
I0428 19:51:40.834069   966 layer_factory.hpp:77] Creating layer sc_d2c
I0428 19:51:40.834076   966 net.cpp:100] Creating Layer sc_d2c
I0428 19:51:40.834081   966 net.cpp:434] sc_d2c <- d2c
I0428 19:51:40.834087   966 net.cpp:395] sc_d2c -> d2c (in-place)
I0428 19:51:40.834144   966 layer_factory.hpp:77] Creating layer sc_d2c
I0428 19:51:40.834345   966 net.cpp:150] Setting up sc_d2c
I0428 19:51:40.834357   966 net.cpp:157] Top shape: 1 256 100 100 (2560000)
I0428 19:51:40.834360   966 net.cpp:165] Memory required for data: 778243904
I0428 19:51:40.834368   966 layer_factory.hpp:77] Creating layer relu_d2c
I0428 19:51:40.834374   966 net.cpp:100] Creating Layer relu_d2c
I0428 19:51:40.834379   966 net.cpp:434] relu_d2c <- d2c
I0428 19:51:40.834385   966 net.cpp:395] relu_d2c -> d2c (in-place)
I0428 19:51:40.834676   966 net.cpp:150] Setting up relu_d2c
I0428 19:51:40.834692   966 net.cpp:157] Top shape: 1 256 100 100 (2560000)
I0428 19:51:40.834697   966 net.cpp:165] Memory required for data: 788483904
I0428 19:51:40.834702   966 layer_factory.hpp:77] Creating layer d2c_relu_d2c_0_split
I0428 19:51:40.834715   966 net.cpp:100] Creating Layer d2c_relu_d2c_0_split
I0428 19:51:40.834719   966 net.cpp:434] d2c_relu_d2c_0_split <- d2c
I0428 19:51:40.834735   966 net.cpp:408] d2c_relu_d2c_0_split -> d2c_relu_d2c_0_split_0
I0428 19:51:40.834745   966 net.cpp:408] d2c_relu_d2c_0_split -> d2c_relu_d2c_0_split_1
I0428 19:51:40.834812   966 net.cpp:150] Setting up d2c_relu_d2c_0_split
I0428 19:51:40.834822   966 net.cpp:157] Top shape: 1 256 100 100 (2560000)
I0428 19:51:40.834830   966 net.cpp:157] Top shape: 1 256 100 100 (2560000)
I0428 19:51:40.834832   966 net.cpp:165] Memory required for data: 808963904
I0428 19:51:40.834836   966 layer_factory.hpp:77] Creating layer pool_d2c-3a
I0428 19:51:40.834843   966 net.cpp:100] Creating Layer pool_d2c-3a
I0428 19:51:40.834848   966 net.cpp:434] pool_d2c-3a <- d2c_relu_d2c_0_split_0
I0428 19:51:40.834856   966 net.cpp:408] pool_d2c-3a -> d3a
I0428 19:51:40.834909   966 net.cpp:150] Setting up pool_d2c-3a
I0428 19:51:40.834918   966 net.cpp:157] Top shape: 1 256 50 50 (640000)
I0428 19:51:40.834945   966 net.cpp:165] Memory required for data: 811523904
I0428 19:51:40.834950   966 layer_factory.hpp:77] Creating layer conv_d3a-b
I0428 19:51:40.834961   966 net.cpp:100] Creating Layer conv_d3a-b
I0428 19:51:40.834965   966 net.cpp:434] conv_d3a-b <- d3a
I0428 19:51:40.834980   966 net.cpp:408] conv_d3a-b -> d3b
I0428 19:51:40.848304   966 net.cpp:150] Setting up conv_d3a-b
I0428 19:51:40.848321   966 net.cpp:157] Top shape: 1 512 48 48 (1179648)
I0428 19:51:40.848325   966 net.cpp:165] Memory required for data: 816242496
I0428 19:51:40.848335   966 layer_factory.hpp:77] Creating layer bn_d3b
I0428 19:51:40.848347   966 net.cpp:100] Creating Layer bn_d3b
I0428 19:51:40.848352   966 net.cpp:434] bn_d3b <- d3b
I0428 19:51:40.848359   966 net.cpp:395] bn_d3b -> d3b (in-place)
I0428 19:51:40.848665   966 net.cpp:150] Setting up bn_d3b
I0428 19:51:40.848676   966 net.cpp:157] Top shape: 1 512 48 48 (1179648)
I0428 19:51:40.848680   966 net.cpp:165] Memory required for data: 820961088
I0428 19:51:40.848690   966 layer_factory.hpp:77] Creating layer sc_d3b
I0428 19:51:40.848697   966 net.cpp:100] Creating Layer sc_d3b
I0428 19:51:40.848701   966 net.cpp:434] sc_d3b <- d3b
I0428 19:51:40.848707   966 net.cpp:395] sc_d3b -> d3b (in-place)
I0428 19:51:40.848757   966 layer_factory.hpp:77] Creating layer sc_d3b
I0428 19:51:40.848922   966 net.cpp:150] Setting up sc_d3b
I0428 19:51:40.848942   966 net.cpp:157] Top shape: 1 512 48 48 (1179648)
I0428 19:51:40.848945   966 net.cpp:165] Memory required for data: 825679680
I0428 19:51:40.848953   966 layer_factory.hpp:77] Creating layer relu_d3b
I0428 19:51:40.848959   966 net.cpp:100] Creating Layer relu_d3b
I0428 19:51:40.848963   966 net.cpp:434] relu_d3b <- d3b
I0428 19:51:40.848969   966 net.cpp:395] relu_d3b -> d3b (in-place)
I0428 19:51:40.849251   966 net.cpp:150] Setting up relu_d3b
I0428 19:51:40.849274   966 net.cpp:157] Top shape: 1 512 48 48 (1179648)
I0428 19:51:40.849277   966 net.cpp:165] Memory required for data: 830398272
I0428 19:51:40.849282   966 layer_factory.hpp:77] Creating layer conv_d3b-c
I0428 19:51:40.849298   966 net.cpp:100] Creating Layer conv_d3b-c
I0428 19:51:40.849303   966 net.cpp:434] conv_d3b-c <- d3b
I0428 19:51:40.849309   966 net.cpp:408] conv_d3b-c -> d3c
I0428 19:51:40.872716   966 net.cpp:150] Setting up conv_d3b-c
I0428 19:51:40.872732   966 net.cpp:157] Top shape: 1 512 46 46 (1083392)
I0428 19:51:40.872736   966 net.cpp:165] Memory required for data: 834731840
I0428 19:51:40.872751   966 layer_factory.hpp:77] Creating layer bn_d3c
I0428 19:51:40.872758   966 net.cpp:100] Creating Layer bn_d3c
I0428 19:51:40.872762   966 net.cpp:434] bn_d3c <- d3c
I0428 19:51:40.872771   966 net.cpp:395] bn_d3c -> d3c (in-place)
I0428 19:51:40.873060   966 net.cpp:150] Setting up bn_d3c
I0428 19:51:40.873070   966 net.cpp:157] Top shape: 1 512 46 46 (1083392)
I0428 19:51:40.873073   966 net.cpp:165] Memory required for data: 839065408
I0428 19:51:40.873083   966 layer_factory.hpp:77] Creating layer sc_d3c
I0428 19:51:40.873090   966 net.cpp:100] Creating Layer sc_d3c
I0428 19:51:40.873093   966 net.cpp:434] sc_d3c <- d3c
I0428 19:51:40.873100   966 net.cpp:395] sc_d3c -> d3c (in-place)
I0428 19:51:40.873148   966 layer_factory.hpp:77] Creating layer sc_d3c
I0428 19:51:40.873311   966 net.cpp:150] Setting up sc_d3c
I0428 19:51:40.873320   966 net.cpp:157] Top shape: 1 512 46 46 (1083392)
I0428 19:51:40.873322   966 net.cpp:165] Memory required for data: 843398976
I0428 19:51:40.873330   966 layer_factory.hpp:77] Creating layer relu_d3c
I0428 19:51:40.873337   966 net.cpp:100] Creating Layer relu_d3c
I0428 19:51:40.873339   966 net.cpp:434] relu_d3c <- d3c
I0428 19:51:40.873344   966 net.cpp:395] relu_d3c -> d3c (in-place)
I0428 19:51:40.874510   966 net.cpp:150] Setting up relu_d3c
I0428 19:51:40.874526   966 net.cpp:157] Top shape: 1 512 46 46 (1083392)
I0428 19:51:40.874529   966 net.cpp:165] Memory required for data: 847732544
I0428 19:51:40.874533   966 layer_factory.hpp:77] Creating layer d3c_relu_d3c_0_split
I0428 19:51:40.874570   966 net.cpp:100] Creating Layer d3c_relu_d3c_0_split
I0428 19:51:40.874574   966 net.cpp:434] d3c_relu_d3c_0_split <- d3c
I0428 19:51:40.874583   966 net.cpp:408] d3c_relu_d3c_0_split -> d3c_relu_d3c_0_split_0
I0428 19:51:40.874591   966 net.cpp:408] d3c_relu_d3c_0_split -> d3c_relu_d3c_0_split_1
I0428 19:51:40.874653   966 net.cpp:150] Setting up d3c_relu_d3c_0_split
I0428 19:51:40.874662   966 net.cpp:157] Top shape: 1 512 46 46 (1083392)
I0428 19:51:40.874667   966 net.cpp:157] Top shape: 1 512 46 46 (1083392)
I0428 19:51:40.874670   966 net.cpp:165] Memory required for data: 856399680
I0428 19:51:40.874675   966 layer_factory.hpp:77] Creating layer pool_d3c-4a
I0428 19:51:40.874680   966 net.cpp:100] Creating Layer pool_d3c-4a
I0428 19:51:40.874685   966 net.cpp:434] pool_d3c-4a <- d3c_relu_d3c_0_split_0
I0428 19:51:40.874691   966 net.cpp:408] pool_d3c-4a -> d4a
I0428 19:51:40.874739   966 net.cpp:150] Setting up pool_d3c-4a
I0428 19:51:40.874748   966 net.cpp:157] Top shape: 1 512 23 23 (270848)
I0428 19:51:40.874752   966 net.cpp:165] Memory required for data: 857483072
I0428 19:51:40.874754   966 layer_factory.hpp:77] Creating layer conv_d4a-b
I0428 19:51:40.874764   966 net.cpp:100] Creating Layer conv_d4a-b
I0428 19:51:40.874768   966 net.cpp:434] conv_d4a-b <- d4a
I0428 19:51:40.874774   966 net.cpp:408] conv_d4a-b -> d4b
I0428 19:51:40.917248   966 net.cpp:150] Setting up conv_d4a-b
I0428 19:51:40.917268   966 net.cpp:157] Top shape: 1 1024 21 21 (451584)
I0428 19:51:40.917282   966 net.cpp:165] Memory required for data: 859289408
I0428 19:51:40.917289   966 layer_factory.hpp:77] Creating layer bn_d4b
I0428 19:51:40.917296   966 net.cpp:100] Creating Layer bn_d4b
I0428 19:51:40.917304   966 net.cpp:434] bn_d4b <- d4b
I0428 19:51:40.917309   966 net.cpp:395] bn_d4b -> d4b (in-place)
I0428 19:51:40.917575   966 net.cpp:150] Setting up bn_d4b
I0428 19:51:40.917595   966 net.cpp:157] Top shape: 1 1024 21 21 (451584)
I0428 19:51:40.917599   966 net.cpp:165] Memory required for data: 861095744
I0428 19:51:40.917608   966 layer_factory.hpp:77] Creating layer sc_d4b
I0428 19:51:40.917613   966 net.cpp:100] Creating Layer sc_d4b
I0428 19:51:40.917618   966 net.cpp:434] sc_d4b <- d4b
I0428 19:51:40.917623   966 net.cpp:395] sc_d4b -> d4b (in-place)
I0428 19:51:40.917671   966 layer_factory.hpp:77] Creating layer sc_d4b
I0428 19:51:40.917807   966 net.cpp:150] Setting up sc_d4b
I0428 19:51:40.917816   966 net.cpp:157] Top shape: 1 1024 21 21 (451584)
I0428 19:51:40.917820   966 net.cpp:165] Memory required for data: 862902080
I0428 19:51:40.917825   966 layer_factory.hpp:77] Creating layer relu_d4b
I0428 19:51:40.917831   966 net.cpp:100] Creating Layer relu_d4b
I0428 19:51:40.917834   966 net.cpp:434] relu_d4b <- d4b
I0428 19:51:40.917840   966 net.cpp:395] relu_d4b -> d4b (in-place)
I0428 19:51:40.918081   966 net.cpp:150] Setting up relu_d4b
I0428 19:51:40.918092   966 net.cpp:157] Top shape: 1 1024 21 21 (451584)
I0428 19:51:40.918095   966 net.cpp:165] Memory required for data: 864708416
I0428 19:51:40.918099   966 layer_factory.hpp:77] Creating layer conv_d4b-c
I0428 19:51:40.918107   966 net.cpp:100] Creating Layer conv_d4b-c
I0428 19:51:40.918110   966 net.cpp:434] conv_d4b-c <- d4b
I0428 19:51:40.918118   966 net.cpp:408] conv_d4b-c -> d4c
I0428 19:51:41.040786   966 net.cpp:150] Setting up conv_d4b-c
I0428 19:51:41.040825   966 net.cpp:157] Top shape: 1 1024 19 19 (369664)
I0428 19:51:41.040828   966 net.cpp:165] Memory required for data: 866187072
I0428 19:51:41.040848   966 layer_factory.hpp:77] Creating layer bn_d4c
I0428 19:51:41.040860   966 net.cpp:100] Creating Layer bn_d4c
I0428 19:51:41.040866   966 net.cpp:434] bn_d4c <- d4c
I0428 19:51:41.040873   966 net.cpp:395] bn_d4c -> d4c (in-place)
I0428 19:51:41.041157   966 net.cpp:150] Setting up bn_d4c
I0428 19:51:41.041167   966 net.cpp:157] Top shape: 1 1024 19 19 (369664)
I0428 19:51:41.041170   966 net.cpp:165] Memory required for data: 867665728
I0428 19:51:41.041178   966 layer_factory.hpp:77] Creating layer sc_d4c
I0428 19:51:41.041223   966 net.cpp:100] Creating Layer sc_d4c
I0428 19:51:41.041230   966 net.cpp:434] sc_d4c <- d4c
I0428 19:51:41.041235   966 net.cpp:395] sc_d4c -> d4c (in-place)
I0428 19:51:41.041292   966 layer_factory.hpp:77] Creating layer sc_d4c
I0428 19:51:41.041434   966 net.cpp:150] Setting up sc_d4c
I0428 19:51:41.041448   966 net.cpp:157] Top shape: 1 1024 19 19 (369664)
I0428 19:51:41.041451   966 net.cpp:165] Memory required for data: 869144384
I0428 19:51:41.041457   966 layer_factory.hpp:77] Creating layer relu_d4c
I0428 19:51:41.041463   966 net.cpp:100] Creating Layer relu_d4c
I0428 19:51:41.041467   966 net.cpp:434] relu_d4c <- d4c
I0428 19:51:41.041472   966 net.cpp:395] relu_d4c -> d4c (in-place)
I0428 19:51:41.041780   966 net.cpp:150] Setting up relu_d4c
I0428 19:51:41.041791   966 net.cpp:157] Top shape: 1 1024 19 19 (369664)
I0428 19:51:41.041795   966 net.cpp:165] Memory required for data: 870623040
I0428 19:51:41.041808   966 layer_factory.hpp:77] Creating layer upconv_d4c_u3a
I0428 19:51:41.041817   966 net.cpp:100] Creating Layer upconv_d4c_u3a
I0428 19:51:41.041821   966 net.cpp:434] upconv_d4c_u3a <- d4c
I0428 19:51:41.041827   966 net.cpp:408] upconv_d4c_u3a -> u3a
I0428 19:51:41.060075   966 net.cpp:150] Setting up upconv_d4c_u3a
I0428 19:51:41.060091   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.060093   966 net.cpp:165] Memory required for data: 873580352
I0428 19:51:41.060111   966 layer_factory.hpp:77] Creating layer bn_u3a
I0428 19:51:41.060118   966 net.cpp:100] Creating Layer bn_u3a
I0428 19:51:41.060122   966 net.cpp:434] bn_u3a <- u3a
I0428 19:51:41.060128   966 net.cpp:395] bn_u3a -> u3a (in-place)
I0428 19:51:41.060412   966 net.cpp:150] Setting up bn_u3a
I0428 19:51:41.060421   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.060425   966 net.cpp:165] Memory required for data: 876537664
I0428 19:51:41.060431   966 layer_factory.hpp:77] Creating layer sc_u3a
I0428 19:51:41.060439   966 net.cpp:100] Creating Layer sc_u3a
I0428 19:51:41.060442   966 net.cpp:434] sc_u3a <- u3a
I0428 19:51:41.060448   966 net.cpp:395] sc_u3a -> u3a (in-place)
I0428 19:51:41.060492   966 layer_factory.hpp:77] Creating layer sc_u3a
I0428 19:51:41.060642   966 net.cpp:150] Setting up sc_u3a
I0428 19:51:41.060652   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.060654   966 net.cpp:165] Memory required for data: 879494976
I0428 19:51:41.060660   966 layer_factory.hpp:77] Creating layer relu_u3a
I0428 19:51:41.060667   966 net.cpp:100] Creating Layer relu_u3a
I0428 19:51:41.060669   966 net.cpp:434] relu_u3a <- u3a
I0428 19:51:41.060675   966 net.cpp:395] relu_u3a -> u3a (in-place)
I0428 19:51:41.060905   966 net.cpp:150] Setting up relu_u3a
I0428 19:51:41.060916   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.060920   966 net.cpp:165] Memory required for data: 882452288
I0428 19:51:41.060922   966 layer_factory.hpp:77] Creating layer u3a_relu_u3a_0_split
I0428 19:51:41.060928   966 net.cpp:100] Creating Layer u3a_relu_u3a_0_split
I0428 19:51:41.060932   966 net.cpp:434] u3a_relu_u3a_0_split <- u3a
I0428 19:51:41.060938   966 net.cpp:408] u3a_relu_u3a_0_split -> u3a_relu_u3a_0_split_0
I0428 19:51:41.060946   966 net.cpp:408] u3a_relu_u3a_0_split -> u3a_relu_u3a_0_split_1
I0428 19:51:41.061000   966 net.cpp:150] Setting up u3a_relu_u3a_0_split
I0428 19:51:41.061009   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.061013   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.061017   966 net.cpp:165] Memory required for data: 888366912
I0428 19:51:41.061019   966 layer_factory.hpp:77] Creating layer crop_d3c-d3cc
I0428 19:51:41.061027   966 net.cpp:100] Creating Layer crop_d3c-d3cc
I0428 19:51:41.061030   966 net.cpp:434] crop_d3c-d3cc <- d3c_relu_d3c_0_split_1
I0428 19:51:41.061035   966 net.cpp:434] crop_d3c-d3cc <- u3a_relu_u3a_0_split_0
I0428 19:51:41.061040   966 net.cpp:408] crop_d3c-d3cc -> d3cc
I0428 19:51:41.061069   966 net.cpp:150] Setting up crop_d3c-d3cc
I0428 19:51:41.061075   966 net.cpp:157] Top shape: 1 512 38 38 (739328)
I0428 19:51:41.061097   966 net.cpp:165] Memory required for data: 891324224
I0428 19:51:41.061101   966 layer_factory.hpp:77] Creating layer concat_d3cc_u3a-b
I0428 19:51:41.061108   966 net.cpp:100] Creating Layer concat_d3cc_u3a-b
I0428 19:51:41.061111   966 net.cpp:434] concat_d3cc_u3a-b <- u3a_relu_u3a_0_split_1
I0428 19:51:41.061116   966 net.cpp:434] concat_d3cc_u3a-b <- d3cc
I0428 19:51:41.061121   966 net.cpp:408] concat_d3cc_u3a-b -> u3b
I0428 19:51:41.061154   966 net.cpp:150] Setting up concat_d3cc_u3a-b
I0428 19:51:41.061162   966 net.cpp:157] Top shape: 1 1024 38 38 (1478656)
I0428 19:51:41.061164   966 net.cpp:165] Memory required for data: 897238848
I0428 19:51:41.061167   966 layer_factory.hpp:77] Creating layer conv_u3b-c
I0428 19:51:41.061177   966 net.cpp:100] Creating Layer conv_u3b-c
I0428 19:51:41.061180   966 net.cpp:434] conv_u3b-c <- u3b
I0428 19:51:41.061187   966 net.cpp:408] conv_u3b-c -> u3c
I0428 19:51:41.101024   966 net.cpp:150] Setting up conv_u3b-c
I0428 19:51:41.101044   966 net.cpp:157] Top shape: 1 512 36 36 (663552)
I0428 19:51:41.101047   966 net.cpp:165] Memory required for data: 899893056
I0428 19:51:41.101055   966 layer_factory.hpp:77] Creating layer bn_u3c
I0428 19:51:41.101063   966 net.cpp:100] Creating Layer bn_u3c
I0428 19:51:41.101068   966 net.cpp:434] bn_u3c <- u3c
I0428 19:51:41.101073   966 net.cpp:395] bn_u3c -> u3c (in-place)
I0428 19:51:41.101341   966 net.cpp:150] Setting up bn_u3c
I0428 19:51:41.101349   966 net.cpp:157] Top shape: 1 512 36 36 (663552)
I0428 19:51:41.101353   966 net.cpp:165] Memory required for data: 902547264
I0428 19:51:41.101361   966 layer_factory.hpp:77] Creating layer sc_u3c
I0428 19:51:41.101368   966 net.cpp:100] Creating Layer sc_u3c
I0428 19:51:41.101372   966 net.cpp:434] sc_u3c <- u3c
I0428 19:51:41.101377   966 net.cpp:395] sc_u3c -> u3c (in-place)
I0428 19:51:41.101421   966 layer_factory.hpp:77] Creating layer sc_u3c
I0428 19:51:41.101589   966 net.cpp:150] Setting up sc_u3c
I0428 19:51:41.101599   966 net.cpp:157] Top shape: 1 512 36 36 (663552)
I0428 19:51:41.101603   966 net.cpp:165] Memory required for data: 905201472
I0428 19:51:41.101608   966 layer_factory.hpp:77] Creating layer relu_u3c
I0428 19:51:41.101613   966 net.cpp:100] Creating Layer relu_u3c
I0428 19:51:41.101618   966 net.cpp:434] relu_u3c <- u3c
I0428 19:51:41.101624   966 net.cpp:395] relu_u3c -> u3c (in-place)
I0428 19:51:41.102694   966 net.cpp:150] Setting up relu_u3c
I0428 19:51:41.102710   966 net.cpp:157] Top shape: 1 512 36 36 (663552)
I0428 19:51:41.102712   966 net.cpp:165] Memory required for data: 907855680
I0428 19:51:41.102716   966 layer_factory.hpp:77] Creating layer conv_u3c-d
I0428 19:51:41.102727   966 net.cpp:100] Creating Layer conv_u3c-d
I0428 19:51:41.102730   966 net.cpp:434] conv_u3c-d <- u3c
I0428 19:51:41.102737   966 net.cpp:408] conv_u3c-d -> u3d
I0428 19:51:41.122714   966 net.cpp:150] Setting up conv_u3c-d
I0428 19:51:41.122730   966 net.cpp:157] Top shape: 1 512 34 34 (591872)
I0428 19:51:41.122732   966 net.cpp:165] Memory required for data: 910223168
I0428 19:51:41.122740   966 layer_factory.hpp:77] Creating layer bn_u3d
I0428 19:51:41.122746   966 net.cpp:100] Creating Layer bn_u3d
I0428 19:51:41.122750   966 net.cpp:434] bn_u3d <- u3d
I0428 19:51:41.122756   966 net.cpp:395] bn_u3d -> u3d (in-place)
I0428 19:51:41.123024   966 net.cpp:150] Setting up bn_u3d
I0428 19:51:41.123046   966 net.cpp:157] Top shape: 1 512 34 34 (591872)
I0428 19:51:41.123049   966 net.cpp:165] Memory required for data: 912590656
I0428 19:51:41.123056   966 layer_factory.hpp:77] Creating layer sc_u3d
I0428 19:51:41.123064   966 net.cpp:100] Creating Layer sc_u3d
I0428 19:51:41.123067   966 net.cpp:434] sc_u3d <- u3d
I0428 19:51:41.123073   966 net.cpp:395] sc_u3d -> u3d (in-place)
I0428 19:51:41.123117   966 layer_factory.hpp:77] Creating layer sc_u3d
I0428 19:51:41.123267   966 net.cpp:150] Setting up sc_u3d
I0428 19:51:41.123276   966 net.cpp:157] Top shape: 1 512 34 34 (591872)
I0428 19:51:41.123280   966 net.cpp:165] Memory required for data: 914958144
I0428 19:51:41.123304   966 layer_factory.hpp:77] Creating layer relu_u3d
I0428 19:51:41.123311   966 net.cpp:100] Creating Layer relu_u3d
I0428 19:51:41.123314   966 net.cpp:434] relu_u3d <- u3d
I0428 19:51:41.123320   966 net.cpp:395] relu_u3d -> u3d (in-place)
I0428 19:51:41.123558   966 net.cpp:150] Setting up relu_u3d
I0428 19:51:41.123569   966 net.cpp:157] Top shape: 1 512 34 34 (591872)
I0428 19:51:41.123574   966 net.cpp:165] Memory required for data: 917325632
I0428 19:51:41.123576   966 layer_factory.hpp:77] Creating layer upconv_u3d_u2a
I0428 19:51:41.123592   966 net.cpp:100] Creating Layer upconv_u3d_u2a
I0428 19:51:41.123598   966 net.cpp:434] upconv_u3d_u2a <- u3d
I0428 19:51:41.123605   966 net.cpp:408] upconv_u3d_u2a -> u2a
I0428 19:51:41.128675   966 net.cpp:150] Setting up upconv_u3d_u2a
I0428 19:51:41.128691   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.128695   966 net.cpp:165] Memory required for data: 922060608
I0428 19:51:41.128703   966 layer_factory.hpp:77] Creating layer bn_u2a
I0428 19:51:41.128710   966 net.cpp:100] Creating Layer bn_u2a
I0428 19:51:41.128713   966 net.cpp:434] bn_u2a <- u2a
I0428 19:51:41.128720   966 net.cpp:395] bn_u2a -> u2a (in-place)
I0428 19:51:41.128984   966 net.cpp:150] Setting up bn_u2a
I0428 19:51:41.129005   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.129009   966 net.cpp:165] Memory required for data: 926795584
I0428 19:51:41.129015   966 layer_factory.hpp:77] Creating layer sc_u2a
I0428 19:51:41.129022   966 net.cpp:100] Creating Layer sc_u2a
I0428 19:51:41.129026   966 net.cpp:434] sc_u2a <- u2a
I0428 19:51:41.129031   966 net.cpp:395] sc_u2a -> u2a (in-place)
I0428 19:51:41.129078   966 layer_factory.hpp:77] Creating layer sc_u2a
I0428 19:51:41.129232   966 net.cpp:150] Setting up sc_u2a
I0428 19:51:41.129241   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.129243   966 net.cpp:165] Memory required for data: 931530560
I0428 19:51:41.129250   966 layer_factory.hpp:77] Creating layer relu_u2a
I0428 19:51:41.129256   966 net.cpp:100] Creating Layer relu_u2a
I0428 19:51:41.129258   966 net.cpp:434] relu_u2a <- u2a
I0428 19:51:41.129264   966 net.cpp:395] relu_u2a -> u2a (in-place)
I0428 19:51:41.129498   966 net.cpp:150] Setting up relu_u2a
I0428 19:51:41.129509   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.129513   966 net.cpp:165] Memory required for data: 936265536
I0428 19:51:41.129515   966 layer_factory.hpp:77] Creating layer u2a_relu_u2a_0_split
I0428 19:51:41.129521   966 net.cpp:100] Creating Layer u2a_relu_u2a_0_split
I0428 19:51:41.129525   966 net.cpp:434] u2a_relu_u2a_0_split <- u2a
I0428 19:51:41.129531   966 net.cpp:408] u2a_relu_u2a_0_split -> u2a_relu_u2a_0_split_0
I0428 19:51:41.129539   966 net.cpp:408] u2a_relu_u2a_0_split -> u2a_relu_u2a_0_split_1
I0428 19:51:41.129592   966 net.cpp:150] Setting up u2a_relu_u2a_0_split
I0428 19:51:41.129602   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.129606   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.129608   966 net.cpp:165] Memory required for data: 945735488
I0428 19:51:41.129612   966 layer_factory.hpp:77] Creating layer crop_d2c-d2cc
I0428 19:51:41.129619   966 net.cpp:100] Creating Layer crop_d2c-d2cc
I0428 19:51:41.129623   966 net.cpp:434] crop_d2c-d2cc <- d2c_relu_d2c_0_split_1
I0428 19:51:41.129628   966 net.cpp:434] crop_d2c-d2cc <- u2a_relu_u2a_0_split_0
I0428 19:51:41.129633   966 net.cpp:408] crop_d2c-d2cc -> d2cc
I0428 19:51:41.129662   966 net.cpp:150] Setting up crop_d2c-d2cc
I0428 19:51:41.129669   966 net.cpp:157] Top shape: 1 256 68 68 (1183744)
I0428 19:51:41.129673   966 net.cpp:165] Memory required for data: 950470464
I0428 19:51:41.129675   966 layer_factory.hpp:77] Creating layer concat_d2cc_u2a-b
I0428 19:51:41.129681   966 net.cpp:100] Creating Layer concat_d2cc_u2a-b
I0428 19:51:41.129684   966 net.cpp:434] concat_d2cc_u2a-b <- u2a_relu_u2a_0_split_1
I0428 19:51:41.129688   966 net.cpp:434] concat_d2cc_u2a-b <- d2cc
I0428 19:51:41.129710   966 net.cpp:408] concat_d2cc_u2a-b -> u2b
I0428 19:51:41.129743   966 net.cpp:150] Setting up concat_d2cc_u2a-b
I0428 19:51:41.129751   966 net.cpp:157] Top shape: 1 512 68 68 (2367488)
I0428 19:51:41.129755   966 net.cpp:165] Memory required for data: 959940416
I0428 19:51:41.129757   966 layer_factory.hpp:77] Creating layer conv_u2b-c
I0428 19:51:41.129767   966 net.cpp:100] Creating Layer conv_u2b-c
I0428 19:51:41.129770   966 net.cpp:434] conv_u2b-c <- u2b
I0428 19:51:41.129776   966 net.cpp:408] conv_u2b-c -> u2c
I0428 19:51:41.140054   966 net.cpp:150] Setting up conv_u2b-c
I0428 19:51:41.140071   966 net.cpp:157] Top shape: 1 256 66 66 (1115136)
I0428 19:51:41.140075   966 net.cpp:165] Memory required for data: 964400960
I0428 19:51:41.140082   966 layer_factory.hpp:77] Creating layer bn_u2c
I0428 19:51:41.140089   966 net.cpp:100] Creating Layer bn_u2c
I0428 19:51:41.140094   966 net.cpp:434] bn_u2c <- u2c
I0428 19:51:41.140099   966 net.cpp:395] bn_u2c -> u2c (in-place)
I0428 19:51:41.140367   966 net.cpp:150] Setting up bn_u2c
I0428 19:51:41.140377   966 net.cpp:157] Top shape: 1 256 66 66 (1115136)
I0428 19:51:41.140380   966 net.cpp:165] Memory required for data: 968861504
I0428 19:51:41.140398   966 layer_factory.hpp:77] Creating layer sc_u2c
I0428 19:51:41.140404   966 net.cpp:100] Creating Layer sc_u2c
I0428 19:51:41.140408   966 net.cpp:434] sc_u2c <- u2c
I0428 19:51:41.140414   966 net.cpp:395] sc_u2c -> u2c (in-place)
I0428 19:51:41.140470   966 layer_factory.hpp:77] Creating layer sc_u2c
I0428 19:51:41.140622   966 net.cpp:150] Setting up sc_u2c
I0428 19:51:41.140630   966 net.cpp:157] Top shape: 1 256 66 66 (1115136)
I0428 19:51:41.140635   966 net.cpp:165] Memory required for data: 973322048
I0428 19:51:41.140640   966 layer_factory.hpp:77] Creating layer relu_u2c
I0428 19:51:41.140645   966 net.cpp:100] Creating Layer relu_u2c
I0428 19:51:41.140648   966 net.cpp:434] relu_u2c <- u2c
I0428 19:51:41.140653   966 net.cpp:395] relu_u2c -> u2c (in-place)
I0428 19:51:41.141762   966 net.cpp:150] Setting up relu_u2c
I0428 19:51:41.141788   966 net.cpp:157] Top shape: 1 256 66 66 (1115136)
I0428 19:51:41.141790   966 net.cpp:165] Memory required for data: 977782592
I0428 19:51:41.141794   966 layer_factory.hpp:77] Creating layer conv_u2c-d
I0428 19:51:41.141804   966 net.cpp:100] Creating Layer conv_u2c-d
I0428 19:51:41.141808   966 net.cpp:434] conv_u2c-d <- u2c
I0428 19:51:41.141815   966 net.cpp:408] conv_u2c-d -> u2d
I0428 19:51:41.147461   966 net.cpp:150] Setting up conv_u2c-d
I0428 19:51:41.147476   966 net.cpp:157] Top shape: 1 256 64 64 (1048576)
I0428 19:51:41.147480   966 net.cpp:165] Memory required for data: 981976896
I0428 19:51:41.147495   966 layer_factory.hpp:77] Creating layer bn_u2d
I0428 19:51:41.147503   966 net.cpp:100] Creating Layer bn_u2d
I0428 19:51:41.147506   966 net.cpp:434] bn_u2d <- u2d
I0428 19:51:41.147513   966 net.cpp:395] bn_u2d -> u2d (in-place)
I0428 19:51:41.147769   966 net.cpp:150] Setting up bn_u2d
I0428 19:51:41.147779   966 net.cpp:157] Top shape: 1 256 64 64 (1048576)
I0428 19:51:41.147783   966 net.cpp:165] Memory required for data: 986171200
I0428 19:51:41.147790   966 layer_factory.hpp:77] Creating layer sc_u2d
I0428 19:51:41.147796   966 net.cpp:100] Creating Layer sc_u2d
I0428 19:51:41.147800   966 net.cpp:434] sc_u2d <- u2d
I0428 19:51:41.147806   966 net.cpp:395] sc_u2d -> u2d (in-place)
I0428 19:51:41.147852   966 layer_factory.hpp:77] Creating layer sc_u2d
I0428 19:51:41.147992   966 net.cpp:150] Setting up sc_u2d
I0428 19:51:41.148002   966 net.cpp:157] Top shape: 1 256 64 64 (1048576)
I0428 19:51:41.148005   966 net.cpp:165] Memory required for data: 990365504
I0428 19:51:41.148010   966 layer_factory.hpp:77] Creating layer relu_u2d
I0428 19:51:41.148016   966 net.cpp:100] Creating Layer relu_u2d
I0428 19:51:41.148020   966 net.cpp:434] relu_u2d <- u2d
I0428 19:51:41.148025   966 net.cpp:395] relu_u2d -> u2d (in-place)
I0428 19:51:41.148247   966 net.cpp:150] Setting up relu_u2d
I0428 19:51:41.148257   966 net.cpp:157] Top shape: 1 256 64 64 (1048576)
I0428 19:51:41.148280   966 net.cpp:165] Memory required for data: 994559808
I0428 19:51:41.148284   966 layer_factory.hpp:77] Creating layer upconv_u2d_u1a
I0428 19:51:41.148291   966 net.cpp:100] Creating Layer upconv_u2d_u1a
I0428 19:51:41.148295   966 net.cpp:434] upconv_u2d_u1a <- u2d
I0428 19:51:41.148303   966 net.cpp:408] upconv_u2d_u1a -> u1a
I0428 19:51:41.149593   966 net.cpp:150] Setting up upconv_u2d_u1a
I0428 19:51:41.149603   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.149606   966 net.cpp:165] Memory required for data: 1002948416
I0428 19:51:41.149612   966 layer_factory.hpp:77] Creating layer bn_u1a
I0428 19:51:41.149618   966 net.cpp:100] Creating Layer bn_u1a
I0428 19:51:41.149621   966 net.cpp:434] bn_u1a <- u1a
I0428 19:51:41.149627   966 net.cpp:395] bn_u1a -> u1a (in-place)
I0428 19:51:41.149891   966 net.cpp:150] Setting up bn_u1a
I0428 19:51:41.149900   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.149904   966 net.cpp:165] Memory required for data: 1011337024
I0428 19:51:41.149910   966 layer_factory.hpp:77] Creating layer sc_u1a
I0428 19:51:41.149917   966 net.cpp:100] Creating Layer sc_u1a
I0428 19:51:41.149920   966 net.cpp:434] sc_u1a <- u1a
I0428 19:51:41.149926   966 net.cpp:395] sc_u1a -> u1a (in-place)
I0428 19:51:41.149972   966 layer_factory.hpp:77] Creating layer sc_u1a
I0428 19:51:41.150144   966 net.cpp:150] Setting up sc_u1a
I0428 19:51:41.150153   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.150156   966 net.cpp:165] Memory required for data: 1019725632
I0428 19:51:41.150161   966 layer_factory.hpp:77] Creating layer relu_u1a
I0428 19:51:41.150168   966 net.cpp:100] Creating Layer relu_u1a
I0428 19:51:41.150171   966 net.cpp:434] relu_u1a <- u1a
I0428 19:51:41.150177   966 net.cpp:395] relu_u1a -> u1a (in-place)
I0428 19:51:41.150395   966 net.cpp:150] Setting up relu_u1a
I0428 19:51:41.150406   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.150409   966 net.cpp:165] Memory required for data: 1028114240
I0428 19:51:41.150413   966 layer_factory.hpp:77] Creating layer u1a_relu_u1a_0_split
I0428 19:51:41.150418   966 net.cpp:100] Creating Layer u1a_relu_u1a_0_split
I0428 19:51:41.150421   966 net.cpp:434] u1a_relu_u1a_0_split <- u1a
I0428 19:51:41.150429   966 net.cpp:408] u1a_relu_u1a_0_split -> u1a_relu_u1a_0_split_0
I0428 19:51:41.150436   966 net.cpp:408] u1a_relu_u1a_0_split -> u1a_relu_u1a_0_split_1
I0428 19:51:41.150490   966 net.cpp:150] Setting up u1a_relu_u1a_0_split
I0428 19:51:41.150499   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.150503   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.150506   966 net.cpp:165] Memory required for data: 1044891456
I0428 19:51:41.150509   966 layer_factory.hpp:77] Creating layer crop_d1c-d1cc
I0428 19:51:41.150516   966 net.cpp:100] Creating Layer crop_d1c-d1cc
I0428 19:51:41.150527   966 net.cpp:434] crop_d1c-d1cc <- d1c_relu_d1c_0_split_1
I0428 19:51:41.150532   966 net.cpp:434] crop_d1c-d1cc <- u1a_relu_u1a_0_split_0
I0428 19:51:41.150537   966 net.cpp:408] crop_d1c-d1cc -> d1cc
I0428 19:51:41.150565   966 net.cpp:150] Setting up crop_d1c-d1cc
I0428 19:51:41.150573   966 net.cpp:157] Top shape: 1 128 128 128 (2097152)
I0428 19:51:41.150575   966 net.cpp:165] Memory required for data: 1053280064
I0428 19:51:41.150578   966 layer_factory.hpp:77] Creating layer concat_d1cc_u1a-b
I0428 19:51:41.150584   966 net.cpp:100] Creating Layer concat_d1cc_u1a-b
I0428 19:51:41.150588   966 net.cpp:434] concat_d1cc_u1a-b <- u1a_relu_u1a_0_split_1
I0428 19:51:41.150591   966 net.cpp:434] concat_d1cc_u1a-b <- d1cc
I0428 19:51:41.150596   966 net.cpp:408] concat_d1cc_u1a-b -> u1b
I0428 19:51:41.150625   966 net.cpp:150] Setting up concat_d1cc_u1a-b
I0428 19:51:41.150630   966 net.cpp:157] Top shape: 1 256 128 128 (4194304)
I0428 19:51:41.150632   966 net.cpp:165] Memory required for data: 1070057280
I0428 19:51:41.150635   966 layer_factory.hpp:77] Creating layer conv_u1b-c
I0428 19:51:41.150645   966 net.cpp:100] Creating Layer conv_u1b-c
I0428 19:51:41.150665   966 net.cpp:434] conv_u1b-c <- u1b
I0428 19:51:41.150673   966 net.cpp:408] conv_u1b-c -> u1c
I0428 19:51:41.154073   966 net.cpp:150] Setting up conv_u1b-c
I0428 19:51:41.154090   966 net.cpp:157] Top shape: 1 128 126 126 (2032128)
I0428 19:51:41.154093   966 net.cpp:165] Memory required for data: 1078185792
I0428 19:51:41.154101   966 layer_factory.hpp:77] Creating layer bn_u1c
I0428 19:51:41.154117   966 net.cpp:100] Creating Layer bn_u1c
I0428 19:51:41.154122   966 net.cpp:434] bn_u1c <- u1c
I0428 19:51:41.154129   966 net.cpp:395] bn_u1c -> u1c (in-place)
I0428 19:51:41.155401   966 net.cpp:150] Setting up bn_u1c
I0428 19:51:41.155416   966 net.cpp:157] Top shape: 1 128 126 126 (2032128)
I0428 19:51:41.155421   966 net.cpp:165] Memory required for data: 1086314304
I0428 19:51:41.155429   966 layer_factory.hpp:77] Creating layer sc_u1c
I0428 19:51:41.155437   966 net.cpp:100] Creating Layer sc_u1c
I0428 19:51:41.155443   966 net.cpp:434] sc_u1c <- u1c
I0428 19:51:41.155452   966 net.cpp:395] sc_u1c -> u1c (in-place)
I0428 19:51:41.155503   966 layer_factory.hpp:77] Creating layer sc_u1c
I0428 19:51:41.155670   966 net.cpp:150] Setting up sc_u1c
I0428 19:51:41.155678   966 net.cpp:157] Top shape: 1 128 126 126 (2032128)
I0428 19:51:41.155683   966 net.cpp:165] Memory required for data: 1094442816
I0428 19:51:41.155689   966 layer_factory.hpp:77] Creating layer relu_u1c
I0428 19:51:41.155694   966 net.cpp:100] Creating Layer relu_u1c
I0428 19:51:41.155696   966 net.cpp:434] relu_u1c <- u1c
I0428 19:51:41.155702   966 net.cpp:395] relu_u1c -> u1c (in-place)
I0428 19:51:41.155936   966 net.cpp:150] Setting up relu_u1c
I0428 19:51:41.155946   966 net.cpp:157] Top shape: 1 128 126 126 (2032128)
I0428 19:51:41.155948   966 net.cpp:165] Memory required for data: 1102571328
I0428 19:51:41.155952   966 layer_factory.hpp:77] Creating layer conv_u1c-d
I0428 19:51:41.155962   966 net.cpp:100] Creating Layer conv_u1c-d
I0428 19:51:41.155966   966 net.cpp:434] conv_u1c-d <- u1c
I0428 19:51:41.155972   966 net.cpp:408] conv_u1c-d -> u1d
I0428 19:51:41.157347   966 net.cpp:150] Setting up conv_u1c-d
I0428 19:51:41.157357   966 net.cpp:157] Top shape: 1 128 124 124 (1968128)
I0428 19:51:41.157361   966 net.cpp:165] Memory required for data: 1110443840
I0428 19:51:41.157366   966 layer_factory.hpp:77] Creating layer bn_u1d
I0428 19:51:41.157372   966 net.cpp:100] Creating Layer bn_u1d
I0428 19:51:41.157376   966 net.cpp:434] bn_u1d <- u1d
I0428 19:51:41.157380   966 net.cpp:395] bn_u1d -> u1d (in-place)
I0428 19:51:41.157657   966 net.cpp:150] Setting up bn_u1d
I0428 19:51:41.157667   966 net.cpp:157] Top shape: 1 128 124 124 (1968128)
I0428 19:51:41.157670   966 net.cpp:165] Memory required for data: 1118316352
I0428 19:51:41.157691   966 layer_factory.hpp:77] Creating layer sc_u1d
I0428 19:51:41.157708   966 net.cpp:100] Creating Layer sc_u1d
I0428 19:51:41.157712   966 net.cpp:434] sc_u1d <- u1d
I0428 19:51:41.157718   966 net.cpp:395] sc_u1d -> u1d (in-place)
I0428 19:51:41.157766   966 layer_factory.hpp:77] Creating layer sc_u1d
I0428 19:51:41.157932   966 net.cpp:150] Setting up sc_u1d
I0428 19:51:41.157940   966 net.cpp:157] Top shape: 1 128 124 124 (1968128)
I0428 19:51:41.157944   966 net.cpp:165] Memory required for data: 1126188864
I0428 19:51:41.157950   966 layer_factory.hpp:77] Creating layer relu_u1d
I0428 19:51:41.157955   966 net.cpp:100] Creating Layer relu_u1d
I0428 19:51:41.157958   966 net.cpp:434] relu_u1d <- u1d
I0428 19:51:41.157964   966 net.cpp:395] relu_u1d -> u1d (in-place)
I0428 19:51:41.159029   966 net.cpp:150] Setting up relu_u1d
I0428 19:51:41.159044   966 net.cpp:157] Top shape: 1 128 124 124 (1968128)
I0428 19:51:41.159047   966 net.cpp:165] Memory required for data: 1134061376
I0428 19:51:41.159051   966 layer_factory.hpp:77] Creating layer upconv_u1d_u0a
I0428 19:51:41.159060   966 net.cpp:100] Creating Layer upconv_u1d_u0a
I0428 19:51:41.159065   966 net.cpp:434] upconv_u1d_u0a <- u1d
I0428 19:51:41.159071   966 net.cpp:408] upconv_u1d_u0a -> u0a
I0428 19:51:41.159945   966 net.cpp:150] Setting up upconv_u1d_u0a
I0428 19:51:41.159955   966 net.cpp:157] Top shape: 1 128 248 248 (7872512)
I0428 19:51:41.159958   966 net.cpp:165] Memory required for data: 1165551424
I0428 19:51:41.159965   966 layer_factory.hpp:77] Creating layer bn_u0a
I0428 19:51:41.159971   966 net.cpp:100] Creating Layer bn_u0a
I0428 19:51:41.159974   966 net.cpp:434] bn_u0a <- u0a
I0428 19:51:41.159981   966 net.cpp:395] bn_u0a -> u0a (in-place)
I0428 19:51:41.160292   966 net.cpp:150] Setting up bn_u0a
I0428 19:51:41.160303   966 net.cpp:157] Top shape: 1 128 248 248 (7872512)
I0428 19:51:41.160306   966 net.cpp:165] Memory required for data: 1197041472
I0428 19:51:41.160313   966 layer_factory.hpp:77] Creating layer sc_u0a
I0428 19:51:41.160320   966 net.cpp:100] Creating Layer sc_u0a
I0428 19:51:41.160323   966 net.cpp:434] sc_u0a <- u0a
I0428 19:51:41.160329   966 net.cpp:395] sc_u0a -> u0a (in-place)
I0428 19:51:41.160377   966 layer_factory.hpp:77] Creating layer sc_u0a
I0428 19:51:41.160630   966 net.cpp:150] Setting up sc_u0a
I0428 19:51:41.160640   966 net.cpp:157] Top shape: 1 128 248 248 (7872512)
I0428 19:51:41.160643   966 net.cpp:165] Memory required for data: 1228531520
I0428 19:51:41.160650   966 layer_factory.hpp:77] Creating layer relu_u0a
I0428 19:51:41.160655   966 net.cpp:100] Creating Layer relu_u0a
I0428 19:51:41.160658   966 net.cpp:434] relu_u0a <- u0a
I0428 19:51:41.160663   966 net.cpp:395] relu_u0a -> u0a (in-place)
I0428 19:51:41.160879   966 net.cpp:150] Setting up relu_u0a
I0428 19:51:41.160890   966 net.cpp:157] Top shape: 1 128 248 248 (7872512)
I0428 19:51:41.160894   966 net.cpp:165] Memory required for data: 1260021568
I0428 19:51:41.160897   966 layer_factory.hpp:77] Creating layer u0a_relu_u0a_0_split
I0428 19:51:41.160903   966 net.cpp:100] Creating Layer u0a_relu_u0a_0_split
I0428 19:51:41.160907   966 net.cpp:434] u0a_relu_u0a_0_split <- u0a
I0428 19:51:41.160913   966 net.cpp:408] u0a_relu_u0a_0_split -> u0a_relu_u0a_0_split_0
I0428 19:51:41.160922   966 net.cpp:408] u0a_relu_u0a_0_split -> u0a_relu_u0a_0_split_1
I0428 19:51:41.160974   966 net.cpp:150] Setting up u0a_relu_u0a_0_split
I0428 19:51:41.160981   966 net.cpp:157] Top shape: 1 128 248 248 (7872512)
I0428 19:51:41.160986   966 net.cpp:157] Top shape: 1 128 248 248 (7872512)
I0428 19:51:41.160989   966 net.cpp:165] Memory required for data: 1323001664
I0428 19:51:41.160991   966 layer_factory.hpp:77] Creating layer crop_d0c-d0cc
I0428 19:51:41.160997   966 net.cpp:100] Creating Layer crop_d0c-d0cc
I0428 19:51:41.161001   966 net.cpp:434] crop_d0c-d0cc <- d0c_relu_d0c_0_split_1
I0428 19:51:41.161006   966 net.cpp:434] crop_d0c-d0cc <- u0a_relu_u0a_0_split_0
I0428 19:51:41.161012   966 net.cpp:408] crop_d0c-d0cc -> d0cc
I0428 19:51:41.161041   966 net.cpp:150] Setting up crop_d0c-d0cc
I0428 19:51:41.161048   966 net.cpp:157] Top shape: 1 64 248 248 (3936256)
I0428 19:51:41.161051   966 net.cpp:165] Memory required for data: 1338746688
I0428 19:51:41.161053   966 layer_factory.hpp:77] Creating layer concat_d0cc_u0a-b
I0428 19:51:41.161058   966 net.cpp:100] Creating Layer concat_d0cc_u0a-b
I0428 19:51:41.161062   966 net.cpp:434] concat_d0cc_u0a-b <- u0a_relu_u0a_0_split_1
I0428 19:51:41.161067   966 net.cpp:434] concat_d0cc_u0a-b <- d0cc
I0428 19:51:41.161072   966 net.cpp:408] concat_d0cc_u0a-b -> u0b
I0428 19:51:41.161100   966 net.cpp:150] Setting up concat_d0cc_u0a-b
I0428 19:51:41.161108   966 net.cpp:157] Top shape: 1 192 248 248 (11808768)
I0428 19:51:41.161111   966 net.cpp:165] Memory required for data: 1385981760
I0428 19:51:41.161114   966 layer_factory.hpp:77] Creating layer conv_u0b-c
I0428 19:51:41.161123   966 net.cpp:100] Creating Layer conv_u0b-c
I0428 19:51:41.161126   966 net.cpp:434] conv_u0b-c <- u0b
I0428 19:51:41.161133   966 net.cpp:408] conv_u0b-c -> u0c
I0428 19:51:41.162327   966 net.cpp:150] Setting up conv_u0b-c
I0428 19:51:41.162336   966 net.cpp:157] Top shape: 1 64 246 246 (3873024)
I0428 19:51:41.162340   966 net.cpp:165] Memory required for data: 1401473856
I0428 19:51:41.162360   966 layer_factory.hpp:77] Creating layer bn_u0c
I0428 19:51:41.162366   966 net.cpp:100] Creating Layer bn_u0c
I0428 19:51:41.162370   966 net.cpp:434] bn_u0c <- u0c
I0428 19:51:41.162376   966 net.cpp:395] bn_u0c -> u0c (in-place)
I0428 19:51:41.163740   966 net.cpp:150] Setting up bn_u0c
I0428 19:51:41.163755   966 net.cpp:157] Top shape: 1 64 246 246 (3873024)
I0428 19:51:41.163769   966 net.cpp:165] Memory required for data: 1416965952
I0428 19:51:41.163779   966 layer_factory.hpp:77] Creating layer sc_u0c
I0428 19:51:41.163786   966 net.cpp:100] Creating Layer sc_u0c
I0428 19:51:41.163789   966 net.cpp:434] sc_u0c <- u0c
I0428 19:51:41.163795   966 net.cpp:395] sc_u0c -> u0c (in-place)
I0428 19:51:41.163851   966 layer_factory.hpp:77] Creating layer sc_u0c
I0428 19:51:41.164105   966 net.cpp:150] Setting up sc_u0c
I0428 19:51:41.164114   966 net.cpp:157] Top shape: 1 64 246 246 (3873024)
I0428 19:51:41.164118   966 net.cpp:165] Memory required for data: 1432458048
I0428 19:51:41.164124   966 layer_factory.hpp:77] Creating layer relu_u0c
I0428 19:51:41.164129   966 net.cpp:100] Creating Layer relu_u0c
I0428 19:51:41.164132   966 net.cpp:434] relu_u0c <- u0c
I0428 19:51:41.164137   966 net.cpp:395] relu_u0c -> u0c (in-place)
I0428 19:51:41.164368   966 net.cpp:150] Setting up relu_u0c
I0428 19:51:41.164378   966 net.cpp:157] Top shape: 1 64 246 246 (3873024)
I0428 19:51:41.164381   966 net.cpp:165] Memory required for data: 1447950144
I0428 19:51:41.164384   966 layer_factory.hpp:77] Creating layer conv_u0c-d
I0428 19:51:41.164394   966 net.cpp:100] Creating Layer conv_u0c-d
I0428 19:51:41.164398   966 net.cpp:434] conv_u0c-d <- u0c
I0428 19:51:41.164404   966 net.cpp:408] conv_u0c-d -> u0d
I0428 19:51:41.165052   966 net.cpp:150] Setting up conv_u0c-d
I0428 19:51:41.165063   966 net.cpp:157] Top shape: 1 64 244 244 (3810304)
I0428 19:51:41.165066   966 net.cpp:165] Memory required for data: 1463191360
I0428 19:51:41.165072   966 layer_factory.hpp:77] Creating layer bn_u0d
I0428 19:51:41.165079   966 net.cpp:100] Creating Layer bn_u0d
I0428 19:51:41.165083   966 net.cpp:434] bn_u0d <- u0d
I0428 19:51:41.165087   966 net.cpp:395] bn_u0d -> u0d (in-place)
I0428 19:51:41.165398   966 net.cpp:150] Setting up bn_u0d
I0428 19:51:41.165407   966 net.cpp:157] Top shape: 1 64 244 244 (3810304)
I0428 19:51:41.165410   966 net.cpp:165] Memory required for data: 1478432576
I0428 19:51:41.165417   966 layer_factory.hpp:77] Creating layer sc_u0d
I0428 19:51:41.165423   966 net.cpp:100] Creating Layer sc_u0d
I0428 19:51:41.165426   966 net.cpp:434] sc_u0d <- u0d
I0428 19:51:41.165431   966 net.cpp:395] sc_u0d -> u0d (in-place)
I0428 19:51:41.165501   966 layer_factory.hpp:77] Creating layer sc_u0d
I0428 19:51:41.165748   966 net.cpp:150] Setting up sc_u0d
I0428 19:51:41.165758   966 net.cpp:157] Top shape: 1 64 244 244 (3810304)
I0428 19:51:41.165761   966 net.cpp:165] Memory required for data: 1493673792
I0428 19:51:41.165767   966 layer_factory.hpp:77] Creating layer relu_u0d
I0428 19:51:41.165772   966 net.cpp:100] Creating Layer relu_u0d
I0428 19:51:41.165776   966 net.cpp:434] relu_u0d <- u0d
I0428 19:51:41.165781   966 net.cpp:395] relu_u0d -> u0d (in-place)
I0428 19:51:41.166002   966 net.cpp:150] Setting up relu_u0d
I0428 19:51:41.166012   966 net.cpp:157] Top shape: 1 64 244 244 (3810304)
I0428 19:51:41.166014   966 net.cpp:165] Memory required for data: 1508915008
I0428 19:51:41.166018   966 layer_factory.hpp:77] Creating layer conv_u0d-score
I0428 19:51:41.166026   966 net.cpp:100] Creating Layer conv_u0d-score
I0428 19:51:41.166029   966 net.cpp:434] conv_u0d-score <- u0d
I0428 19:51:41.166035   966 net.cpp:408] conv_u0d-score -> score
I0428 19:51:41.166411   966 net.cpp:150] Setting up conv_u0d-score
I0428 19:51:41.166420   966 net.cpp:157] Top shape: 1 3 244 244 (178608)
I0428 19:51:41.166424   966 net.cpp:165] Memory required for data: 1509629440
I0428 19:51:41.166429   966 layer_factory.hpp:77] Creating layer loss
I0428 19:51:41.166440   966 net.cpp:100] Creating Layer loss
I0428 19:51:41.166457   966 net.cpp:434] loss <- score
I0428 19:51:41.166470   966 net.cpp:434] loss <- label
I0428 19:51:41.166473   966 net.cpp:434] loss <- weights
I0428 19:51:41.166479   966 net.cpp:408] loss -> loss
I0428 19:51:41.166487   966 layer_factory.hpp:77] Creating layer loss
I0428 19:51:41.168790   966 net.cpp:150] Setting up loss
I0428 19:51:41.168807   966 net.cpp:157] Top shape: (1)
I0428 19:51:41.168810   966 net.cpp:160]     with loss weight 1
I0428 19:51:41.168822   966 net.cpp:165] Memory required for data: 1509629444
I0428 19:51:41.168825   966 net.cpp:226] loss needs backward computation.
I0428 19:51:41.168830   966 net.cpp:226] conv_u0d-score needs backward computation.
I0428 19:51:41.168834   966 net.cpp:226] relu_u0d needs backward computation.
I0428 19:51:41.168838   966 net.cpp:226] sc_u0d needs backward computation.
I0428 19:51:41.168839   966 net.cpp:226] bn_u0d needs backward computation.
I0428 19:51:41.168843   966 net.cpp:226] conv_u0c-d needs backward computation.
I0428 19:51:41.168846   966 net.cpp:226] relu_u0c needs backward computation.
I0428 19:51:41.168849   966 net.cpp:226] sc_u0c needs backward computation.
I0428 19:51:41.168851   966 net.cpp:226] bn_u0c needs backward computation.
I0428 19:51:41.168854   966 net.cpp:226] conv_u0b-c needs backward computation.
I0428 19:51:41.168858   966 net.cpp:226] concat_d0cc_u0a-b needs backward computation.
I0428 19:51:41.168861   966 net.cpp:226] crop_d0c-d0cc needs backward computation.
I0428 19:51:41.168865   966 net.cpp:226] u0a_relu_u0a_0_split needs backward computation.
I0428 19:51:41.168869   966 net.cpp:226] relu_u0a needs backward computation.
I0428 19:51:41.168872   966 net.cpp:226] sc_u0a needs backward computation.
I0428 19:51:41.168875   966 net.cpp:226] bn_u0a needs backward computation.
I0428 19:51:41.168877   966 net.cpp:226] upconv_u1d_u0a needs backward computation.
I0428 19:51:41.168880   966 net.cpp:226] relu_u1d needs backward computation.
I0428 19:51:41.168884   966 net.cpp:226] sc_u1d needs backward computation.
I0428 19:51:41.168886   966 net.cpp:226] bn_u1d needs backward computation.
I0428 19:51:41.168889   966 net.cpp:226] conv_u1c-d needs backward computation.
I0428 19:51:41.168892   966 net.cpp:226] relu_u1c needs backward computation.
I0428 19:51:41.168895   966 net.cpp:226] sc_u1c needs backward computation.
I0428 19:51:41.168898   966 net.cpp:226] bn_u1c needs backward computation.
I0428 19:51:41.168900   966 net.cpp:226] conv_u1b-c needs backward computation.
I0428 19:51:41.168905   966 net.cpp:226] concat_d1cc_u1a-b needs backward computation.
I0428 19:51:41.168908   966 net.cpp:226] crop_d1c-d1cc needs backward computation.
I0428 19:51:41.168912   966 net.cpp:226] u1a_relu_u1a_0_split needs backward computation.
I0428 19:51:41.168915   966 net.cpp:226] relu_u1a needs backward computation.
I0428 19:51:41.168918   966 net.cpp:226] sc_u1a needs backward computation.
I0428 19:51:41.168921   966 net.cpp:226] bn_u1a needs backward computation.
I0428 19:51:41.168925   966 net.cpp:226] upconv_u2d_u1a needs backward computation.
I0428 19:51:41.168927   966 net.cpp:226] relu_u2d needs backward computation.
I0428 19:51:41.168929   966 net.cpp:226] sc_u2d needs backward computation.
I0428 19:51:41.168933   966 net.cpp:226] bn_u2d needs backward computation.
I0428 19:51:41.168936   966 net.cpp:226] conv_u2c-d needs backward computation.
I0428 19:51:41.168938   966 net.cpp:226] relu_u2c needs backward computation.
I0428 19:51:41.168941   966 net.cpp:226] sc_u2c needs backward computation.
I0428 19:51:41.168944   966 net.cpp:226] bn_u2c needs backward computation.
I0428 19:51:41.168954   966 net.cpp:226] conv_u2b-c needs backward computation.
I0428 19:51:41.168956   966 net.cpp:226] concat_d2cc_u2a-b needs backward computation.
I0428 19:51:41.168961   966 net.cpp:226] crop_d2c-d2cc needs backward computation.
I0428 19:51:41.168965   966 net.cpp:226] u2a_relu_u2a_0_split needs backward computation.
I0428 19:51:41.168968   966 net.cpp:226] relu_u2a needs backward computation.
I0428 19:51:41.168970   966 net.cpp:226] sc_u2a needs backward computation.
I0428 19:51:41.168987   966 net.cpp:226] bn_u2a needs backward computation.
I0428 19:51:41.168992   966 net.cpp:226] upconv_u3d_u2a needs backward computation.
I0428 19:51:41.168994   966 net.cpp:226] relu_u3d needs backward computation.
I0428 19:51:41.168997   966 net.cpp:226] sc_u3d needs backward computation.
I0428 19:51:41.168999   966 net.cpp:226] bn_u3d needs backward computation.
I0428 19:51:41.169003   966 net.cpp:226] conv_u3c-d needs backward computation.
I0428 19:51:41.169006   966 net.cpp:226] relu_u3c needs backward computation.
I0428 19:51:41.169008   966 net.cpp:226] sc_u3c needs backward computation.
I0428 19:51:41.169011   966 net.cpp:226] bn_u3c needs backward computation.
I0428 19:51:41.169014   966 net.cpp:226] conv_u3b-c needs backward computation.
I0428 19:51:41.169018   966 net.cpp:226] concat_d3cc_u3a-b needs backward computation.
I0428 19:51:41.169021   966 net.cpp:226] crop_d3c-d3cc needs backward computation.
I0428 19:51:41.169026   966 net.cpp:226] u3a_relu_u3a_0_split needs backward computation.
I0428 19:51:41.169030   966 net.cpp:226] relu_u3a needs backward computation.
I0428 19:51:41.169034   966 net.cpp:226] sc_u3a needs backward computation.
I0428 19:51:41.169035   966 net.cpp:226] bn_u3a needs backward computation.
I0428 19:51:41.169039   966 net.cpp:226] upconv_d4c_u3a needs backward computation.
I0428 19:51:41.169044   966 net.cpp:226] relu_d4c needs backward computation.
I0428 19:51:41.169045   966 net.cpp:226] sc_d4c needs backward computation.
I0428 19:51:41.169049   966 net.cpp:226] bn_d4c needs backward computation.
I0428 19:51:41.169051   966 net.cpp:226] conv_d4b-c needs backward computation.
I0428 19:51:41.169055   966 net.cpp:226] relu_d4b needs backward computation.
I0428 19:51:41.169059   966 net.cpp:226] sc_d4b needs backward computation.
I0428 19:51:41.169060   966 net.cpp:226] bn_d4b needs backward computation.
I0428 19:51:41.169064   966 net.cpp:226] conv_d4a-b needs backward computation.
I0428 19:51:41.169067   966 net.cpp:226] pool_d3c-4a needs backward computation.
I0428 19:51:41.169070   966 net.cpp:226] d3c_relu_d3c_0_split needs backward computation.
I0428 19:51:41.169073   966 net.cpp:226] relu_d3c needs backward computation.
I0428 19:51:41.169076   966 net.cpp:226] sc_d3c needs backward computation.
I0428 19:51:41.169080   966 net.cpp:226] bn_d3c needs backward computation.
I0428 19:51:41.169083   966 net.cpp:226] conv_d3b-c needs backward computation.
I0428 19:51:41.169086   966 net.cpp:226] relu_d3b needs backward computation.
I0428 19:51:41.169088   966 net.cpp:226] sc_d3b needs backward computation.
I0428 19:51:41.169092   966 net.cpp:226] bn_d3b needs backward computation.
I0428 19:51:41.169095   966 net.cpp:226] conv_d3a-b needs backward computation.
I0428 19:51:41.169098   966 net.cpp:226] pool_d2c-3a needs backward computation.
I0428 19:51:41.169101   966 net.cpp:226] d2c_relu_d2c_0_split needs backward computation.
I0428 19:51:41.169106   966 net.cpp:226] relu_d2c needs backward computation.
I0428 19:51:41.169108   966 net.cpp:226] sc_d2c needs backward computation.
I0428 19:51:41.169111   966 net.cpp:226] bn_d2c needs backward computation.
I0428 19:51:41.169114   966 net.cpp:226] conv_d2b-c needs backward computation.
I0428 19:51:41.169118   966 net.cpp:226] relu_d2b needs backward computation.
I0428 19:51:41.169121   966 net.cpp:226] sc_d2b needs backward computation.
I0428 19:51:41.169124   966 net.cpp:226] bn_d2b needs backward computation.
I0428 19:51:41.169126   966 net.cpp:226] conv_d2a-b needs backward computation.
I0428 19:51:41.169131   966 net.cpp:226] pool_d1c-2a needs backward computation.
I0428 19:51:41.169133   966 net.cpp:226] d1c_relu_d1c_0_split needs backward computation.
I0428 19:51:41.169137   966 net.cpp:226] relu_d1c needs backward computation.
I0428 19:51:41.169139   966 net.cpp:226] sc_d1c needs backward computation.
I0428 19:51:41.169143   966 net.cpp:226] bn_d1c needs backward computation.
I0428 19:51:41.169145   966 net.cpp:226] conv_d1b-c needs backward computation.
I0428 19:51:41.169148   966 net.cpp:226] relu_d1b needs backward computation.
I0428 19:51:41.169157   966 net.cpp:226] sc_d1b needs backward computation.
I0428 19:51:41.169162   966 net.cpp:226] bn_d1b needs backward computation.
I0428 19:51:41.169164   966 net.cpp:226] conv_d1a-b needs backward computation.
I0428 19:51:41.169167   966 net.cpp:226] pool_d0c-1a needs backward computation.
I0428 19:51:41.169170   966 net.cpp:226] d0c_relu_d0c_0_split needs backward computation.
I0428 19:51:41.169174   966 net.cpp:226] relu_d0c needs backward computation.
I0428 19:51:41.169178   966 net.cpp:226] sc_d0c needs backward computation.
I0428 19:51:41.169180   966 net.cpp:226] bn_d0c needs backward computation.
I0428 19:51:41.169183   966 net.cpp:226] conv_d0b-c needs backward computation.
I0428 19:51:41.169188   966 net.cpp:226] relu_d0b needs backward computation.
I0428 19:51:41.169189   966 net.cpp:226] sc_d0b needs backward computation.
I0428 19:51:41.169193   966 net.cpp:226] bn_d0b needs backward computation.
I0428 19:51:41.169195   966 net.cpp:226] conv_d0a-b needs backward computation.
I0428 19:51:41.169200   966 net.cpp:228] loaddata does not need backward computation.
I0428 19:51:41.169211   966 net.cpp:270] This network produces output loss
I0428 19:51:41.169292   966 net.cpp:283] Network initialization done.
I0428 19:51:41.169651   966 solver.cpp:60] Solver scaffolding done.
I0428 19:51:41.195935   966 solver.cpp:337] Iteration 0, Testing net (#0)
I0428 19:51:41.221303   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 19:51:41.221319   966 net.cpp:693] Ignoring source layer visualize
I0428 19:51:41.221323   966 net.cpp:693] Ignoring source layer fake
I0428 19:57:16.470649   966 solver.cpp:404]     Test net output #0: loss = 1.18435 (* 1 = 1.18435 loss)
I0428 19:57:16.987993   966 solver.cpp:228] Iteration 0, loss = 1.1629
I0428 19:57:16.988011   966 solver.cpp:244]     Train net output #0: loss = 1.1629 (* 1 = 1.1629 loss)
I0428 19:57:16.988034   966 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0428 19:58:07.208554   966 solver.cpp:228] Iteration 100, loss = 0.165841
I0428 19:58:07.208719   966 solver.cpp:244]     Train net output #0: loss = 0.165841 (* 1 = 0.165841 loss)
I0428 19:58:07.208726   966 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0428 19:58:57.560670   966 solver.cpp:228] Iteration 200, loss = 0.162687
I0428 19:58:57.562804   966 solver.cpp:244]     Train net output #0: loss = 0.162687 (* 1 = 0.162687 loss)
I0428 19:58:57.562813   966 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0428 20:00:02.438911   966 solver.cpp:228] Iteration 300, loss = 0.254739
I0428 20:00:02.439085   966 solver.cpp:244]     Train net output #0: loss = 0.254739 (* 1 = 0.254739 loss)
I0428 20:00:02.439092   966 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0428 20:00:52.751343   966 solver.cpp:228] Iteration 400, loss = 0.256874
I0428 20:00:52.751509   966 solver.cpp:244]     Train net output #0: loss = 0.256874 (* 1 = 0.256874 loss)
I0428 20:00:52.751515   966 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0428 20:01:43.110940   966 solver.cpp:228] Iteration 500, loss = 0.218963
I0428 20:01:43.111096   966 solver.cpp:244]     Train net output #0: loss = 0.218963 (* 1 = 0.218963 loss)
I0428 20:01:43.111102   966 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0428 20:02:48.071925   966 solver.cpp:228] Iteration 600, loss = 0.150612
I0428 20:02:48.072100   966 solver.cpp:244]     Train net output #0: loss = 0.150612 (* 1 = 0.150612 loss)
I0428 20:02:48.072108   966 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0428 20:03:38.420317   966 solver.cpp:228] Iteration 700, loss = 0.175836
I0428 20:03:38.420511   966 solver.cpp:244]     Train net output #0: loss = 0.175836 (* 1 = 0.175836 loss)
I0428 20:03:38.420518   966 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0428 20:04:45.617988   966 solver.cpp:228] Iteration 800, loss = 0.169527
I0428 20:04:45.618154   966 solver.cpp:244]     Train net output #0: loss = 0.169527 (* 1 = 0.169527 loss)
I0428 20:04:45.618160   966 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0428 20:05:35.826659   966 solver.cpp:228] Iteration 900, loss = 0.837144
I0428 20:05:35.826833   966 solver.cpp:244]     Train net output #0: loss = 0.837144 (* 1 = 0.837144 loss)
I0428 20:05:35.826838   966 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0428 20:06:25.862257   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_1000.caffemodel
I0428 20:06:31.626921   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_1000.solverstate
I0428 20:06:31.826586   966 solver.cpp:337] Iteration 1000, Testing net (#0)
I0428 20:06:31.826710   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 20:06:31.826717   966 net.cpp:693] Ignoring source layer visualize
I0428 20:06:31.826719   966 net.cpp:693] Ignoring source layer fake
I0428 20:11:30.880439   966 solver.cpp:404]     Test net output #0: loss = 0.412976 (* 1 = 0.412976 loss)
I0428 20:11:31.195904   966 solver.cpp:228] Iteration 1000, loss = 0.524494
I0428 20:11:31.195924   966 solver.cpp:244]     Train net output #0: loss = 0.524494 (* 1 = 0.524494 loss)
I0428 20:11:31.195946   966 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0428 20:12:21.586642   966 solver.cpp:228] Iteration 1100, loss = 0.54389
I0428 20:12:21.586800   966 solver.cpp:244]     Train net output #0: loss = 0.54389 (* 1 = 0.54389 loss)
I0428 20:12:21.586807   966 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0428 20:13:30.671249   966 solver.cpp:228] Iteration 1200, loss = 0.188295
I0428 20:13:30.671445   966 solver.cpp:244]     Train net output #0: loss = 0.188295 (* 1 = 0.188295 loss)
I0428 20:13:30.671452   966 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0428 20:14:21.046900   966 solver.cpp:228] Iteration 1300, loss = 0.287676
I0428 20:14:21.047734   966 solver.cpp:244]     Train net output #0: loss = 0.287676 (* 1 = 0.287676 loss)
I0428 20:14:21.047758   966 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0428 20:15:11.448616   966 solver.cpp:228] Iteration 1400, loss = 0.22332
I0428 20:15:11.448760   966 solver.cpp:244]     Train net output #0: loss = 0.22332 (* 1 = 0.22332 loss)
I0428 20:15:11.448766   966 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0428 20:16:20.864361   966 solver.cpp:228] Iteration 1500, loss = 0.261546
I0428 20:16:20.864552   966 solver.cpp:244]     Train net output #0: loss = 0.261546 (* 1 = 0.261546 loss)
I0428 20:16:20.864558   966 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0428 20:17:11.163195   966 solver.cpp:228] Iteration 1600, loss = 0.5259
I0428 20:17:11.163341   966 solver.cpp:244]     Train net output #0: loss = 0.5259 (* 1 = 0.5259 loss)
I0428 20:17:11.163347   966 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0428 20:18:01.563966   966 solver.cpp:228] Iteration 1700, loss = 0.24514
I0428 20:18:01.564118   966 solver.cpp:244]     Train net output #0: loss = 0.24514 (* 1 = 0.24514 loss)
I0428 20:18:01.564126   966 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0428 20:18:51.972057   966 solver.cpp:228] Iteration 1800, loss = 0.204432
I0428 20:18:51.972209   966 solver.cpp:244]     Train net output #0: loss = 0.204432 (* 1 = 0.204432 loss)
I0428 20:18:51.972216   966 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0428 20:19:59.723847   966 solver.cpp:228] Iteration 1900, loss = 0.415188
I0428 20:19:59.724006   966 solver.cpp:244]     Train net output #0: loss = 0.415188 (* 1 = 0.415188 loss)
I0428 20:19:59.724014   966 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0428 20:20:49.789448   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_2000.caffemodel
I0428 20:21:02.629597   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_2000.solverstate
I0428 20:21:02.832234   966 solver.cpp:337] Iteration 2000, Testing net (#0)
I0428 20:21:02.832358   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 20:21:02.832365   966 net.cpp:693] Ignoring source layer visualize
I0428 20:21:02.832367   966 net.cpp:693] Ignoring source layer fake
I0428 20:26:02.693734   966 solver.cpp:404]     Test net output #0: loss = 0.402925 (* 1 = 0.402925 loss)
I0428 20:26:03.009223   966 solver.cpp:228] Iteration 2000, loss = 0.345722
I0428 20:26:03.009263   966 solver.cpp:244]     Train net output #0: loss = 0.345722 (* 1 = 0.345722 loss)
I0428 20:26:03.009270   966 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0428 20:26:53.431155   966 solver.cpp:228] Iteration 2100, loss = 0.146427
I0428 20:26:53.431344   966 solver.cpp:244]     Train net output #0: loss = 0.146427 (* 1 = 0.146427 loss)
I0428 20:26:53.431351   966 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0428 20:28:00.803447   966 solver.cpp:228] Iteration 2200, loss = 0.170676
I0428 20:28:00.803623   966 solver.cpp:244]     Train net output #0: loss = 0.170676 (* 1 = 0.170676 loss)
I0428 20:28:00.803630   966 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0428 20:28:51.144217   966 solver.cpp:228] Iteration 2300, loss = 0.0678184
I0428 20:28:51.144407   966 solver.cpp:244]     Train net output #0: loss = 0.0678184 (* 1 = 0.0678184 loss)
I0428 20:28:51.144415   966 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0428 20:29:41.549229   966 solver.cpp:228] Iteration 2400, loss = 0.221229
I0428 20:29:41.549414   966 solver.cpp:244]     Train net output #0: loss = 0.221229 (* 1 = 0.221229 loss)
I0428 20:29:41.549422   966 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0428 20:30:48.873734   966 solver.cpp:228] Iteration 2500, loss = 0.20373
I0428 20:30:48.873885   966 solver.cpp:244]     Train net output #0: loss = 0.20373 (* 1 = 0.20373 loss)
I0428 20:30:48.873893   966 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0428 20:31:39.170536   966 solver.cpp:228] Iteration 2600, loss = 0.321117
I0428 20:31:39.170722   966 solver.cpp:244]     Train net output #0: loss = 0.321117 (* 1 = 0.321117 loss)
I0428 20:31:39.170732   966 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0428 20:32:29.571826   966 solver.cpp:228] Iteration 2700, loss = 0.285632
I0428 20:32:29.571971   966 solver.cpp:244]     Train net output #0: loss = 0.285632 (* 1 = 0.285632 loss)
I0428 20:32:29.571979   966 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0428 20:33:19.991861   966 solver.cpp:228] Iteration 2800, loss = 0.0996031
I0428 20:33:19.992015   966 solver.cpp:244]     Train net output #0: loss = 0.0996031 (* 1 = 0.0996031 loss)
I0428 20:33:19.992022   966 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0428 20:34:25.806613   966 solver.cpp:228] Iteration 2900, loss = 0.15274
I0428 20:34:25.806769   966 solver.cpp:244]     Train net output #0: loss = 0.15274 (* 1 = 0.15274 loss)
I0428 20:34:25.806777   966 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0428 20:35:15.889304   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_3000.caffemodel
I0428 20:35:26.010866   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_3000.solverstate
I0428 20:35:26.215648   966 solver.cpp:337] Iteration 3000, Testing net (#0)
I0428 20:35:26.215772   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 20:35:26.215795   966 net.cpp:693] Ignoring source layer visualize
I0428 20:35:26.215796   966 net.cpp:693] Ignoring source layer fake
I0428 20:40:26.458719   966 solver.cpp:404]     Test net output #0: loss = 0.375755 (* 1 = 0.375755 loss)
I0428 20:40:26.775032   966 solver.cpp:228] Iteration 3000, loss = 0.23796
I0428 20:40:26.775063   966 solver.cpp:244]     Train net output #0: loss = 0.23796 (* 1 = 0.23796 loss)
I0428 20:40:26.775070   966 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0428 20:41:17.204708   966 solver.cpp:228] Iteration 3100, loss = 0.0726869
I0428 20:41:17.204876   966 solver.cpp:244]     Train net output #0: loss = 0.0726869 (* 1 = 0.0726869 loss)
I0428 20:41:17.204883   966 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I0428 20:42:23.073470   966 solver.cpp:228] Iteration 3200, loss = 0.178987
I0428 20:42:23.073658   966 solver.cpp:244]     Train net output #0: loss = 0.178987 (* 1 = 0.178987 loss)
I0428 20:42:23.073668   966 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I0428 20:43:13.493662   966 solver.cpp:228] Iteration 3300, loss = 0.176475
I0428 20:43:13.493826   966 solver.cpp:244]     Train net output #0: loss = 0.176475 (* 1 = 0.176475 loss)
I0428 20:43:13.493834   966 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I0428 20:44:18.058121   966 solver.cpp:228] Iteration 3400, loss = 0.122702
I0428 20:44:18.058293   966 solver.cpp:244]     Train net output #0: loss = 0.122702 (* 1 = 0.122702 loss)
I0428 20:44:18.058300   966 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I0428 20:45:08.424134   966 solver.cpp:228] Iteration 3500, loss = 0.17567
I0428 20:45:08.424315   966 solver.cpp:244]     Train net output #0: loss = 0.17567 (* 1 = 0.17567 loss)
I0428 20:45:08.424322   966 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I0428 20:45:58.845701   966 solver.cpp:228] Iteration 3600, loss = 0.197595
I0428 20:45:58.845881   966 solver.cpp:244]     Train net output #0: loss = 0.197595 (* 1 = 0.197595 loss)
I0428 20:45:58.845891   966 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I0428 20:47:04.565502   966 solver.cpp:228] Iteration 3700, loss = 0.0446871
I0428 20:47:04.565682   966 solver.cpp:244]     Train net output #0: loss = 0.0446871 (* 1 = 0.0446871 loss)
I0428 20:47:04.565696   966 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I0428 20:47:54.973528   966 solver.cpp:228] Iteration 3800, loss = 0.0407769
I0428 20:47:54.973707   966 solver.cpp:244]     Train net output #0: loss = 0.0407769 (* 1 = 0.0407769 loss)
I0428 20:47:54.973714   966 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I0428 20:48:45.398821   966 solver.cpp:228] Iteration 3900, loss = 0.0271535
I0428 20:48:45.398977   966 solver.cpp:244]     Train net output #0: loss = 0.0271535 (* 1 = 0.0271535 loss)
I0428 20:48:45.398983   966 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I0428 20:49:52.510584   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_4000.caffemodel
I0428 20:50:11.345779   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_4000.solverstate
I0428 20:50:11.586100   966 solver.cpp:337] Iteration 4000, Testing net (#0)
I0428 20:50:11.586249   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 20:50:11.586257   966 net.cpp:693] Ignoring source layer visualize
I0428 20:50:11.586261   966 net.cpp:693] Ignoring source layer fake
I0428 20:55:11.332185   966 solver.cpp:404]     Test net output #0: loss = 0.313525 (* 1 = 0.313525 loss)
I0428 20:55:11.648808   966 solver.cpp:228] Iteration 4000, loss = 0.38154
I0428 20:55:11.648849   966 solver.cpp:244]     Train net output #0: loss = 0.38154 (* 1 = 0.38154 loss)
I0428 20:55:11.648857   966 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I0428 20:56:02.088654   966 solver.cpp:228] Iteration 4100, loss = 0.260285
I0428 20:56:02.088816   966 solver.cpp:244]     Train net output #0: loss = 0.260285 (* 1 = 0.260285 loss)
I0428 20:56:02.088824   966 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I0428 20:56:52.539099   966 solver.cpp:228] Iteration 4200, loss = 0.237306
I0428 20:56:52.539258   966 solver.cpp:244]     Train net output #0: loss = 0.237306 (* 1 = 0.237306 loss)
I0428 20:56:52.539263   966 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I0428 20:58:01.134546   966 solver.cpp:228] Iteration 4300, loss = 0.306567
I0428 20:58:01.134737   966 solver.cpp:244]     Train net output #0: loss = 0.306567 (* 1 = 0.306567 loss)
I0428 20:58:01.134745   966 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I0428 20:58:51.506991   966 solver.cpp:228] Iteration 4400, loss = 0.222934
I0428 20:58:51.507170   966 solver.cpp:244]     Train net output #0: loss = 0.222934 (* 1 = 0.222934 loss)
I0428 20:58:51.507179   966 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
I0428 20:59:41.946069   966 solver.cpp:228] Iteration 4500, loss = 0.0879563
I0428 20:59:41.946229   966 solver.cpp:244]     Train net output #0: loss = 0.0879563 (* 1 = 0.0879563 loss)
I0428 20:59:41.946236   966 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I0428 21:00:32.393489   966 solver.cpp:228] Iteration 4600, loss = 0.122121
I0428 21:00:32.393648   966 solver.cpp:244]     Train net output #0: loss = 0.122121 (* 1 = 0.122121 loss)
I0428 21:00:32.393654   966 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
I0428 21:01:40.924495   966 solver.cpp:228] Iteration 4700, loss = 0.247341
I0428 21:01:40.924638   966 solver.cpp:244]     Train net output #0: loss = 0.247341 (* 1 = 0.247341 loss)
I0428 21:01:40.924645   966 sgd_solver.cpp:106] Iteration 4700, lr = 0.01
I0428 21:02:31.352609   966 solver.cpp:228] Iteration 4800, loss = 0.158266
I0428 21:02:31.352784   966 solver.cpp:244]     Train net output #0: loss = 0.158266 (* 1 = 0.158266 loss)
I0428 21:02:31.352793   966 sgd_solver.cpp:106] Iteration 4800, lr = 0.01
I0428 21:03:21.794970   966 solver.cpp:228] Iteration 4900, loss = 0.445168
I0428 21:03:21.795135   966 solver.cpp:244]     Train net output #0: loss = 0.445168 (* 1 = 0.445168 loss)
I0428 21:03:21.795142   966 sgd_solver.cpp:106] Iteration 4900, lr = 0.01
I0428 21:04:29.187280   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_5000.caffemodel
I0428 21:04:37.036734   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_5000.solverstate
I0428 21:04:37.229820   966 solver.cpp:337] Iteration 5000, Testing net (#0)
I0428 21:04:37.229960   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 21:04:37.229965   966 net.cpp:693] Ignoring source layer visualize
I0428 21:04:37.229969   966 net.cpp:693] Ignoring source layer fake
I0428 21:09:36.635515   966 solver.cpp:404]     Test net output #0: loss = 0.280854 (* 1 = 0.280854 loss)
I0428 21:09:36.952895   966 solver.cpp:228] Iteration 5000, loss = 0.25918
I0428 21:09:36.952937   966 solver.cpp:244]     Train net output #0: loss = 0.25918 (* 1 = 0.25918 loss)
I0428 21:09:36.952944   966 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I0428 21:10:27.399462   966 solver.cpp:228] Iteration 5100, loss = 0.149735
I0428 21:10:27.399652   966 solver.cpp:244]     Train net output #0: loss = 0.149735 (* 1 = 0.149735 loss)
I0428 21:10:27.399658   966 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I0428 21:11:17.860096   966 solver.cpp:228] Iteration 5200, loss = 0.161746
I0428 21:11:17.860255   966 solver.cpp:244]     Train net output #0: loss = 0.161746 (* 1 = 0.161746 loss)
I0428 21:11:17.860261   966 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I0428 21:12:25.711518   966 solver.cpp:228] Iteration 5300, loss = 0.164397
I0428 21:12:25.711671   966 solver.cpp:244]     Train net output #0: loss = 0.164397 (* 1 = 0.164397 loss)
I0428 21:12:25.711679   966 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I0428 21:13:16.024343   966 solver.cpp:228] Iteration 5400, loss = 0.113671
I0428 21:13:16.024493   966 solver.cpp:244]     Train net output #0: loss = 0.113671 (* 1 = 0.113671 loss)
I0428 21:13:16.024500   966 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
I0428 21:14:06.461377   966 solver.cpp:228] Iteration 5500, loss = 0.174701
I0428 21:14:06.461555   966 solver.cpp:244]     Train net output #0: loss = 0.174701 (* 1 = 0.174701 loss)
I0428 21:14:06.461562   966 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I0428 21:14:56.914314   966 solver.cpp:228] Iteration 5600, loss = 0.144349
I0428 21:14:56.914505   966 solver.cpp:244]     Train net output #0: loss = 0.144349 (* 1 = 0.144349 loss)
I0428 21:14:56.914512   966 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I0428 21:16:04.784829   966 solver.cpp:228] Iteration 5700, loss = 0.0701007
I0428 21:16:04.784960   966 solver.cpp:244]     Train net output #0: loss = 0.0701007 (* 1 = 0.0701007 loss)
I0428 21:16:04.784965   966 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I0428 21:16:55.217286   966 solver.cpp:228] Iteration 5800, loss = 0.107316
I0428 21:16:55.217501   966 solver.cpp:244]     Train net output #0: loss = 0.107316 (* 1 = 0.107316 loss)
I0428 21:16:55.217510   966 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I0428 21:17:45.671528   966 solver.cpp:228] Iteration 5900, loss = 0.131254
I0428 21:17:45.671687   966 solver.cpp:244]     Train net output #0: loss = 0.131254 (* 1 = 0.131254 loss)
I0428 21:17:45.671695   966 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I0428 21:18:51.587785   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_6000.caffemodel
I0428 21:19:03.293609   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_6000.solverstate
I0428 21:19:03.496546   966 solver.cpp:337] Iteration 6000, Testing net (#0)
I0428 21:19:03.496672   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 21:19:03.496680   966 net.cpp:693] Ignoring source layer visualize
I0428 21:19:03.496681   966 net.cpp:693] Ignoring source layer fake
I0428 21:24:03.747931   966 solver.cpp:404]     Test net output #0: loss = 0.257706 (* 1 = 0.257706 loss)
I0428 21:24:04.063588   966 solver.cpp:228] Iteration 6000, loss = 0.13356
I0428 21:24:04.063608   966 solver.cpp:244]     Train net output #0: loss = 0.13356 (* 1 = 0.13356 loss)
I0428 21:24:04.063630   966 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I0428 21:24:54.525039   966 solver.cpp:228] Iteration 6100, loss = 0.0953968
I0428 21:24:54.525200   966 solver.cpp:244]     Train net output #0: loss = 0.0953968 (* 1 = 0.0953968 loss)
I0428 21:24:54.525208   966 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I0428 21:25:44.989902   966 solver.cpp:228] Iteration 6200, loss = 0.164907
I0428 21:25:44.990061   966 solver.cpp:244]     Train net output #0: loss = 0.164907 (* 1 = 0.164907 loss)
I0428 21:25:44.990068   966 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I0428 21:26:48.136596   966 solver.cpp:228] Iteration 6300, loss = 0.126204
I0428 21:26:48.136759   966 solver.cpp:244]     Train net output #0: loss = 0.126204 (* 1 = 0.126204 loss)
I0428 21:26:48.136765   966 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I0428 21:27:38.570861   966 solver.cpp:228] Iteration 6400, loss = 0.18836
I0428 21:27:38.571069   966 solver.cpp:244]     Train net output #0: loss = 0.18836 (* 1 = 0.18836 loss)
I0428 21:27:38.571081   966 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
I0428 21:29:00.881680   966 solver.cpp:228] Iteration 6500, loss = 0.215158
I0428 21:29:00.881906   966 solver.cpp:244]     Train net output #0: loss = 0.215158 (* 1 = 0.215158 loss)
I0428 21:29:00.881924   966 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I0428 21:29:50.060772   966 solver.cpp:228] Iteration 6600, loss = 0.208818
I0428 21:29:50.060979   966 solver.cpp:244]     Train net output #0: loss = 0.208818 (* 1 = 0.208818 loss)
I0428 21:29:50.060986   966 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I0428 21:30:39.498746   966 solver.cpp:228] Iteration 6700, loss = 0.142023
I0428 21:30:39.498908   966 solver.cpp:244]     Train net output #0: loss = 0.142023 (* 1 = 0.142023 loss)
I0428 21:30:39.498914   966 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I0428 21:31:43.213958   966 solver.cpp:228] Iteration 6800, loss = 0.119805
I0428 21:31:43.214151   966 solver.cpp:244]     Train net output #0: loss = 0.119805 (* 1 = 0.119805 loss)
I0428 21:31:43.214165   966 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I0428 21:32:32.620873   966 solver.cpp:228] Iteration 6900, loss = 0.137921
I0428 21:32:32.621032   966 solver.cpp:244]     Train net output #0: loss = 0.137921 (* 1 = 0.137921 loss)
I0428 21:32:32.621040   966 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I0428 21:33:21.749338   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_7000.caffemodel
I0428 21:33:24.883821   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_7000.solverstate
I0428 21:33:25.185583   966 solver.cpp:337] Iteration 7000, Testing net (#0)
I0428 21:33:25.185808   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 21:33:25.185833   966 net.cpp:693] Ignoring source layer visualize
I0428 21:33:25.185838   966 net.cpp:693] Ignoring source layer fake
I0428 21:38:16.695129   966 solver.cpp:404]     Test net output #0: loss = 0.253411 (* 1 = 0.253411 loss)
I0428 21:38:17.007899   966 solver.cpp:228] Iteration 7000, loss = 0.109085
I0428 21:38:17.007923   966 solver.cpp:244]     Train net output #0: loss = 0.109085 (* 1 = 0.109085 loss)
I0428 21:38:17.007944   966 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I0428 21:39:22.809628   966 solver.cpp:228] Iteration 7100, loss = 0.260329
I0428 21:39:22.809795   966 solver.cpp:244]     Train net output #0: loss = 0.260329 (* 1 = 0.260329 loss)
I0428 21:39:22.809803   966 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I0428 21:40:12.210453   966 solver.cpp:228] Iteration 7200, loss = 0.145922
I0428 21:40:12.210608   966 solver.cpp:244]     Train net output #0: loss = 0.145922 (* 1 = 0.145922 loss)
I0428 21:40:12.210614   966 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I0428 21:41:01.664589   966 solver.cpp:228] Iteration 7300, loss = 0.0904173
I0428 21:41:01.664744   966 solver.cpp:244]     Train net output #0: loss = 0.0904173 (* 1 = 0.0904173 loss)
I0428 21:41:01.664752   966 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I0428 21:42:07.861085   966 solver.cpp:228] Iteration 7400, loss = 0.0807676
I0428 21:42:07.861253   966 solver.cpp:244]     Train net output #0: loss = 0.0807676 (* 1 = 0.0807676 loss)
I0428 21:42:07.861259   966 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
I0428 21:42:57.242749   966 solver.cpp:228] Iteration 7500, loss = 0.226569
I0428 21:42:57.242919   966 solver.cpp:244]     Train net output #0: loss = 0.226569 (* 1 = 0.226569 loss)
I0428 21:42:57.242926   966 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I0428 21:43:46.704246   966 solver.cpp:228] Iteration 7600, loss = 0.192878
I0428 21:43:46.704409   966 solver.cpp:244]     Train net output #0: loss = 0.192878 (* 1 = 0.192878 loss)
I0428 21:43:46.704416   966 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I0428 21:44:36.197187   966 solver.cpp:228] Iteration 7700, loss = 0.190552
I0428 21:44:36.197350   966 solver.cpp:244]     Train net output #0: loss = 0.190552 (* 1 = 0.190552 loss)
I0428 21:44:36.197356   966 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I0428 21:45:43.530975   966 solver.cpp:228] Iteration 7800, loss = 0.203541
I0428 21:45:43.531136   966 solver.cpp:244]     Train net output #0: loss = 0.203541 (* 1 = 0.203541 loss)
I0428 21:45:43.531142   966 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I0428 21:46:32.965564   966 solver.cpp:228] Iteration 7900, loss = 0.0886909
I0428 21:46:32.965721   966 solver.cpp:244]     Train net output #0: loss = 0.0886909 (* 1 = 0.0886909 loss)
I0428 21:46:32.965728   966 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I0428 21:47:22.156811   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_8000.caffemodel
I0428 21:47:33.645321   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_8000.solverstate
I0428 21:47:33.845901   966 solver.cpp:337] Iteration 8000, Testing net (#0)
I0428 21:47:33.846029   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 21:47:33.846051   966 net.cpp:693] Ignoring source layer visualize
I0428 21:47:33.846053   966 net.cpp:693] Ignoring source layer fake
I0428 21:52:26.791281   966 solver.cpp:404]     Test net output #0: loss = 0.213004 (* 1 = 0.213004 loss)
I0428 21:52:27.101359   966 solver.cpp:228] Iteration 8000, loss = 0.111326
I0428 21:52:27.101400   966 solver.cpp:244]     Train net output #0: loss = 0.111326 (* 1 = 0.111326 loss)
I0428 21:52:27.101407   966 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I0428 21:53:33.704938   966 solver.cpp:228] Iteration 8100, loss = 0.253525
I0428 21:53:33.705106   966 solver.cpp:244]     Train net output #0: loss = 0.253525 (* 1 = 0.253525 loss)
I0428 21:53:33.705112   966 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I0428 21:54:23.089596   966 solver.cpp:228] Iteration 8200, loss = 0.133145
I0428 21:54:23.089764   966 solver.cpp:244]     Train net output #0: loss = 0.133145 (* 1 = 0.133145 loss)
I0428 21:54:23.089771   966 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I0428 21:55:12.547847   966 solver.cpp:228] Iteration 8300, loss = 0.232077
I0428 21:55:12.548058   966 solver.cpp:244]     Train net output #0: loss = 0.232077 (* 1 = 0.232077 loss)
I0428 21:55:12.548068   966 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I0428 21:56:02.016360   966 solver.cpp:228] Iteration 8400, loss = 0.110377
I0428 21:56:02.016531   966 solver.cpp:244]     Train net output #0: loss = 0.110377 (* 1 = 0.110377 loss)
I0428 21:56:02.016538   966 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I0428 21:57:06.821616   966 solver.cpp:228] Iteration 8500, loss = 0.0587149
I0428 21:57:06.821774   966 solver.cpp:244]     Train net output #0: loss = 0.0587149 (* 1 = 0.0587149 loss)
I0428 21:57:06.821779   966 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I0428 21:57:56.257730   966 solver.cpp:228] Iteration 8600, loss = 0.0746741
I0428 21:57:56.257880   966 solver.cpp:244]     Train net output #0: loss = 0.0746741 (* 1 = 0.0746741 loss)
I0428 21:57:56.257887   966 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I0428 21:58:45.730384   966 solver.cpp:228] Iteration 8700, loss = 0.0953972
I0428 21:58:45.730553   966 solver.cpp:244]     Train net output #0: loss = 0.0953972 (* 1 = 0.0953972 loss)
I0428 21:58:45.730561   966 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I0428 21:59:41.278254   966 solver.cpp:228] Iteration 8800, loss = 0.117864
I0428 21:59:41.278417   966 solver.cpp:244]     Train net output #0: loss = 0.117864 (* 1 = 0.117864 loss)
I0428 21:59:41.278424   966 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I0428 22:00:30.718266   966 solver.cpp:228] Iteration 8900, loss = 0.0920034
I0428 22:00:30.718425   966 solver.cpp:244]     Train net output #0: loss = 0.0920034 (* 1 = 0.0920034 loss)
I0428 22:00:30.718431   966 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I0428 22:01:19.900890   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_9000.caffemodel
I0428 22:01:27.460758   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_9000.solverstate
I0428 22:01:27.650843   966 solver.cpp:337] Iteration 9000, Testing net (#0)
I0428 22:01:27.650965   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 22:01:27.650971   966 net.cpp:693] Ignoring source layer visualize
I0428 22:01:27.650974   966 net.cpp:693] Ignoring source layer fake
I0428 22:06:19.508523   966 solver.cpp:404]     Test net output #0: loss = 0.219986 (* 1 = 0.219986 loss)
I0428 22:06:19.819640   966 solver.cpp:228] Iteration 9000, loss = 0.0657299
I0428 22:06:19.819681   966 solver.cpp:244]     Train net output #0: loss = 0.0657299 (* 1 = 0.0657299 loss)
I0428 22:06:19.819689   966 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I0428 22:07:20.133472   966 solver.cpp:228] Iteration 9100, loss = 0.111471
I0428 22:07:20.133652   966 solver.cpp:244]     Train net output #0: loss = 0.111471 (* 1 = 0.111471 loss)
I0428 22:07:20.133659   966 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I0428 22:08:09.565125   966 solver.cpp:228] Iteration 9200, loss = 0.136212
I0428 22:08:09.565320   966 solver.cpp:244]     Train net output #0: loss = 0.136212 (* 1 = 0.136212 loss)
I0428 22:08:09.565327   966 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I0428 22:08:59.032603   966 solver.cpp:228] Iteration 9300, loss = 0.166152
I0428 22:08:59.032763   966 solver.cpp:244]     Train net output #0: loss = 0.166152 (* 1 = 0.166152 loss)
I0428 22:08:59.032769   966 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I0428 22:09:50.115360   966 solver.cpp:228] Iteration 9400, loss = 0.215781
I0428 22:09:50.115505   966 solver.cpp:244]     Train net output #0: loss = 0.215781 (* 1 = 0.215781 loss)
I0428 22:09:50.115514   966 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I0428 22:10:39.615262   966 solver.cpp:228] Iteration 9500, loss = 0.248989
I0428 22:10:39.615422   966 solver.cpp:244]     Train net output #0: loss = 0.248989 (* 1 = 0.248989 loss)
I0428 22:10:39.615429   966 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I0428 22:11:29.108795   966 solver.cpp:228] Iteration 9600, loss = 0.171784
I0428 22:11:29.108968   966 solver.cpp:244]     Train net output #0: loss = 0.171784 (* 1 = 0.171784 loss)
I0428 22:11:29.108974   966 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I0428 22:12:19.964238   966 solver.cpp:228] Iteration 9700, loss = 0.342075
I0428 22:12:19.964398   966 solver.cpp:244]     Train net output #0: loss = 0.342075 (* 1 = 0.342075 loss)
I0428 22:12:19.964404   966 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I0428 22:13:09.457258   966 solver.cpp:228] Iteration 9800, loss = 0.0939924
I0428 22:13:09.457538   966 solver.cpp:244]     Train net output #0: loss = 0.0939924 (* 1 = 0.0939924 loss)
I0428 22:13:09.457545   966 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I0428 22:14:00.544703   966 solver.cpp:228] Iteration 9900, loss = 0.0880777
I0428 22:14:00.544863   966 solver.cpp:244]     Train net output #0: loss = 0.0880777 (* 1 = 0.0880777 loss)
I0428 22:14:00.544870   966 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I0428 22:14:49.748497   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_10000.caffemodel
I0428 22:15:01.841949   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_10000.solverstate
I0428 22:15:02.040467   966 solver.cpp:337] Iteration 10000, Testing net (#0)
I0428 22:15:02.040608   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 22:15:02.040616   966 net.cpp:693] Ignoring source layer visualize
I0428 22:15:02.040617   966 net.cpp:693] Ignoring source layer fake
I0428 22:19:53.856822   966 solver.cpp:404]     Test net output #0: loss = 0.231704 (* 1 = 0.231704 loss)
I0428 22:19:54.166676   966 solver.cpp:228] Iteration 10000, loss = 0.117538
I0428 22:19:54.166694   966 solver.cpp:244]     Train net output #0: loss = 0.117538 (* 1 = 0.117538 loss)
I0428 22:19:54.166716   966 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I0428 22:20:43.668639   966 solver.cpp:228] Iteration 10100, loss = 0.146632
I0428 22:20:43.668793   966 solver.cpp:244]     Train net output #0: loss = 0.146632 (* 1 = 0.146632 loss)
I0428 22:20:43.668800   966 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I0428 22:21:34.954820   966 solver.cpp:228] Iteration 10200, loss = 0.235328
I0428 22:21:34.954988   966 solver.cpp:244]     Train net output #0: loss = 0.235328 (* 1 = 0.235328 loss)
I0428 22:21:34.954994   966 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I0428 22:22:24.454234   966 solver.cpp:228] Iteration 10300, loss = 0.0539271
I0428 22:22:24.454388   966 solver.cpp:244]     Train net output #0: loss = 0.0539271 (* 1 = 0.0539271 loss)
I0428 22:22:24.454394   966 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I0428 22:23:13.954352   966 solver.cpp:228] Iteration 10400, loss = 0.169667
I0428 22:23:13.954514   966 solver.cpp:244]     Train net output #0: loss = 0.169667 (* 1 = 0.169667 loss)
I0428 22:23:13.954520   966 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I0428 22:24:03.461740   966 solver.cpp:228] Iteration 10500, loss = 0.112469
I0428 22:24:03.461915   966 solver.cpp:244]     Train net output #0: loss = 0.112469 (* 1 = 0.112469 loss)
I0428 22:24:03.461923   966 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I0428 22:24:54.854466   966 solver.cpp:228] Iteration 10600, loss = 0.104008
I0428 22:24:54.854681   966 solver.cpp:244]     Train net output #0: loss = 0.104008 (* 1 = 0.104008 loss)
I0428 22:24:54.854701   966 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I0428 22:25:44.348752   966 solver.cpp:228] Iteration 10700, loss = 0.0905106
I0428 22:25:44.348932   966 solver.cpp:244]     Train net output #0: loss = 0.0905106 (* 1 = 0.0905106 loss)
I0428 22:25:44.348942   966 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I0428 22:26:33.866312   966 solver.cpp:228] Iteration 10800, loss = 0.0968938
I0428 22:26:33.866497   966 solver.cpp:244]     Train net output #0: loss = 0.0968938 (* 1 = 0.0968938 loss)
I0428 22:26:33.866503   966 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I0428 22:27:25.394958   966 solver.cpp:228] Iteration 10900, loss = 0.178508
I0428 22:27:25.395122   966 solver.cpp:244]     Train net output #0: loss = 0.178508 (* 1 = 0.178508 loss)
I0428 22:27:25.395128   966 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I0428 22:28:14.607612   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_11000.caffemodel
I0428 22:28:21.688052   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_11000.solverstate
I0428 22:28:21.884821   966 solver.cpp:337] Iteration 11000, Testing net (#0)
I0428 22:28:21.884948   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 22:28:21.884953   966 net.cpp:693] Ignoring source layer visualize
I0428 22:28:21.884954   966 net.cpp:693] Ignoring source layer fake
I0428 22:33:13.927409   966 solver.cpp:404]     Test net output #0: loss = 0.20088 (* 1 = 0.20088 loss)
I0428 22:33:14.238631   966 solver.cpp:228] Iteration 11000, loss = 0.242552
I0428 22:33:14.238667   966 solver.cpp:244]     Train net output #0: loss = 0.242552 (* 1 = 0.242552 loss)
I0428 22:33:14.238674   966 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I0428 22:34:03.767120   966 solver.cpp:228] Iteration 11100, loss = 0.111198
I0428 22:34:03.767290   966 solver.cpp:244]     Train net output #0: loss = 0.111198 (* 1 = 0.111198 loss)
I0428 22:34:03.767297   966 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I0428 22:34:55.222601   966 solver.cpp:228] Iteration 11200, loss = 0.128901
I0428 22:34:55.222776   966 solver.cpp:244]     Train net output #0: loss = 0.128901 (* 1 = 0.128901 loss)
I0428 22:34:55.222784   966 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I0428 22:35:44.725131   966 solver.cpp:228] Iteration 11300, loss = 0.117615
I0428 22:35:44.725284   966 solver.cpp:244]     Train net output #0: loss = 0.117615 (* 1 = 0.117615 loss)
I0428 22:35:44.725291   966 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I0428 22:36:34.234303   966 solver.cpp:228] Iteration 11400, loss = 0.169163
I0428 22:36:34.235738   966 solver.cpp:244]     Train net output #0: loss = 0.169163 (* 1 = 0.169163 loss)
I0428 22:36:34.235744   966 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I0428 22:37:23.736536   966 solver.cpp:228] Iteration 11500, loss = 0.121086
I0428 22:37:23.736704   966 solver.cpp:244]     Train net output #0: loss = 0.121086 (* 1 = 0.121086 loss)
I0428 22:37:23.736711   966 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
I0428 22:38:15.133378   966 solver.cpp:228] Iteration 11600, loss = 0.0844846
I0428 22:38:15.133548   966 solver.cpp:244]     Train net output #0: loss = 0.0844846 (* 1 = 0.0844846 loss)
I0428 22:38:15.133555   966 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I0428 22:39:04.633695   966 solver.cpp:228] Iteration 11700, loss = 0.123273
I0428 22:39:04.633877   966 solver.cpp:244]     Train net output #0: loss = 0.123273 (* 1 = 0.123273 loss)
I0428 22:39:04.633884   966 sgd_solver.cpp:106] Iteration 11700, lr = 0.001
I0428 22:39:54.133419   966 solver.cpp:228] Iteration 11800, loss = 0.146803
I0428 22:39:54.133586   966 solver.cpp:244]     Train net output #0: loss = 0.146803 (* 1 = 0.146803 loss)
I0428 22:39:54.133592   966 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I0428 22:40:45.563591   966 solver.cpp:228] Iteration 11900, loss = 0.117603
I0428 22:40:45.563771   966 solver.cpp:244]     Train net output #0: loss = 0.117603 (* 1 = 0.117603 loss)
I0428 22:40:45.563777   966 sgd_solver.cpp:106] Iteration 11900, lr = 0.001
I0428 22:41:34.732866   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_12000.caffemodel
I0428 22:41:47.813117   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_12000.solverstate
I0428 22:41:48.127660   966 solver.cpp:337] Iteration 12000, Testing net (#0)
I0428 22:41:48.127785   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 22:41:48.127791   966 net.cpp:693] Ignoring source layer visualize
I0428 22:41:48.127794   966 net.cpp:693] Ignoring source layer fake
I0428 22:46:40.426017   966 solver.cpp:404]     Test net output #0: loss = 0.207792 (* 1 = 0.207792 loss)
I0428 22:46:40.739840   966 solver.cpp:228] Iteration 12000, loss = 0.139678
I0428 22:46:40.739876   966 solver.cpp:244]     Train net output #0: loss = 0.139678 (* 1 = 0.139678 loss)
I0428 22:46:40.739881   966 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I0428 22:47:30.230252   966 solver.cpp:228] Iteration 12100, loss = 0.128706
I0428 22:47:30.230417   966 solver.cpp:244]     Train net output #0: loss = 0.128706 (* 1 = 0.128706 loss)
I0428 22:47:30.230423   966 sgd_solver.cpp:106] Iteration 12100, lr = 0.001
I0428 22:48:21.512593   966 solver.cpp:228] Iteration 12200, loss = 0.117338
I0428 22:48:21.512770   966 solver.cpp:244]     Train net output #0: loss = 0.117338 (* 1 = 0.117338 loss)
I0428 22:48:21.512778   966 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I0428 22:49:10.996479   966 solver.cpp:228] Iteration 12300, loss = 0.143223
I0428 22:49:10.996673   966 solver.cpp:244]     Train net output #0: loss = 0.143223 (* 1 = 0.143223 loss)
I0428 22:49:10.996680   966 sgd_solver.cpp:106] Iteration 12300, lr = 0.001
I0428 22:50:00.483963   966 solver.cpp:228] Iteration 12400, loss = 0.121916
I0428 22:50:00.484154   966 solver.cpp:244]     Train net output #0: loss = 0.121916 (* 1 = 0.121916 loss)
I0428 22:50:00.484161   966 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I0428 22:50:51.273190   966 solver.cpp:228] Iteration 12500, loss = 0.108163
I0428 22:50:51.273351   966 solver.cpp:244]     Train net output #0: loss = 0.108163 (* 1 = 0.108163 loss)
I0428 22:50:51.273358   966 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I0428 22:51:40.768219   966 solver.cpp:228] Iteration 12600, loss = 0.166461
I0428 22:51:40.768987   966 solver.cpp:244]     Train net output #0: loss = 0.166461 (* 1 = 0.166461 loss)
I0428 22:51:40.768996   966 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I0428 22:52:30.271265   966 solver.cpp:228] Iteration 12700, loss = 0.118953
I0428 22:52:30.271427   966 solver.cpp:244]     Train net output #0: loss = 0.118953 (* 1 = 0.118953 loss)
I0428 22:52:30.271435   966 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I0428 22:53:21.162998   966 solver.cpp:228] Iteration 12800, loss = 0.0913774
I0428 22:53:21.163158   966 solver.cpp:244]     Train net output #0: loss = 0.0913774 (* 1 = 0.0913774 loss)
I0428 22:53:21.163166   966 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I0428 22:54:10.666988   966 solver.cpp:228] Iteration 12900, loss = 0.223719
I0428 22:54:10.667148   966 solver.cpp:244]     Train net output #0: loss = 0.223719 (* 1 = 0.223719 loss)
I0428 22:54:10.667155   966 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I0428 22:54:59.850970   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_13000.caffemodel
I0428 22:55:06.820868   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_13000.solverstate
I0428 22:55:07.013530   966 solver.cpp:337] Iteration 13000, Testing net (#0)
I0428 22:55:07.013654   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 22:55:07.013660   966 net.cpp:693] Ignoring source layer visualize
I0428 22:55:07.013664   966 net.cpp:693] Ignoring source layer fake
I0428 22:59:59.985038   966 solver.cpp:404]     Test net output #0: loss = 0.222277 (* 1 = 0.222277 loss)
I0428 23:00:00.293725   966 solver.cpp:228] Iteration 13000, loss = 0.138091
I0428 23:00:00.293745   966 solver.cpp:244]     Train net output #0: loss = 0.138091 (* 1 = 0.138091 loss)
I0428 23:00:00.293766   966 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I0428 23:00:51.408270   966 solver.cpp:228] Iteration 13100, loss = 0.153628
I0428 23:00:51.408468   966 solver.cpp:244]     Train net output #0: loss = 0.153628 (* 1 = 0.153628 loss)
I0428 23:00:51.408474   966 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I0428 23:01:40.901682   966 solver.cpp:228] Iteration 13200, loss = 0.14948
I0428 23:01:40.901818   966 solver.cpp:244]     Train net output #0: loss = 0.14948 (* 1 = 0.14948 loss)
I0428 23:01:40.901824   966 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I0428 23:02:32.249181   966 solver.cpp:228] Iteration 13300, loss = 0.147859
I0428 23:02:32.249335   966 solver.cpp:244]     Train net output #0: loss = 0.147859 (* 1 = 0.147859 loss)
I0428 23:02:32.249342   966 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I0428 23:03:21.739424   966 solver.cpp:228] Iteration 13400, loss = 0.218963
I0428 23:03:21.739578   966 solver.cpp:244]     Train net output #0: loss = 0.218963 (* 1 = 0.218963 loss)
I0428 23:03:21.739586   966 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I0428 23:04:11.241408   966 solver.cpp:228] Iteration 13500, loss = 0.132032
I0428 23:04:11.241575   966 solver.cpp:244]     Train net output #0: loss = 0.132032 (* 1 = 0.132032 loss)
I0428 23:04:11.241581   966 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I0428 23:05:00.746690   966 solver.cpp:228] Iteration 13600, loss = 0.199597
I0428 23:05:00.746832   966 solver.cpp:244]     Train net output #0: loss = 0.199597 (* 1 = 0.199597 loss)
I0428 23:05:00.746839   966 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I0428 23:05:52.204922   966 solver.cpp:228] Iteration 13700, loss = 0.153734
I0428 23:05:52.205060   966 solver.cpp:244]     Train net output #0: loss = 0.153734 (* 1 = 0.153734 loss)
I0428 23:05:52.205066   966 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I0428 23:06:41.711191   966 solver.cpp:228] Iteration 13800, loss = 0.159399
I0428 23:06:41.711364   966 solver.cpp:244]     Train net output #0: loss = 0.159399 (* 1 = 0.159399 loss)
I0428 23:06:41.711370   966 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I0428 23:07:31.225162   966 solver.cpp:228] Iteration 13900, loss = 0.161824
I0428 23:07:31.227145   966 solver.cpp:244]     Train net output #0: loss = 0.161824 (* 1 = 0.161824 loss)
I0428 23:07:31.227154   966 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I0428 23:08:22.357106   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_14000.caffemodel
I0428 23:08:37.975100   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_14000.solverstate
I0428 23:08:38.182044   966 solver.cpp:337] Iteration 14000, Testing net (#0)
I0428 23:08:38.182168   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 23:08:38.182191   966 net.cpp:693] Ignoring source layer visualize
I0428 23:08:38.182193   966 net.cpp:693] Ignoring source layer fake
I0428 23:13:30.995184   966 solver.cpp:404]     Test net output #0: loss = 0.200495 (* 1 = 0.200495 loss)
I0428 23:13:31.304831   966 solver.cpp:228] Iteration 14000, loss = 0.110123
I0428 23:13:31.304852   966 solver.cpp:244]     Train net output #0: loss = 0.110123 (* 1 = 0.110123 loss)
I0428 23:13:31.304874   966 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I0428 23:14:20.827833   966 solver.cpp:228] Iteration 14100, loss = 0.0608497
I0428 23:14:20.828001   966 solver.cpp:244]     Train net output #0: loss = 0.0608497 (* 1 = 0.0608497 loss)
I0428 23:14:20.828009   966 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I0428 23:15:10.347549   966 solver.cpp:228] Iteration 14200, loss = 0.132555
I0428 23:15:10.347712   966 solver.cpp:244]     Train net output #0: loss = 0.132555 (* 1 = 0.132555 loss)
I0428 23:15:10.347718   966 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I0428 23:15:59.862252   966 solver.cpp:228] Iteration 14300, loss = 0.191888
I0428 23:15:59.862411   966 solver.cpp:244]     Train net output #0: loss = 0.191888 (* 1 = 0.191888 loss)
I0428 23:15:59.862418   966 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I0428 23:16:51.291148   966 solver.cpp:228] Iteration 14400, loss = 0.0782705
I0428 23:16:51.292639   966 solver.cpp:244]     Train net output #0: loss = 0.0782705 (* 1 = 0.0782705 loss)
I0428 23:16:51.292649   966 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I0428 23:17:40.794683   966 solver.cpp:228] Iteration 14500, loss = 0.0722092
I0428 23:17:40.794852   966 solver.cpp:244]     Train net output #0: loss = 0.0722092 (* 1 = 0.0722092 loss)
I0428 23:17:40.794860   966 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I0428 23:18:30.302989   966 solver.cpp:228] Iteration 14600, loss = 0.170283
I0428 23:18:30.303145   966 solver.cpp:244]     Train net output #0: loss = 0.170283 (* 1 = 0.170283 loss)
I0428 23:18:30.303153   966 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I0428 23:19:21.654599   966 solver.cpp:228] Iteration 14700, loss = 0.0873791
I0428 23:19:21.654825   966 solver.cpp:244]     Train net output #0: loss = 0.0873791 (* 1 = 0.0873791 loss)
I0428 23:19:21.654834   966 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I0428 23:20:11.147611   966 solver.cpp:228] Iteration 14800, loss = 0.090477
I0428 23:20:11.147769   966 solver.cpp:244]     Train net output #0: loss = 0.090477 (* 1 = 0.090477 loss)
I0428 23:20:11.147778   966 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I0428 23:21:00.652798   966 solver.cpp:228] Iteration 14900, loss = 0.0793001
I0428 23:21:00.652956   966 solver.cpp:244]     Train net output #0: loss = 0.0793001 (* 1 = 0.0793001 loss)
I0428 23:21:00.652963   966 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I0428 23:21:51.748116   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_15000.caffemodel
I0428 23:22:00.292351   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_15000.solverstate
I0428 23:22:00.498536   966 solver.cpp:337] Iteration 15000, Testing net (#0)
I0428 23:22:00.498643   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 23:22:00.498649   966 net.cpp:693] Ignoring source layer visualize
I0428 23:22:00.498652   966 net.cpp:693] Ignoring source layer fake
I0428 23:26:53.497011   966 solver.cpp:404]     Test net output #0: loss = 0.234002 (* 1 = 0.234002 loss)
I0428 23:26:53.807174   966 solver.cpp:228] Iteration 15000, loss = 0.103657
I0428 23:26:53.807196   966 solver.cpp:244]     Train net output #0: loss = 0.103657 (* 1 = 0.103657 loss)
I0428 23:26:53.807219   966 sgd_solver.cpp:106] Iteration 15000, lr = 0.0001
I0428 23:27:43.303606   966 solver.cpp:228] Iteration 15100, loss = 0.0174556
I0428 23:27:43.303783   966 solver.cpp:244]     Train net output #0: loss = 0.0174556 (* 1 = 0.0174556 loss)
I0428 23:27:43.303791   966 sgd_solver.cpp:106] Iteration 15100, lr = 0.0001
I0428 23:28:32.798163   966 solver.cpp:228] Iteration 15200, loss = 0.117229
I0428 23:28:32.798501   966 solver.cpp:244]     Train net output #0: loss = 0.117229 (* 1 = 0.117229 loss)
I0428 23:28:32.798509   966 sgd_solver.cpp:106] Iteration 15200, lr = 0.0001
I0428 23:29:22.302083   966 solver.cpp:228] Iteration 15300, loss = 0.0781531
I0428 23:29:22.302268   966 solver.cpp:244]     Train net output #0: loss = 0.0781531 (* 1 = 0.0781531 loss)
I0428 23:29:22.302276   966 sgd_solver.cpp:106] Iteration 15300, lr = 0.0001
I0428 23:30:13.701903   966 solver.cpp:228] Iteration 15400, loss = 0.233068
I0428 23:30:13.702071   966 solver.cpp:244]     Train net output #0: loss = 0.233068 (* 1 = 0.233068 loss)
I0428 23:30:13.702078   966 sgd_solver.cpp:106] Iteration 15400, lr = 0.0001
I0428 23:31:03.186097   966 solver.cpp:228] Iteration 15500, loss = 0.086274
I0428 23:31:03.186269   966 solver.cpp:244]     Train net output #0: loss = 0.086274 (* 1 = 0.086274 loss)
I0428 23:31:03.186278   966 sgd_solver.cpp:106] Iteration 15500, lr = 0.0001
I0428 23:31:52.681048   966 solver.cpp:228] Iteration 15600, loss = 0.105031
I0428 23:31:52.681208   966 solver.cpp:244]     Train net output #0: loss = 0.105031 (* 1 = 0.105031 loss)
I0428 23:31:52.681216   966 sgd_solver.cpp:106] Iteration 15600, lr = 0.0001
I0428 23:32:43.680404   966 solver.cpp:228] Iteration 15700, loss = 0.14332
I0428 23:32:43.680589   966 solver.cpp:244]     Train net output #0: loss = 0.14332 (* 1 = 0.14332 loss)
I0428 23:32:43.680598   966 sgd_solver.cpp:106] Iteration 15700, lr = 0.0001
I0428 23:33:33.186450   966 solver.cpp:228] Iteration 15800, loss = 0.128615
I0428 23:33:33.186619   966 solver.cpp:244]     Train net output #0: loss = 0.128615 (* 1 = 0.128615 loss)
I0428 23:33:33.186626   966 sgd_solver.cpp:106] Iteration 15800, lr = 0.0001
I0428 23:34:24.080837   966 solver.cpp:228] Iteration 15900, loss = 0.175811
I0428 23:34:24.081023   966 solver.cpp:244]     Train net output #0: loss = 0.175811 (* 1 = 0.175811 loss)
I0428 23:34:24.081030   966 sgd_solver.cpp:106] Iteration 15900, lr = 0.0001
I0428 23:35:13.274093   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_16000.caffemodel
I0428 23:35:19.466819   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_16000.solverstate
I0428 23:35:19.670087   966 solver.cpp:337] Iteration 16000, Testing net (#0)
I0428 23:35:19.670210   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 23:35:19.670233   966 net.cpp:693] Ignoring source layer visualize
I0428 23:35:19.670234   966 net.cpp:693] Ignoring source layer fake
I0428 23:40:12.417296   966 solver.cpp:404]     Test net output #0: loss = 0.203339 (* 1 = 0.203339 loss)
I0428 23:40:12.726045   966 solver.cpp:228] Iteration 16000, loss = 0.223071
I0428 23:40:12.726086   966 solver.cpp:244]     Train net output #0: loss = 0.223071 (* 1 = 0.223071 loss)
I0428 23:40:12.726092   966 sgd_solver.cpp:106] Iteration 16000, lr = 0.0001
I0428 23:41:02.259943   966 solver.cpp:228] Iteration 16100, loss = 0.23473
I0428 23:41:02.260107   966 solver.cpp:244]     Train net output #0: loss = 0.23473 (* 1 = 0.23473 loss)
I0428 23:41:02.260113   966 sgd_solver.cpp:106] Iteration 16100, lr = 0.0001
I0428 23:41:53.416560   966 solver.cpp:228] Iteration 16200, loss = 0.0804052
I0428 23:41:53.416720   966 solver.cpp:244]     Train net output #0: loss = 0.0804052 (* 1 = 0.0804052 loss)
I0428 23:41:53.416726   966 sgd_solver.cpp:106] Iteration 16200, lr = 0.0001
I0428 23:42:42.935619   966 solver.cpp:228] Iteration 16300, loss = 0.132641
I0428 23:42:42.935787   966 solver.cpp:244]     Train net output #0: loss = 0.132641 (* 1 = 0.132641 loss)
I0428 23:42:42.935797   966 sgd_solver.cpp:106] Iteration 16300, lr = 0.0001
I0428 23:43:32.445164   966 solver.cpp:228] Iteration 16400, loss = 0.0651688
I0428 23:43:32.445330   966 solver.cpp:244]     Train net output #0: loss = 0.0651688 (* 1 = 0.0651688 loss)
I0428 23:43:32.445338   966 sgd_solver.cpp:106] Iteration 16400, lr = 0.0001
I0428 23:44:24.040343   966 solver.cpp:228] Iteration 16500, loss = 0.224381
I0428 23:44:24.040457   966 solver.cpp:244]     Train net output #0: loss = 0.224381 (* 1 = 0.224381 loss)
I0428 23:44:24.040462   966 sgd_solver.cpp:106] Iteration 16500, lr = 0.0001
I0428 23:45:13.546596   966 solver.cpp:228] Iteration 16600, loss = 0.184887
I0428 23:45:13.546778   966 solver.cpp:244]     Train net output #0: loss = 0.184887 (* 1 = 0.184887 loss)
I0428 23:45:13.546787   966 sgd_solver.cpp:106] Iteration 16600, lr = 0.0001
I0428 23:46:03.079905   966 solver.cpp:228] Iteration 16700, loss = 0.277151
I0428 23:46:03.080063   966 solver.cpp:244]     Train net output #0: loss = 0.277151 (* 1 = 0.277151 loss)
I0428 23:46:03.080073   966 sgd_solver.cpp:106] Iteration 16700, lr = 0.0001
I0428 23:46:54.516242   966 solver.cpp:228] Iteration 16800, loss = 0.0839892
I0428 23:46:54.516413   966 solver.cpp:244]     Train net output #0: loss = 0.0839892 (* 1 = 0.0839892 loss)
I0428 23:46:54.516420   966 sgd_solver.cpp:106] Iteration 16800, lr = 0.0001
I0428 23:47:44.028928   966 solver.cpp:228] Iteration 16900, loss = 0.151256
I0428 23:47:44.029068   966 solver.cpp:244]     Train net output #0: loss = 0.151256 (* 1 = 0.151256 loss)
I0428 23:47:44.029075   966 sgd_solver.cpp:106] Iteration 16900, lr = 0.0001
I0428 23:48:33.245880   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_17000.caffemodel
I0428 23:48:55.290356   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_17000.solverstate
I0428 23:48:55.548522   966 solver.cpp:337] Iteration 17000, Testing net (#0)
I0428 23:48:55.548638   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0428 23:48:55.548646   966 net.cpp:693] Ignoring source layer visualize
I0428 23:48:55.548666   966 net.cpp:693] Ignoring source layer fake
I0428 23:53:48.209697   966 solver.cpp:404]     Test net output #0: loss = 0.194607 (* 1 = 0.194607 loss)
I0428 23:53:48.519623   966 solver.cpp:228] Iteration 17000, loss = 0.144345
I0428 23:53:48.519666   966 solver.cpp:244]     Train net output #0: loss = 0.144345 (* 1 = 0.144345 loss)
I0428 23:53:48.519673   966 sgd_solver.cpp:106] Iteration 17000, lr = 0.0001
I0428 23:54:39.975183   966 solver.cpp:228] Iteration 17100, loss = 0.24382
I0428 23:54:39.975358   966 solver.cpp:244]     Train net output #0: loss = 0.24382 (* 1 = 0.24382 loss)
I0428 23:54:39.975364   966 sgd_solver.cpp:106] Iteration 17100, lr = 0.0001
I0428 23:55:29.492532   966 solver.cpp:228] Iteration 17200, loss = 0.194372
I0428 23:55:29.492735   966 solver.cpp:244]     Train net output #0: loss = 0.194372 (* 1 = 0.194372 loss)
I0428 23:55:29.492743   966 sgd_solver.cpp:106] Iteration 17200, lr = 0.0001
I0428 23:56:19.000001   966 solver.cpp:228] Iteration 17300, loss = 0.20356
I0428 23:56:19.000169   966 solver.cpp:244]     Train net output #0: loss = 0.20356 (* 1 = 0.20356 loss)
I0428 23:56:19.000175   966 sgd_solver.cpp:106] Iteration 17300, lr = 0.0001
I0428 23:57:08.524752   966 solver.cpp:228] Iteration 17400, loss = 0.098964
I0428 23:57:08.524912   966 solver.cpp:244]     Train net output #0: loss = 0.098964 (* 1 = 0.098964 loss)
I0428 23:57:08.524919   966 sgd_solver.cpp:106] Iteration 17400, lr = 0.0001
I0428 23:57:59.933851   966 solver.cpp:228] Iteration 17500, loss = 0.0958126
I0428 23:57:59.934016   966 solver.cpp:244]     Train net output #0: loss = 0.0958126 (* 1 = 0.0958126 loss)
I0428 23:57:59.934023   966 sgd_solver.cpp:106] Iteration 17500, lr = 0.0001
I0428 23:58:49.440949   966 solver.cpp:228] Iteration 17600, loss = 0.178918
I0428 23:58:49.441121   966 solver.cpp:244]     Train net output #0: loss = 0.178918 (* 1 = 0.178918 loss)
I0428 23:58:49.441128   966 sgd_solver.cpp:106] Iteration 17600, lr = 0.0001
I0428 23:59:38.957198   966 solver.cpp:228] Iteration 17700, loss = 0.111551
I0428 23:59:38.957367   966 solver.cpp:244]     Train net output #0: loss = 0.111551 (* 1 = 0.111551 loss)
I0428 23:59:38.957376   966 sgd_solver.cpp:106] Iteration 17700, lr = 0.0001
I0429 00:00:30.519206   966 solver.cpp:228] Iteration 17800, loss = 0.0731443
I0429 00:00:30.520087   966 solver.cpp:244]     Train net output #0: loss = 0.0731443 (* 1 = 0.0731443 loss)
I0429 00:00:30.520097   966 sgd_solver.cpp:106] Iteration 17800, lr = 0.0001
I0429 00:01:20.013178   966 solver.cpp:228] Iteration 17900, loss = 0.113773
I0429 00:01:20.013340   966 solver.cpp:244]     Train net output #0: loss = 0.113773 (* 1 = 0.113773 loss)
I0429 00:01:20.013347   966 sgd_solver.cpp:106] Iteration 17900, lr = 0.0001
I0429 00:02:09.211591   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_18000.caffemodel
I0429 00:02:20.393170   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_18000.solverstate
I0429 00:02:20.596603   966 solver.cpp:337] Iteration 18000, Testing net (#0)
I0429 00:02:20.596751   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 00:02:20.596758   966 net.cpp:693] Ignoring source layer visualize
I0429 00:02:20.596761   966 net.cpp:693] Ignoring source layer fake
I0429 00:07:13.300981   966 solver.cpp:404]     Test net output #0: loss = 0.179724 (* 1 = 0.179724 loss)
I0429 00:07:13.609946   966 solver.cpp:228] Iteration 18000, loss = 0.170716
I0429 00:07:13.609973   966 solver.cpp:244]     Train net output #0: loss = 0.170716 (* 1 = 0.170716 loss)
I0429 00:07:13.609979   966 sgd_solver.cpp:106] Iteration 18000, lr = 0.0001
I0429 00:08:03.112651   966 solver.cpp:228] Iteration 18100, loss = 0.120309
I0429 00:08:03.112813   966 solver.cpp:244]     Train net output #0: loss = 0.120309 (* 1 = 0.120309 loss)
I0429 00:08:03.112820   966 sgd_solver.cpp:106] Iteration 18100, lr = 0.0001
I0429 00:08:54.550611   966 solver.cpp:228] Iteration 18200, loss = 0.133634
I0429 00:08:54.550772   966 solver.cpp:244]     Train net output #0: loss = 0.133634 (* 1 = 0.133634 loss)
I0429 00:08:54.550779   966 sgd_solver.cpp:106] Iteration 18200, lr = 0.0001
I0429 00:09:44.044965   966 solver.cpp:228] Iteration 18300, loss = 0.148951
I0429 00:09:44.045125   966 solver.cpp:244]     Train net output #0: loss = 0.148951 (* 1 = 0.148951 loss)
I0429 00:09:44.045131   966 sgd_solver.cpp:106] Iteration 18300, lr = 0.0001
I0429 00:10:33.551064   966 solver.cpp:228] Iteration 18400, loss = 0.115174
I0429 00:10:33.551225   966 solver.cpp:244]     Train net output #0: loss = 0.115174 (* 1 = 0.115174 loss)
I0429 00:10:33.551232   966 sgd_solver.cpp:106] Iteration 18400, lr = 0.0001
I0429 00:11:24.822010   966 solver.cpp:228] Iteration 18500, loss = 0.124157
I0429 00:11:24.822175   966 solver.cpp:244]     Train net output #0: loss = 0.124157 (* 1 = 0.124157 loss)
I0429 00:11:24.822182   966 sgd_solver.cpp:106] Iteration 18500, lr = 0.0001
I0429 00:12:14.329031   966 solver.cpp:228] Iteration 18600, loss = 0.123996
I0429 00:12:14.329224   966 solver.cpp:244]     Train net output #0: loss = 0.123996 (* 1 = 0.123996 loss)
I0429 00:12:14.329231   966 sgd_solver.cpp:106] Iteration 18600, lr = 0.0001
I0429 00:13:03.830796   966 solver.cpp:228] Iteration 18700, loss = 0.108244
I0429 00:13:03.830963   966 solver.cpp:244]     Train net output #0: loss = 0.108244 (* 1 = 0.108244 loss)
I0429 00:13:03.830970   966 sgd_solver.cpp:106] Iteration 18700, lr = 0.0001
I0429 00:13:54.658871   966 solver.cpp:228] Iteration 18800, loss = 0.126513
I0429 00:13:54.659036   966 solver.cpp:244]     Train net output #0: loss = 0.126513 (* 1 = 0.126513 loss)
I0429 00:13:54.659044   966 sgd_solver.cpp:106] Iteration 18800, lr = 0.0001
I0429 00:14:44.173140   966 solver.cpp:228] Iteration 18900, loss = 0.0895972
I0429 00:14:44.173352   966 solver.cpp:244]     Train net output #0: loss = 0.0895972 (* 1 = 0.0895972 loss)
I0429 00:14:44.173360   966 sgd_solver.cpp:106] Iteration 18900, lr = 0.0001
I0429 00:15:34.800343   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_19000.caffemodel
I0429 00:15:51.842272   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_19000.solverstate
I0429 00:15:52.050810   966 solver.cpp:337] Iteration 19000, Testing net (#0)
I0429 00:15:52.050940   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 00:15:52.050946   966 net.cpp:693] Ignoring source layer visualize
I0429 00:15:52.050950   966 net.cpp:693] Ignoring source layer fake
I0429 00:20:45.304695   966 solver.cpp:404]     Test net output #0: loss = 0.186921 (* 1 = 0.186921 loss)
I0429 00:20:45.616235   966 solver.cpp:228] Iteration 19000, loss = 0.251792
I0429 00:20:45.616258   966 solver.cpp:244]     Train net output #0: loss = 0.251792 (* 1 = 0.251792 loss)
I0429 00:20:45.616279   966 sgd_solver.cpp:106] Iteration 19000, lr = 0.0001
I0429 00:21:35.143590   966 solver.cpp:228] Iteration 19100, loss = 0.221189
I0429 00:21:35.143764   966 solver.cpp:244]     Train net output #0: loss = 0.221189 (* 1 = 0.221189 loss)
I0429 00:21:35.143772   966 sgd_solver.cpp:106] Iteration 19100, lr = 0.0001
I0429 00:22:24.658246   966 solver.cpp:228] Iteration 19200, loss = 0.305291
I0429 00:22:24.658413   966 solver.cpp:244]     Train net output #0: loss = 0.305291 (* 1 = 0.305291 loss)
I0429 00:22:24.658421   966 sgd_solver.cpp:106] Iteration 19200, lr = 0.0001
I0429 00:23:15.795300   966 solver.cpp:228] Iteration 19300, loss = 0.103122
I0429 00:23:15.796598   966 solver.cpp:244]     Train net output #0: loss = 0.103122 (* 1 = 0.103122 loss)
I0429 00:23:15.796607   966 sgd_solver.cpp:106] Iteration 19300, lr = 0.0001
I0429 00:24:05.288342   966 solver.cpp:228] Iteration 19400, loss = 0.0826139
I0429 00:24:05.288508   966 solver.cpp:244]     Train net output #0: loss = 0.0826139 (* 1 = 0.0826139 loss)
I0429 00:24:05.288516   966 sgd_solver.cpp:106] Iteration 19400, lr = 0.0001
I0429 00:24:54.809643   966 solver.cpp:228] Iteration 19500, loss = 0.15728
I0429 00:24:54.809820   966 solver.cpp:244]     Train net output #0: loss = 0.15728 (* 1 = 0.15728 loss)
I0429 00:24:54.809828   966 sgd_solver.cpp:106] Iteration 19500, lr = 0.0001
I0429 00:25:46.211755   966 solver.cpp:228] Iteration 19600, loss = 0.111825
I0429 00:25:46.211917   966 solver.cpp:244]     Train net output #0: loss = 0.111825 (* 1 = 0.111825 loss)
I0429 00:25:46.211926   966 sgd_solver.cpp:106] Iteration 19600, lr = 0.0001
I0429 00:26:35.748000   966 solver.cpp:228] Iteration 19700, loss = 0.214216
I0429 00:26:35.748178   966 solver.cpp:244]     Train net output #0: loss = 0.214216 (* 1 = 0.214216 loss)
I0429 00:26:35.748186   966 sgd_solver.cpp:106] Iteration 19700, lr = 0.0001
I0429 00:27:25.267295   966 solver.cpp:228] Iteration 19800, loss = 0.147113
I0429 00:27:25.267443   966 solver.cpp:244]     Train net output #0: loss = 0.147113 (* 1 = 0.147113 loss)
I0429 00:27:25.267451   966 sgd_solver.cpp:106] Iteration 19800, lr = 0.0001
I0429 00:28:16.741331   966 solver.cpp:228] Iteration 19900, loss = 0.124439
I0429 00:28:16.741503   966 solver.cpp:244]     Train net output #0: loss = 0.124439 (* 1 = 0.124439 loss)
I0429 00:28:16.741509   966 sgd_solver.cpp:106] Iteration 19900, lr = 0.0001
I0429 00:29:05.948210   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_20000.caffemodel
I0429 00:29:20.133031   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_20000.solverstate
I0429 00:29:20.324597   966 solver.cpp:337] Iteration 20000, Testing net (#0)
I0429 00:29:20.324720   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 00:29:20.324726   966 net.cpp:693] Ignoring source layer visualize
I0429 00:29:20.324728   966 net.cpp:693] Ignoring source layer fake
I0429 00:34:12.983152   966 solver.cpp:404]     Test net output #0: loss = 0.197196 (* 1 = 0.197196 loss)
I0429 00:34:13.292897   966 solver.cpp:228] Iteration 20000, loss = 0.169065
I0429 00:34:13.292915   966 solver.cpp:244]     Train net output #0: loss = 0.169065 (* 1 = 0.169065 loss)
I0429 00:34:13.292937   966 sgd_solver.cpp:106] Iteration 20000, lr = 0.0001
I0429 00:35:02.823196   966 solver.cpp:228] Iteration 20100, loss = 0.141571
I0429 00:35:02.823395   966 solver.cpp:244]     Train net output #0: loss = 0.141571 (* 1 = 0.141571 loss)
I0429 00:35:02.823401   966 sgd_solver.cpp:106] Iteration 20100, lr = 0.0001
I0429 00:35:52.343992   966 solver.cpp:228] Iteration 20200, loss = 0.100075
I0429 00:35:52.344167   966 solver.cpp:244]     Train net output #0: loss = 0.100075 (* 1 = 0.100075 loss)
I0429 00:35:52.344173   966 sgd_solver.cpp:106] Iteration 20200, lr = 0.0001
I0429 00:36:43.836446   966 solver.cpp:228] Iteration 20300, loss = 0.103489
I0429 00:36:43.836632   966 solver.cpp:244]     Train net output #0: loss = 0.103489 (* 1 = 0.103489 loss)
I0429 00:36:43.836640   966 sgd_solver.cpp:106] Iteration 20300, lr = 0.0001
I0429 00:37:33.332391   966 solver.cpp:228] Iteration 20400, loss = 0.21314
I0429 00:37:33.332533   966 solver.cpp:244]     Train net output #0: loss = 0.21314 (* 1 = 0.21314 loss)
I0429 00:37:33.332540   966 sgd_solver.cpp:106] Iteration 20400, lr = 0.0001
I0429 00:38:22.847216   966 solver.cpp:228] Iteration 20500, loss = 0.154085
I0429 00:38:22.847357   966 solver.cpp:244]     Train net output #0: loss = 0.154085 (* 1 = 0.154085 loss)
I0429 00:38:22.847364   966 sgd_solver.cpp:106] Iteration 20500, lr = 0.0001
I0429 00:39:14.299033   966 solver.cpp:228] Iteration 20600, loss = 0.194549
I0429 00:39:14.299211   966 solver.cpp:244]     Train net output #0: loss = 0.194549 (* 1 = 0.194549 loss)
I0429 00:39:14.299217   966 sgd_solver.cpp:106] Iteration 20600, lr = 0.0001
I0429 00:40:03.800104   966 solver.cpp:228] Iteration 20700, loss = 0.132955
I0429 00:40:03.800808   966 solver.cpp:244]     Train net output #0: loss = 0.132955 (* 1 = 0.132955 loss)
I0429 00:40:03.800817   966 sgd_solver.cpp:106] Iteration 20700, lr = 0.0001
I0429 00:40:53.321434   966 solver.cpp:228] Iteration 20800, loss = 0.0806054
I0429 00:40:53.321616   966 solver.cpp:244]     Train net output #0: loss = 0.0806054 (* 1 = 0.0806054 loss)
I0429 00:40:53.321624   966 sgd_solver.cpp:106] Iteration 20800, lr = 0.0001
I0429 00:41:42.832389   966 solver.cpp:228] Iteration 20900, loss = 0.235847
I0429 00:41:42.832530   966 solver.cpp:244]     Train net output #0: loss = 0.235847 (* 1 = 0.235847 loss)
I0429 00:41:42.832536   966 sgd_solver.cpp:106] Iteration 20900, lr = 0.0001
I0429 00:42:33.918947   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_21000.caffemodel
I0429 00:43:13.733158   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_21000.solverstate
I0429 00:43:13.926949   966 solver.cpp:337] Iteration 21000, Testing net (#0)
I0429 00:43:13.927091   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 00:43:13.927098   966 net.cpp:693] Ignoring source layer visualize
I0429 00:43:13.927101   966 net.cpp:693] Ignoring source layer fake
I0429 00:48:06.305217   966 solver.cpp:404]     Test net output #0: loss = 0.182791 (* 1 = 0.182791 loss)
I0429 00:48:06.615104   966 solver.cpp:228] Iteration 21000, loss = 0.0979851
I0429 00:48:06.615147   966 solver.cpp:244]     Train net output #0: loss = 0.0979851 (* 1 = 0.0979851 loss)
I0429 00:48:06.615155   966 sgd_solver.cpp:106] Iteration 21000, lr = 0.0001
I0429 00:48:56.117566   966 solver.cpp:228] Iteration 21100, loss = 0.132695
I0429 00:48:56.117733   966 solver.cpp:244]     Train net output #0: loss = 0.132695 (* 1 = 0.132695 loss)
I0429 00:48:56.117740   966 sgd_solver.cpp:106] Iteration 21100, lr = 0.0001
I0429 00:49:45.619047   966 solver.cpp:228] Iteration 21200, loss = 0.113302
I0429 00:49:45.619235   966 solver.cpp:244]     Train net output #0: loss = 0.113302 (* 1 = 0.113302 loss)
I0429 00:49:45.619241   966 sgd_solver.cpp:106] Iteration 21200, lr = 0.0001
I0429 00:50:37.039760   966 solver.cpp:228] Iteration 21300, loss = 0.105035
I0429 00:50:37.039933   966 solver.cpp:244]     Train net output #0: loss = 0.105035 (* 1 = 0.105035 loss)
I0429 00:50:37.039940   966 sgd_solver.cpp:106] Iteration 21300, lr = 0.0001
I0429 00:51:26.530697   966 solver.cpp:228] Iteration 21400, loss = 0.148579
I0429 00:51:26.530882   966 solver.cpp:244]     Train net output #0: loss = 0.148579 (* 1 = 0.148579 loss)
I0429 00:51:26.530890   966 sgd_solver.cpp:106] Iteration 21400, lr = 0.0001
I0429 00:52:16.031289   966 solver.cpp:228] Iteration 21500, loss = 0.120866
I0429 00:52:16.031443   966 solver.cpp:244]     Train net output #0: loss = 0.120866 (* 1 = 0.120866 loss)
I0429 00:52:16.031450   966 sgd_solver.cpp:106] Iteration 21500, lr = 0.0001
I0429 00:53:07.302592   966 solver.cpp:228] Iteration 21600, loss = 0.133852
I0429 00:53:07.302744   966 solver.cpp:244]     Train net output #0: loss = 0.133852 (* 1 = 0.133852 loss)
I0429 00:53:07.302752   966 sgd_solver.cpp:106] Iteration 21600, lr = 0.0001
I0429 00:53:56.784023   966 solver.cpp:228] Iteration 21700, loss = 0.0866736
I0429 00:53:56.784174   966 solver.cpp:244]     Train net output #0: loss = 0.0866736 (* 1 = 0.0866736 loss)
I0429 00:53:56.784180   966 sgd_solver.cpp:106] Iteration 21700, lr = 0.0001
I0429 00:54:46.279906   966 solver.cpp:228] Iteration 21800, loss = 0.100791
I0429 00:54:46.280069   966 solver.cpp:244]     Train net output #0: loss = 0.100791 (* 1 = 0.100791 loss)
I0429 00:54:46.280077   966 sgd_solver.cpp:106] Iteration 21800, lr = 0.0001
I0429 00:55:37.126677   966 solver.cpp:228] Iteration 21900, loss = 0.162536
I0429 00:55:37.126833   966 solver.cpp:244]     Train net output #0: loss = 0.162536 (* 1 = 0.162536 loss)
I0429 00:55:37.126840   966 sgd_solver.cpp:106] Iteration 21900, lr = 0.0001
I0429 00:56:26.326009   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_22000.caffemodel
I0429 00:56:52.283900   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_22000.solverstate
I0429 00:56:52.472838   966 solver.cpp:337] Iteration 22000, Testing net (#0)
I0429 00:56:52.472982   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 00:56:52.472990   966 net.cpp:693] Ignoring source layer visualize
I0429 00:56:52.472992   966 net.cpp:693] Ignoring source layer fake
I0429 01:01:45.846748   966 solver.cpp:404]     Test net output #0: loss = 0.18369 (* 1 = 0.18369 loss)
I0429 01:01:46.156703   966 solver.cpp:228] Iteration 22000, loss = 0.0817532
I0429 01:01:46.156723   966 solver.cpp:244]     Train net output #0: loss = 0.0817532 (* 1 = 0.0817532 loss)
I0429 01:01:46.156744   966 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I0429 01:02:35.666765   966 solver.cpp:228] Iteration 22100, loss = 0.178683
I0429 01:02:35.666939   966 solver.cpp:244]     Train net output #0: loss = 0.178683 (* 1 = 0.178683 loss)
I0429 01:02:35.666947   966 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I0429 01:03:26.581123   966 solver.cpp:228] Iteration 22200, loss = 0.14971
I0429 01:03:26.581807   966 solver.cpp:244]     Train net output #0: loss = 0.14971 (* 1 = 0.14971 loss)
I0429 01:03:26.581815   966 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I0429 01:04:16.086429   966 solver.cpp:228] Iteration 22300, loss = 0.132104
I0429 01:04:16.088245   966 solver.cpp:244]     Train net output #0: loss = 0.132104 (* 1 = 0.132104 loss)
I0429 01:04:16.088253   966 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I0429 01:05:07.247896   966 solver.cpp:228] Iteration 22400, loss = 0.0765949
I0429 01:05:07.248059   966 solver.cpp:244]     Train net output #0: loss = 0.0765949 (* 1 = 0.0765949 loss)
I0429 01:05:07.248065   966 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I0429 01:05:56.737012   966 solver.cpp:228] Iteration 22500, loss = 0.119208
I0429 01:05:56.737184   966 solver.cpp:244]     Train net output #0: loss = 0.119208 (* 1 = 0.119208 loss)
I0429 01:05:56.737191   966 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0429 01:06:46.247107   966 solver.cpp:228] Iteration 22600, loss = 0.300607
I0429 01:06:46.247299   966 solver.cpp:244]     Train net output #0: loss = 0.300607 (* 1 = 0.300607 loss)
I0429 01:06:46.247308   966 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0429 01:07:37.665235   966 solver.cpp:228] Iteration 22700, loss = 0.0731156
I0429 01:07:37.666218   966 solver.cpp:244]     Train net output #0: loss = 0.0731156 (* 1 = 0.0731156 loss)
I0429 01:07:37.666226   966 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0429 01:08:27.174265   966 solver.cpp:228] Iteration 22800, loss = 0.118195
I0429 01:08:27.174468   966 solver.cpp:244]     Train net output #0: loss = 0.118195 (* 1 = 0.118195 loss)
I0429 01:08:27.174474   966 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0429 01:09:16.679333   966 solver.cpp:228] Iteration 22900, loss = 0.31946
I0429 01:09:16.679492   966 solver.cpp:244]     Train net output #0: loss = 0.31946 (* 1 = 0.31946 loss)
I0429 01:09:16.679500   966 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0429 01:10:05.875540   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_23000.caffemodel
I0429 01:10:26.428843   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_23000.solverstate
I0429 01:10:26.630425   966 solver.cpp:337] Iteration 23000, Testing net (#0)
I0429 01:10:26.630568   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 01:10:26.630574   966 net.cpp:693] Ignoring source layer visualize
I0429 01:10:26.630578   966 net.cpp:693] Ignoring source layer fake
I0429 01:15:19.133028   966 solver.cpp:404]     Test net output #0: loss = 0.199588 (* 1 = 0.199588 loss)
I0429 01:15:19.443786   966 solver.cpp:228] Iteration 23000, loss = 0.193324
I0429 01:15:19.443806   966 solver.cpp:244]     Train net output #0: loss = 0.193324 (* 1 = 0.193324 loss)
I0429 01:15:19.443828   966 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0429 01:16:10.950420   966 solver.cpp:228] Iteration 23100, loss = 0.222061
I0429 01:16:10.950585   966 solver.cpp:244]     Train net output #0: loss = 0.222061 (* 1 = 0.222061 loss)
I0429 01:16:10.950592   966 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0429 01:17:00.448732   966 solver.cpp:228] Iteration 23200, loss = 0.167096
I0429 01:17:00.448894   966 solver.cpp:244]     Train net output #0: loss = 0.167096 (* 1 = 0.167096 loss)
I0429 01:17:00.448901   966 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0429 01:17:49.963264   966 solver.cpp:228] Iteration 23300, loss = 0.199979
I0429 01:17:49.963428   966 solver.cpp:244]     Train net output #0: loss = 0.199979 (* 1 = 0.199979 loss)
I0429 01:17:49.963436   966 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0429 01:18:41.533301   966 solver.cpp:228] Iteration 23400, loss = 0.102812
I0429 01:18:41.533490   966 solver.cpp:244]     Train net output #0: loss = 0.102812 (* 1 = 0.102812 loss)
I0429 01:18:41.533499   966 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0429 01:19:31.043232   966 solver.cpp:228] Iteration 23500, loss = 0.111552
I0429 01:19:31.043391   966 solver.cpp:244]     Train net output #0: loss = 0.111552 (* 1 = 0.111552 loss)
I0429 01:19:31.043400   966 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0429 01:20:20.552861   966 solver.cpp:228] Iteration 23600, loss = 0.165401
I0429 01:20:20.553038   966 solver.cpp:244]     Train net output #0: loss = 0.165401 (* 1 = 0.165401 loss)
I0429 01:20:20.553046   966 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0429 01:21:11.999415   966 solver.cpp:228] Iteration 23700, loss = 0.114301
I0429 01:21:11.999577   966 solver.cpp:244]     Train net output #0: loss = 0.114301 (* 1 = 0.114301 loss)
I0429 01:21:11.999584   966 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0429 01:22:01.504526   966 solver.cpp:228] Iteration 23800, loss = 0.161666
I0429 01:22:01.504684   966 solver.cpp:244]     Train net output #0: loss = 0.161666 (* 1 = 0.161666 loss)
I0429 01:22:01.504691   966 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0429 01:22:51.008616   966 solver.cpp:228] Iteration 23900, loss = 0.202195
I0429 01:22:51.008798   966 solver.cpp:244]     Train net output #0: loss = 0.202195 (* 1 = 0.202195 loss)
I0429 01:22:51.008806   966 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0429 01:23:40.203590   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_24000.caffemodel
I0429 01:23:54.267385   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_24000.solverstate
I0429 01:23:54.455881   966 solver.cpp:337] Iteration 24000, Testing net (#0)
I0429 01:23:54.456007   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 01:23:54.456028   966 net.cpp:693] Ignoring source layer visualize
I0429 01:23:54.456030   966 net.cpp:693] Ignoring source layer fake
I0429 01:28:47.115921   966 solver.cpp:404]     Test net output #0: loss = 0.195525 (* 1 = 0.195525 loss)
I0429 01:28:47.426358   966 solver.cpp:228] Iteration 24000, loss = 0.160803
I0429 01:28:47.426378   966 solver.cpp:244]     Train net output #0: loss = 0.160803 (* 1 = 0.160803 loss)
I0429 01:28:47.426400   966 sgd_solver.cpp:106] Iteration 24000, lr = 1e-05
I0429 01:29:38.859843   966 solver.cpp:228] Iteration 24100, loss = 0.11187
I0429 01:29:38.860013   966 solver.cpp:244]     Train net output #0: loss = 0.11187 (* 1 = 0.11187 loss)
I0429 01:29:38.860023   966 sgd_solver.cpp:106] Iteration 24100, lr = 1e-05
I0429 01:30:28.360569   966 solver.cpp:228] Iteration 24200, loss = 0.137966
I0429 01:30:28.360721   966 solver.cpp:244]     Train net output #0: loss = 0.137966 (* 1 = 0.137966 loss)
I0429 01:30:28.360729   966 sgd_solver.cpp:106] Iteration 24200, lr = 1e-05
I0429 01:31:17.861544   966 solver.cpp:228] Iteration 24300, loss = 0.137054
I0429 01:31:17.861709   966 solver.cpp:244]     Train net output #0: loss = 0.137054 (* 1 = 0.137054 loss)
I0429 01:31:17.861717   966 sgd_solver.cpp:106] Iteration 24300, lr = 1e-05
I0429 01:32:09.272016   966 solver.cpp:228] Iteration 24400, loss = 0.113239
I0429 01:32:09.272197   966 solver.cpp:244]     Train net output #0: loss = 0.113239 (* 1 = 0.113239 loss)
I0429 01:32:09.272203   966 sgd_solver.cpp:106] Iteration 24400, lr = 1e-05
I0429 01:32:58.779355   966 solver.cpp:228] Iteration 24500, loss = 0.0932602
I0429 01:32:58.779515   966 solver.cpp:244]     Train net output #0: loss = 0.0932602 (* 1 = 0.0932602 loss)
I0429 01:32:58.779522   966 sgd_solver.cpp:106] Iteration 24500, lr = 1e-05
I0429 01:33:48.288054   966 solver.cpp:228] Iteration 24600, loss = 0.162604
I0429 01:33:48.288230   966 solver.cpp:244]     Train net output #0: loss = 0.162604 (* 1 = 0.162604 loss)
I0429 01:33:48.288238   966 sgd_solver.cpp:106] Iteration 24600, lr = 1e-05
I0429 01:34:39.579542   966 solver.cpp:228] Iteration 24700, loss = 0.0981914
I0429 01:34:39.579699   966 solver.cpp:244]     Train net output #0: loss = 0.0981914 (* 1 = 0.0981914 loss)
I0429 01:34:39.579706   966 sgd_solver.cpp:106] Iteration 24700, lr = 1e-05
I0429 01:35:29.097373   966 solver.cpp:228] Iteration 24800, loss = 0.12702
I0429 01:35:29.097532   966 solver.cpp:244]     Train net output #0: loss = 0.12702 (* 1 = 0.12702 loss)
I0429 01:35:29.097539   966 sgd_solver.cpp:106] Iteration 24800, lr = 1e-05
I0429 01:36:18.610607   966 solver.cpp:228] Iteration 24900, loss = 0.110601
I0429 01:36:18.610769   966 solver.cpp:244]     Train net output #0: loss = 0.110601 (* 1 = 0.110601 loss)
I0429 01:36:18.610775   966 sgd_solver.cpp:106] Iteration 24900, lr = 1e-05
I0429 01:37:09.173092   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_25000.caffemodel
I0429 01:37:34.585183   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_25000.solverstate
I0429 01:37:34.776192   966 solver.cpp:337] Iteration 25000, Testing net (#0)
I0429 01:37:34.776319   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 01:37:34.776324   966 net.cpp:693] Ignoring source layer visualize
I0429 01:37:34.776327   966 net.cpp:693] Ignoring source layer fake
I0429 01:42:27.638850   966 solver.cpp:404]     Test net output #0: loss = 0.189525 (* 1 = 0.189525 loss)
I0429 01:42:27.950595   966 solver.cpp:228] Iteration 25000, loss = 0.0864845
I0429 01:42:27.950634   966 solver.cpp:244]     Train net output #0: loss = 0.0864845 (* 1 = 0.0864845 loss)
I0429 01:42:27.950641   966 sgd_solver.cpp:106] Iteration 25000, lr = 1e-05
I0429 01:43:17.468485   966 solver.cpp:228] Iteration 25100, loss = 0.137056
I0429 01:43:17.468646   966 solver.cpp:244]     Train net output #0: loss = 0.137056 (* 1 = 0.137056 loss)
I0429 01:43:17.468652   966 sgd_solver.cpp:106] Iteration 25100, lr = 1e-05
I0429 01:44:06.986744   966 solver.cpp:228] Iteration 25200, loss = 0.0994199
I0429 01:44:06.986907   966 solver.cpp:244]     Train net output #0: loss = 0.0994199 (* 1 = 0.0994199 loss)
I0429 01:44:06.986914   966 sgd_solver.cpp:106] Iteration 25200, lr = 1e-05
I0429 01:44:57.924731   966 solver.cpp:228] Iteration 25300, loss = 0.0877365
I0429 01:44:57.924921   966 solver.cpp:244]     Train net output #0: loss = 0.0877365 (* 1 = 0.0877365 loss)
I0429 01:44:57.924928   966 sgd_solver.cpp:106] Iteration 25300, lr = 1e-05
I0429 01:45:47.442925   966 solver.cpp:228] Iteration 25400, loss = 0.176245
I0429 01:45:47.443091   966 solver.cpp:244]     Train net output #0: loss = 0.176245 (* 1 = 0.176245 loss)
I0429 01:45:47.443099   966 sgd_solver.cpp:106] Iteration 25400, lr = 1e-05
I0429 01:46:36.957000   966 solver.cpp:228] Iteration 25500, loss = 0.0523412
I0429 01:46:36.957172   966 solver.cpp:244]     Train net output #0: loss = 0.0523412 (* 1 = 0.0523412 loss)
I0429 01:46:36.957180   966 sgd_solver.cpp:106] Iteration 25500, lr = 1e-05
I0429 01:47:28.170115   966 solver.cpp:228] Iteration 25600, loss = 0.0989991
I0429 01:47:28.173357   966 solver.cpp:244]     Train net output #0: loss = 0.0989991 (* 1 = 0.0989991 loss)
I0429 01:47:28.173365   966 sgd_solver.cpp:106] Iteration 25600, lr = 1e-05
I0429 01:48:17.682718   966 solver.cpp:228] Iteration 25700, loss = 0.178395
I0429 01:48:17.682909   966 solver.cpp:244]     Train net output #0: loss = 0.178395 (* 1 = 0.178395 loss)
I0429 01:48:17.682916   966 sgd_solver.cpp:106] Iteration 25700, lr = 1e-05
I0429 01:49:09.117674   966 solver.cpp:228] Iteration 25800, loss = 0.165492
I0429 01:49:09.117847   966 solver.cpp:244]     Train net output #0: loss = 0.165492 (* 1 = 0.165492 loss)
I0429 01:49:09.117856   966 sgd_solver.cpp:106] Iteration 25800, lr = 1e-05
I0429 01:49:58.634451   966 solver.cpp:228] Iteration 25900, loss = 0.222593
I0429 01:49:58.634620   966 solver.cpp:244]     Train net output #0: loss = 0.222593 (* 1 = 0.222593 loss)
I0429 01:49:58.634627   966 sgd_solver.cpp:106] Iteration 25900, lr = 1e-05
I0429 01:50:47.833614   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_26000.caffemodel
I0429 01:51:07.436591   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_26000.solverstate
I0429 01:51:07.682752   966 solver.cpp:337] Iteration 26000, Testing net (#0)
I0429 01:51:07.682885   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 01:51:07.682909   966 net.cpp:693] Ignoring source layer visualize
I0429 01:51:07.682912   966 net.cpp:693] Ignoring source layer fake
I0429 01:56:00.304729   966 solver.cpp:404]     Test net output #0: loss = 0.183861 (* 1 = 0.183861 loss)
I0429 01:56:00.615382   966 solver.cpp:228] Iteration 26000, loss = 0.070754
I0429 01:56:00.615428   966 solver.cpp:244]     Train net output #0: loss = 0.070754 (* 1 = 0.070754 loss)
I0429 01:56:00.615437   966 sgd_solver.cpp:106] Iteration 26000, lr = 1e-05
I0429 01:56:50.141760   966 solver.cpp:228] Iteration 26100, loss = 0.226977
I0429 01:56:50.141927   966 solver.cpp:244]     Train net output #0: loss = 0.226977 (* 1 = 0.226977 loss)
I0429 01:56:50.141937   966 sgd_solver.cpp:106] Iteration 26100, lr = 1e-05
I0429 01:57:41.589651   966 solver.cpp:228] Iteration 26200, loss = 0.178708
I0429 01:57:41.589839   966 solver.cpp:244]     Train net output #0: loss = 0.178708 (* 1 = 0.178708 loss)
I0429 01:57:41.589848   966 sgd_solver.cpp:106] Iteration 26200, lr = 1e-05
I0429 01:58:31.111454   966 solver.cpp:228] Iteration 26300, loss = 0.108012
I0429 01:58:31.111614   966 solver.cpp:244]     Train net output #0: loss = 0.108012 (* 1 = 0.108012 loss)
I0429 01:58:31.111621   966 sgd_solver.cpp:106] Iteration 26300, lr = 1e-05
I0429 01:59:20.629962   966 solver.cpp:228] Iteration 26400, loss = 0.0951063
I0429 01:59:20.630126   966 solver.cpp:244]     Train net output #0: loss = 0.0951063 (* 1 = 0.0951063 loss)
I0429 01:59:20.630133   966 sgd_solver.cpp:106] Iteration 26400, lr = 1e-05
I0429 02:00:12.184368   966 solver.cpp:228] Iteration 26500, loss = 0.150576
I0429 02:00:12.184531   966 solver.cpp:244]     Train net output #0: loss = 0.150576 (* 1 = 0.150576 loss)
I0429 02:00:12.184540   966 sgd_solver.cpp:106] Iteration 26500, lr = 1e-05
I0429 02:01:01.692549   966 solver.cpp:228] Iteration 26600, loss = 0.185362
I0429 02:01:01.692716   966 solver.cpp:244]     Train net output #0: loss = 0.185362 (* 1 = 0.185362 loss)
I0429 02:01:01.692723   966 sgd_solver.cpp:106] Iteration 26600, lr = 1e-05
I0429 02:01:51.210877   966 solver.cpp:228] Iteration 26700, loss = 0.169121
I0429 02:01:51.211046   966 solver.cpp:244]     Train net output #0: loss = 0.169121 (* 1 = 0.169121 loss)
I0429 02:01:51.211055   966 sgd_solver.cpp:106] Iteration 26700, lr = 1e-05
I0429 02:02:40.720734   966 solver.cpp:228] Iteration 26800, loss = 0.188994
I0429 02:02:40.720968   966 solver.cpp:244]     Train net output #0: loss = 0.188994 (* 1 = 0.188994 loss)
I0429 02:02:40.720974   966 sgd_solver.cpp:106] Iteration 26800, lr = 1e-05
I0429 02:03:32.328308   966 solver.cpp:228] Iteration 26900, loss = 0.12163
I0429 02:03:32.328466   966 solver.cpp:244]     Train net output #0: loss = 0.12163 (* 1 = 0.12163 loss)
I0429 02:03:32.328474   966 sgd_solver.cpp:106] Iteration 26900, lr = 1e-05
I0429 02:04:21.527225   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_27000.caffemodel
I0429 02:04:43.916725   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_27000.solverstate
I0429 02:04:44.106678   966 solver.cpp:337] Iteration 27000, Testing net (#0)
I0429 02:04:44.106801   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 02:04:44.106807   966 net.cpp:693] Ignoring source layer visualize
I0429 02:04:44.106809   966 net.cpp:693] Ignoring source layer fake
I0429 02:09:37.065491   966 solver.cpp:404]     Test net output #0: loss = 0.183673 (* 1 = 0.183673 loss)
I0429 02:09:37.374434   966 solver.cpp:228] Iteration 27000, loss = 0.107211
I0429 02:09:37.374454   966 solver.cpp:244]     Train net output #0: loss = 0.107211 (* 1 = 0.107211 loss)
I0429 02:09:37.374460   966 sgd_solver.cpp:106] Iteration 27000, lr = 1e-05
I0429 02:10:26.889149   966 solver.cpp:228] Iteration 27100, loss = 0.146488
I0429 02:10:26.889333   966 solver.cpp:244]     Train net output #0: loss = 0.146488 (* 1 = 0.146488 loss)
I0429 02:10:26.889340   966 sgd_solver.cpp:106] Iteration 27100, lr = 1e-05
I0429 02:11:18.247757   966 solver.cpp:228] Iteration 27200, loss = 0.181761
I0429 02:11:18.247927   966 solver.cpp:244]     Train net output #0: loss = 0.181761 (* 1 = 0.181761 loss)
I0429 02:11:18.247936   966 sgd_solver.cpp:106] Iteration 27200, lr = 1e-05
I0429 02:12:07.745206   966 solver.cpp:228] Iteration 27300, loss = 0.1754
I0429 02:12:07.745400   966 solver.cpp:244]     Train net output #0: loss = 0.1754 (* 1 = 0.1754 loss)
I0429 02:12:07.745409   966 sgd_solver.cpp:106] Iteration 27300, lr = 1e-05
I0429 02:12:57.254215   966 solver.cpp:228] Iteration 27400, loss = 0.0683615
I0429 02:12:57.254364   966 solver.cpp:244]     Train net output #0: loss = 0.0683615 (* 1 = 0.0683615 loss)
I0429 02:12:57.254370   966 sgd_solver.cpp:106] Iteration 27400, lr = 1e-05
I0429 02:13:48.788877   966 solver.cpp:228] Iteration 27500, loss = 0.105926
I0429 02:13:48.789073   966 solver.cpp:244]     Train net output #0: loss = 0.105926 (* 1 = 0.105926 loss)
I0429 02:13:48.789082   966 sgd_solver.cpp:106] Iteration 27500, lr = 1e-05
I0429 02:14:38.286675   966 solver.cpp:228] Iteration 27600, loss = 0.0689985
I0429 02:14:38.286851   966 solver.cpp:244]     Train net output #0: loss = 0.0689985 (* 1 = 0.0689985 loss)
I0429 02:14:38.286860   966 sgd_solver.cpp:106] Iteration 27600, lr = 1e-05
I0429 02:15:27.804122   966 solver.cpp:228] Iteration 27700, loss = 0.101939
I0429 02:15:27.804293   966 solver.cpp:244]     Train net output #0: loss = 0.101939 (* 1 = 0.101939 loss)
I0429 02:15:27.804301   966 sgd_solver.cpp:106] Iteration 27700, lr = 1e-05
I0429 02:16:17.308434   966 solver.cpp:228] Iteration 27800, loss = 0.126326
I0429 02:16:17.308604   966 solver.cpp:244]     Train net output #0: loss = 0.126326 (* 1 = 0.126326 loss)
I0429 02:16:17.308611   966 sgd_solver.cpp:106] Iteration 27800, lr = 1e-05
I0429 02:17:08.576277   966 solver.cpp:228] Iteration 27900, loss = 0.0827183
I0429 02:17:08.576443   966 solver.cpp:244]     Train net output #0: loss = 0.0827183 (* 1 = 0.0827183 loss)
I0429 02:17:08.576450   966 sgd_solver.cpp:106] Iteration 27900, lr = 1e-05
I0429 02:17:57.772295   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_28000.caffemodel
I0429 02:18:08.336901   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_28000.solverstate
I0429 02:18:08.536568   966 solver.cpp:337] Iteration 28000, Testing net (#0)
I0429 02:18:08.536695   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 02:18:08.536701   966 net.cpp:693] Ignoring source layer visualize
I0429 02:18:08.536703   966 net.cpp:693] Ignoring source layer fake
I0429 02:23:01.496230   966 solver.cpp:404]     Test net output #0: loss = 0.175918 (* 1 = 0.175918 loss)
I0429 02:23:01.807050   966 solver.cpp:228] Iteration 28000, loss = 0.201932
I0429 02:23:01.807085   966 solver.cpp:244]     Train net output #0: loss = 0.201932 (* 1 = 0.201932 loss)
I0429 02:23:01.807091   966 sgd_solver.cpp:106] Iteration 28000, lr = 1e-05
I0429 02:23:51.313300   966 solver.cpp:228] Iteration 28100, loss = 0.16534
I0429 02:23:51.313501   966 solver.cpp:244]     Train net output #0: loss = 0.16534 (* 1 = 0.16534 loss)
I0429 02:23:51.313508   966 sgd_solver.cpp:106] Iteration 28100, lr = 1e-05
I0429 02:24:42.153838   966 solver.cpp:228] Iteration 28200, loss = 0.136768
I0429 02:24:42.154007   966 solver.cpp:244]     Train net output #0: loss = 0.136768 (* 1 = 0.136768 loss)
I0429 02:24:42.154014   966 sgd_solver.cpp:106] Iteration 28200, lr = 1e-05
I0429 02:25:31.672291   966 solver.cpp:228] Iteration 28300, loss = 0.130525
I0429 02:25:31.672478   966 solver.cpp:244]     Train net output #0: loss = 0.130525 (* 1 = 0.130525 loss)
I0429 02:25:31.672488   966 sgd_solver.cpp:106] Iteration 28300, lr = 1e-05
I0429 02:26:22.726740   966 solver.cpp:228] Iteration 28400, loss = 0.112649
I0429 02:26:22.726905   966 solver.cpp:244]     Train net output #0: loss = 0.112649 (* 1 = 0.112649 loss)
I0429 02:26:22.726912   966 sgd_solver.cpp:106] Iteration 28400, lr = 1e-05
I0429 02:27:12.246616   966 solver.cpp:228] Iteration 28500, loss = 0.121591
I0429 02:27:12.246781   966 solver.cpp:244]     Train net output #0: loss = 0.121591 (* 1 = 0.121591 loss)
I0429 02:27:12.246788   966 sgd_solver.cpp:106] Iteration 28500, lr = 1e-05
I0429 02:28:01.784152   966 solver.cpp:228] Iteration 28600, loss = 0.0905216
I0429 02:28:01.784309   966 solver.cpp:244]     Train net output #0: loss = 0.0905216 (* 1 = 0.0905216 loss)
I0429 02:28:01.784317   966 sgd_solver.cpp:106] Iteration 28600, lr = 1e-05
I0429 02:28:52.903982   966 solver.cpp:228] Iteration 28700, loss = 0.144179
I0429 02:28:52.904167   966 solver.cpp:244]     Train net output #0: loss = 0.144179 (* 1 = 0.144179 loss)
I0429 02:28:52.904175   966 sgd_solver.cpp:106] Iteration 28700, lr = 1e-05
I0429 02:29:42.430896   966 solver.cpp:228] Iteration 28800, loss = 0.113071
I0429 02:29:42.431077   966 solver.cpp:244]     Train net output #0: loss = 0.113071 (* 1 = 0.113071 loss)
I0429 02:29:42.431084   966 sgd_solver.cpp:106] Iteration 28800, lr = 1e-05
I0429 02:30:31.963618   966 solver.cpp:228] Iteration 28900, loss = 0.125145
I0429 02:30:31.963781   966 solver.cpp:244]     Train net output #0: loss = 0.125145 (* 1 = 0.125145 loss)
I0429 02:30:31.963788   966 sgd_solver.cpp:106] Iteration 28900, lr = 1e-05
I0429 02:31:23.594123   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_29000.caffemodel
I0429 02:31:50.508282   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_29000.solverstate
I0429 02:31:50.702720   966 solver.cpp:337] Iteration 29000, Testing net (#0)
I0429 02:31:50.702848   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 02:31:50.702855   966 net.cpp:693] Ignoring source layer visualize
I0429 02:31:50.702858   966 net.cpp:693] Ignoring source layer fake
I0429 02:36:43.726609   966 solver.cpp:404]     Test net output #0: loss = 0.187249 (* 1 = 0.187249 loss)
I0429 02:36:44.035110   966 solver.cpp:228] Iteration 29000, loss = 0.338815
I0429 02:36:44.035130   966 solver.cpp:244]     Train net output #0: loss = 0.338815 (* 1 = 0.338815 loss)
I0429 02:36:44.035153   966 sgd_solver.cpp:106] Iteration 29000, lr = 1e-05
I0429 02:37:33.569104   966 solver.cpp:228] Iteration 29100, loss = 0.160934
I0429 02:37:33.569299   966 solver.cpp:244]     Train net output #0: loss = 0.160934 (* 1 = 0.160934 loss)
I0429 02:37:33.569308   966 sgd_solver.cpp:106] Iteration 29100, lr = 1e-05
I0429 02:38:23.092046   966 solver.cpp:228] Iteration 29200, loss = 0.194363
I0429 02:38:23.092234   966 solver.cpp:244]     Train net output #0: loss = 0.194363 (* 1 = 0.194363 loss)
I0429 02:38:23.092243   966 sgd_solver.cpp:106] Iteration 29200, lr = 1e-05
I0429 02:39:14.513922   966 solver.cpp:228] Iteration 29300, loss = 0.156743
I0429 02:39:14.514107   966 solver.cpp:244]     Train net output #0: loss = 0.156743 (* 1 = 0.156743 loss)
I0429 02:39:14.514113   966 sgd_solver.cpp:106] Iteration 29300, lr = 1e-05
I0429 02:40:04.040310   966 solver.cpp:228] Iteration 29400, loss = 0.167259
I0429 02:40:04.040475   966 solver.cpp:244]     Train net output #0: loss = 0.167259 (* 1 = 0.167259 loss)
I0429 02:40:04.040482   966 sgd_solver.cpp:106] Iteration 29400, lr = 1e-05
I0429 02:40:53.577657   966 solver.cpp:228] Iteration 29500, loss = 0.0805818
I0429 02:40:53.577819   966 solver.cpp:244]     Train net output #0: loss = 0.0805818 (* 1 = 0.0805818 loss)
I0429 02:40:53.577826   966 sgd_solver.cpp:106] Iteration 29500, lr = 1e-05
I0429 02:41:45.088554   966 solver.cpp:228] Iteration 29600, loss = 0.114291
I0429 02:41:45.088825   966 solver.cpp:244]     Train net output #0: loss = 0.114291 (* 1 = 0.114291 loss)
I0429 02:41:45.088841   966 sgd_solver.cpp:106] Iteration 29600, lr = 1e-05
I0429 02:42:34.602355   966 solver.cpp:228] Iteration 29700, loss = 0.102014
I0429 02:42:34.602694   966 solver.cpp:244]     Train net output #0: loss = 0.102014 (* 1 = 0.102014 loss)
I0429 02:42:34.602704   966 sgd_solver.cpp:106] Iteration 29700, lr = 1e-05
I0429 02:43:24.123117   966 solver.cpp:228] Iteration 29800, loss = 0.323392
I0429 02:43:24.123286   966 solver.cpp:244]     Train net output #0: loss = 0.323392 (* 1 = 0.323392 loss)
I0429 02:43:24.123293   966 sgd_solver.cpp:106] Iteration 29800, lr = 1e-05
I0429 02:44:13.642469   966 solver.cpp:228] Iteration 29900, loss = 0.152512
I0429 02:44:13.644064   966 solver.cpp:244]     Train net output #0: loss = 0.152512 (* 1 = 0.152512 loss)
I0429 02:44:13.644070   966 sgd_solver.cpp:106] Iteration 29900, lr = 1e-05
I0429 02:45:04.656610   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_30000.caffemodel
I0429 02:45:15.301964   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_30000.solverstate
I0429 02:45:15.492025   966 solver.cpp:337] Iteration 30000, Testing net (#0)
I0429 02:45:15.492163   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 02:45:15.492169   966 net.cpp:693] Ignoring source layer visualize
I0429 02:45:15.492172   966 net.cpp:693] Ignoring source layer fake
I0429 02:50:07.958492   966 solver.cpp:404]     Test net output #0: loss = 0.192999 (* 1 = 0.192999 loss)
I0429 02:50:08.267686   966 solver.cpp:228] Iteration 30000, loss = 0.126335
I0429 02:50:08.267705   966 solver.cpp:244]     Train net output #0: loss = 0.126335 (* 1 = 0.126335 loss)
I0429 02:50:08.267727   966 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0429 02:50:57.802677   966 solver.cpp:228] Iteration 30100, loss = 0.272679
I0429 02:50:57.802847   966 solver.cpp:244]     Train net output #0: loss = 0.272679 (* 1 = 0.272679 loss)
I0429 02:50:57.802855   966 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0429 02:51:47.329813   966 solver.cpp:228] Iteration 30200, loss = 0.129348
I0429 02:51:47.329972   966 solver.cpp:244]     Train net output #0: loss = 0.129348 (* 1 = 0.129348 loss)
I0429 02:51:47.329979   966 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0429 02:52:38.644803   966 solver.cpp:228] Iteration 30300, loss = 0.148109
I0429 02:52:38.645365   966 solver.cpp:244]     Train net output #0: loss = 0.148109 (* 1 = 0.148109 loss)
I0429 02:52:38.645372   966 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0429 02:53:28.145203   966 solver.cpp:228] Iteration 30400, loss = 0.15835
I0429 02:53:28.145375   966 solver.cpp:244]     Train net output #0: loss = 0.15835 (* 1 = 0.15835 loss)
I0429 02:53:28.145381   966 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0429 02:54:17.646805   966 solver.cpp:228] Iteration 30500, loss = 0.164159
I0429 02:54:17.646988   966 solver.cpp:244]     Train net output #0: loss = 0.164159 (* 1 = 0.164159 loss)
I0429 02:54:17.646997   966 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0429 02:55:07.157058   966 solver.cpp:228] Iteration 30600, loss = 0.0864348
I0429 02:55:07.157209   966 solver.cpp:244]     Train net output #0: loss = 0.0864348 (* 1 = 0.0864348 loss)
I0429 02:55:07.157217   966 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0429 02:55:58.444710   966 solver.cpp:228] Iteration 30700, loss = 0.1044
I0429 02:55:58.444872   966 solver.cpp:244]     Train net output #0: loss = 0.1044 (* 1 = 0.1044 loss)
I0429 02:55:58.444880   966 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0429 02:56:47.958559   966 solver.cpp:228] Iteration 30800, loss = 0.108342
I0429 02:56:47.958706   966 solver.cpp:244]     Train net output #0: loss = 0.108342 (* 1 = 0.108342 loss)
I0429 02:56:47.958714   966 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0429 02:57:37.467228   966 solver.cpp:228] Iteration 30900, loss = 0.102923
I0429 02:57:37.467396   966 solver.cpp:244]     Train net output #0: loss = 0.102923 (* 1 = 0.102923 loss)
I0429 02:57:37.467402   966 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0429 02:58:28.324733   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_31000.caffemodel
I0429 02:58:51.648243   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_31000.solverstate
I0429 02:58:51.846576   966 solver.cpp:337] Iteration 31000, Testing net (#0)
I0429 02:58:51.846696   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 02:58:51.846702   966 net.cpp:693] Ignoring source layer visualize
I0429 02:58:51.846704   966 net.cpp:693] Ignoring source layer fake
I0429 03:03:43.828446   966 solver.cpp:404]     Test net output #0: loss = 0.183765 (* 1 = 0.183765 loss)
I0429 03:03:44.138078   966 solver.cpp:228] Iteration 31000, loss = 0.113877
I0429 03:03:44.138098   966 solver.cpp:244]     Train net output #0: loss = 0.113877 (* 1 = 0.113877 loss)
I0429 03:03:44.138120   966 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0429 03:04:33.655760   966 solver.cpp:228] Iteration 31100, loss = 0.331839
I0429 03:04:33.655941   966 solver.cpp:244]     Train net output #0: loss = 0.331839 (* 1 = 0.331839 loss)
I0429 03:04:33.655948   966 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0429 03:05:23.160050   966 solver.cpp:228] Iteration 31200, loss = 0.218071
I0429 03:05:23.160643   966 solver.cpp:244]     Train net output #0: loss = 0.218071 (* 1 = 0.218071 loss)
I0429 03:05:23.160651   966 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0429 03:06:13.971737   966 solver.cpp:228] Iteration 31300, loss = 0.138067
I0429 03:06:13.972748   966 solver.cpp:244]     Train net output #0: loss = 0.138067 (* 1 = 0.138067 loss)
I0429 03:06:13.972755   966 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0429 03:07:03.480569   966 solver.cpp:228] Iteration 31400, loss = 0.110454
I0429 03:07:03.480717   966 solver.cpp:244]     Train net output #0: loss = 0.110454 (* 1 = 0.110454 loss)
I0429 03:07:03.480725   966 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0429 03:07:54.350915   966 solver.cpp:228] Iteration 31500, loss = 0.419495
I0429 03:07:54.352252   966 solver.cpp:244]     Train net output #0: loss = 0.419495 (* 1 = 0.419495 loss)
I0429 03:07:54.352260   966 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0429 03:08:43.863533   966 solver.cpp:228] Iteration 31600, loss = 0.139881
I0429 03:08:43.863695   966 solver.cpp:244]     Train net output #0: loss = 0.139881 (* 1 = 0.139881 loss)
I0429 03:08:43.863704   966 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0429 03:09:33.378710   966 solver.cpp:228] Iteration 31700, loss = 0.127852
I0429 03:09:33.378868   966 solver.cpp:244]     Train net output #0: loss = 0.127852 (* 1 = 0.127852 loss)
I0429 03:09:33.378875   966 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0429 03:10:24.446439   966 solver.cpp:228] Iteration 31800, loss = 0.103479
I0429 03:10:24.446601   966 solver.cpp:244]     Train net output #0: loss = 0.103479 (* 1 = 0.103479 loss)
I0429 03:10:24.446609   966 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0429 03:11:13.957280   966 solver.cpp:228] Iteration 31900, loss = 0.102644
I0429 03:11:13.957490   966 solver.cpp:244]     Train net output #0: loss = 0.102644 (* 1 = 0.102644 loss)
I0429 03:11:13.957499   966 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0429 03:12:03.164585   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_32000.caffemodel
I0429 03:12:38.521059   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_32000.solverstate
I0429 03:12:38.729840   966 solver.cpp:337] Iteration 32000, Testing net (#0)
I0429 03:12:38.729928   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 03:12:38.729934   966 net.cpp:693] Ignoring source layer visualize
I0429 03:12:38.729949   966 net.cpp:693] Ignoring source layer fake
I0429 03:17:31.550132   966 solver.cpp:404]     Test net output #0: loss = 0.183314 (* 1 = 0.183314 loss)
I0429 03:17:31.860785   966 solver.cpp:228] Iteration 32000, loss = 0.100214
I0429 03:17:31.860826   966 solver.cpp:244]     Train net output #0: loss = 0.100214 (* 1 = 0.100214 loss)
I0429 03:17:31.860832   966 sgd_solver.cpp:106] Iteration 32000, lr = 1e-06
I0429 03:18:23.176231   966 solver.cpp:228] Iteration 32100, loss = 0.187029
I0429 03:18:23.176401   966 solver.cpp:244]     Train net output #0: loss = 0.187029 (* 1 = 0.187029 loss)
I0429 03:18:23.176409   966 sgd_solver.cpp:106] Iteration 32100, lr = 1e-06
I0429 03:19:12.677341   966 solver.cpp:228] Iteration 32200, loss = 0.0655451
I0429 03:19:12.677538   966 solver.cpp:244]     Train net output #0: loss = 0.0655451 (* 1 = 0.0655451 loss)
I0429 03:19:12.677547   966 sgd_solver.cpp:106] Iteration 32200, lr = 1e-06
I0429 03:20:02.192709   966 solver.cpp:228] Iteration 32300, loss = 0.27104
I0429 03:20:02.192946   966 solver.cpp:244]     Train net output #0: loss = 0.27104 (* 1 = 0.27104 loss)
I0429 03:20:02.192966   966 sgd_solver.cpp:106] Iteration 32300, lr = 1e-06
I0429 03:20:53.612319   966 solver.cpp:228] Iteration 32400, loss = 0.0914339
I0429 03:20:53.612481   966 solver.cpp:244]     Train net output #0: loss = 0.0914339 (* 1 = 0.0914339 loss)
I0429 03:20:53.612488   966 sgd_solver.cpp:106] Iteration 32400, lr = 1e-06
I0429 03:21:43.137783   966 solver.cpp:228] Iteration 32500, loss = 0.0554698
I0429 03:21:43.137923   966 solver.cpp:244]     Train net output #0: loss = 0.0554698 (* 1 = 0.0554698 loss)
I0429 03:21:43.137929   966 sgd_solver.cpp:106] Iteration 32500, lr = 1e-06
I0429 03:22:32.654355   966 solver.cpp:228] Iteration 32600, loss = 0.121593
I0429 03:22:32.654515   966 solver.cpp:244]     Train net output #0: loss = 0.121593 (* 1 = 0.121593 loss)
I0429 03:22:32.654522   966 sgd_solver.cpp:106] Iteration 32600, lr = 1e-06
I0429 03:23:22.167510   966 solver.cpp:228] Iteration 32700, loss = 0.140644
I0429 03:23:22.167671   966 solver.cpp:244]     Train net output #0: loss = 0.140644 (* 1 = 0.140644 loss)
I0429 03:23:22.167680   966 sgd_solver.cpp:106] Iteration 32700, lr = 1e-06
I0429 03:24:13.536200   966 solver.cpp:228] Iteration 32800, loss = 0.106313
I0429 03:24:13.536355   966 solver.cpp:244]     Train net output #0: loss = 0.106313 (* 1 = 0.106313 loss)
I0429 03:24:13.536361   966 sgd_solver.cpp:106] Iteration 32800, lr = 1e-06
I0429 03:25:03.053836   966 solver.cpp:228] Iteration 32900, loss = 0.17488
I0429 03:25:03.054003   966 solver.cpp:244]     Train net output #0: loss = 0.17488 (* 1 = 0.17488 loss)
I0429 03:25:03.054009   966 sgd_solver.cpp:106] Iteration 32900, lr = 1e-06
I0429 03:25:52.264801   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_33000.caffemodel
I0429 03:26:09.588626   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_33000.solverstate
I0429 03:26:09.795935   966 solver.cpp:337] Iteration 33000, Testing net (#0)
I0429 03:26:09.796073   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 03:26:09.796085   966 net.cpp:693] Ignoring source layer visualize
I0429 03:26:09.796088   966 net.cpp:693] Ignoring source layer fake
I0429 03:31:02.707993   966 solver.cpp:404]     Test net output #0: loss = 0.181517 (* 1 = 0.181517 loss)
I0429 03:31:03.017061   966 solver.cpp:228] Iteration 33000, loss = 0.18756
I0429 03:31:03.017101   966 solver.cpp:244]     Train net output #0: loss = 0.18756 (* 1 = 0.18756 loss)
I0429 03:31:03.017108   966 sgd_solver.cpp:106] Iteration 33000, lr = 1e-06
I0429 03:31:54.395118   966 solver.cpp:228] Iteration 33100, loss = 0.232598
I0429 03:31:54.395292   966 solver.cpp:244]     Train net output #0: loss = 0.232598 (* 1 = 0.232598 loss)
I0429 03:31:54.395298   966 sgd_solver.cpp:106] Iteration 33100, lr = 1e-06
I0429 03:32:43.907300   966 solver.cpp:228] Iteration 33200, loss = 0.195595
I0429 03:32:43.907459   966 solver.cpp:244]     Train net output #0: loss = 0.195595 (* 1 = 0.195595 loss)
I0429 03:32:43.907466   966 sgd_solver.cpp:106] Iteration 33200, lr = 1e-06
I0429 03:33:33.416195   966 solver.cpp:228] Iteration 33300, loss = 0.103124
I0429 03:33:33.416365   966 solver.cpp:244]     Train net output #0: loss = 0.103124 (* 1 = 0.103124 loss)
I0429 03:33:33.416371   966 sgd_solver.cpp:106] Iteration 33300, lr = 1e-06
I0429 03:34:22.932667   966 solver.cpp:228] Iteration 33400, loss = 0.090403
I0429 03:34:22.932831   966 solver.cpp:244]     Train net output #0: loss = 0.090403 (* 1 = 0.090403 loss)
I0429 03:34:22.932837   966 sgd_solver.cpp:106] Iteration 33400, lr = 1e-06
I0429 03:35:14.236572   966 solver.cpp:228] Iteration 33500, loss = 0.132915
I0429 03:35:14.236738   966 solver.cpp:244]     Train net output #0: loss = 0.132915 (* 1 = 0.132915 loss)
I0429 03:35:14.236745   966 sgd_solver.cpp:106] Iteration 33500, lr = 1e-06
I0429 03:36:03.738869   966 solver.cpp:228] Iteration 33600, loss = 0.1791
I0429 03:36:03.739049   966 solver.cpp:244]     Train net output #0: loss = 0.1791 (* 1 = 0.1791 loss)
I0429 03:36:03.739056   966 sgd_solver.cpp:106] Iteration 33600, lr = 1e-06
I0429 03:36:53.247830   966 solver.cpp:228] Iteration 33700, loss = 0.16129
I0429 03:36:53.248005   966 solver.cpp:244]     Train net output #0: loss = 0.16129 (* 1 = 0.16129 loss)
I0429 03:36:53.248011   966 sgd_solver.cpp:106] Iteration 33700, lr = 1e-06
I0429 03:37:44.544827   966 solver.cpp:228] Iteration 33800, loss = 0.14736
I0429 03:37:44.544994   966 solver.cpp:244]     Train net output #0: loss = 0.14736 (* 1 = 0.14736 loss)
I0429 03:37:44.545002   966 sgd_solver.cpp:106] Iteration 33800, lr = 1e-06
I0429 03:38:34.042670   966 solver.cpp:228] Iteration 33900, loss = 0.174578
I0429 03:38:34.042821   966 solver.cpp:244]     Train net output #0: loss = 0.174578 (* 1 = 0.174578 loss)
I0429 03:38:34.042829   966 sgd_solver.cpp:106] Iteration 33900, lr = 1e-06
I0429 03:39:23.229383   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_34000.caffemodel
I0429 03:39:35.326485   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_34000.solverstate
I0429 03:39:35.639168   966 solver.cpp:337] Iteration 34000, Testing net (#0)
I0429 03:39:35.639288   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 03:39:35.639298   966 net.cpp:693] Ignoring source layer visualize
I0429 03:39:35.639300   966 net.cpp:693] Ignoring source layer fake
I0429 03:44:28.209045   966 solver.cpp:404]     Test net output #0: loss = 0.190032 (* 1 = 0.190032 loss)
I0429 03:44:28.519019   966 solver.cpp:228] Iteration 34000, loss = 0.0964804
I0429 03:44:28.519039   966 solver.cpp:244]     Train net output #0: loss = 0.0964804 (* 1 = 0.0964804 loss)
I0429 03:44:28.519062   966 sgd_solver.cpp:106] Iteration 34000, lr = 1e-06
I0429 03:45:19.948971   966 solver.cpp:228] Iteration 34100, loss = 0.389576
I0429 03:45:19.949121   966 solver.cpp:244]     Train net output #0: loss = 0.389576 (* 1 = 0.389576 loss)
I0429 03:45:19.949129   966 sgd_solver.cpp:106] Iteration 34100, lr = 1e-06
I0429 03:46:09.432911   966 solver.cpp:228] Iteration 34200, loss = 0.0525759
I0429 03:46:09.433117   966 solver.cpp:244]     Train net output #0: loss = 0.0525759 (* 1 = 0.0525759 loss)
I0429 03:46:09.433126   966 sgd_solver.cpp:106] Iteration 34200, lr = 1e-06
I0429 03:46:58.935564   966 solver.cpp:228] Iteration 34300, loss = 0.0661247
I0429 03:46:58.935730   966 solver.cpp:244]     Train net output #0: loss = 0.0661247 (* 1 = 0.0661247 loss)
I0429 03:46:58.935737   966 sgd_solver.cpp:106] Iteration 34300, lr = 1e-06
I0429 03:47:49.755791   966 solver.cpp:228] Iteration 34400, loss = 0.0945341
I0429 03:47:49.756007   966 solver.cpp:244]     Train net output #0: loss = 0.0945341 (* 1 = 0.0945341 loss)
I0429 03:47:49.756016   966 sgd_solver.cpp:106] Iteration 34400, lr = 1e-06
I0429 03:48:39.277004   966 solver.cpp:228] Iteration 34500, loss = 0.120769
I0429 03:48:39.277169   966 solver.cpp:244]     Train net output #0: loss = 0.120769 (* 1 = 0.120769 loss)
I0429 03:48:39.277176   966 sgd_solver.cpp:106] Iteration 34500, lr = 1e-06
I0429 03:49:28.786877   966 solver.cpp:228] Iteration 34600, loss = 0.106279
I0429 03:49:28.787044   966 solver.cpp:244]     Train net output #0: loss = 0.106279 (* 1 = 0.106279 loss)
I0429 03:49:28.787050   966 sgd_solver.cpp:106] Iteration 34600, lr = 1e-06
I0429 03:50:19.677837   966 solver.cpp:228] Iteration 34700, loss = 0.153836
I0429 03:50:19.678037   966 solver.cpp:244]     Train net output #0: loss = 0.153836 (* 1 = 0.153836 loss)
I0429 03:50:19.678045   966 sgd_solver.cpp:106] Iteration 34700, lr = 1e-06
I0429 03:51:09.188035   966 solver.cpp:228] Iteration 34800, loss = 0.234054
I0429 03:51:09.188185   966 solver.cpp:244]     Train net output #0: loss = 0.234054 (* 1 = 0.234054 loss)
I0429 03:51:09.188191   966 sgd_solver.cpp:106] Iteration 34800, lr = 1e-06
I0429 03:52:00.335670   966 solver.cpp:228] Iteration 34900, loss = 0.129457
I0429 03:52:00.335860   966 solver.cpp:244]     Train net output #0: loss = 0.129457 (* 1 = 0.129457 loss)
I0429 03:52:00.335868   966 sgd_solver.cpp:106] Iteration 34900, lr = 1e-06
I0429 03:52:49.535207   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_35000.caffemodel
I0429 03:53:11.473091   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_35000.solverstate
I0429 03:53:11.731737   966 solver.cpp:337] Iteration 35000, Testing net (#0)
I0429 03:53:11.731871   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 03:53:11.731895   966 net.cpp:693] Ignoring source layer visualize
I0429 03:53:11.731899   966 net.cpp:693] Ignoring source layer fake
I0429 03:58:04.617002   966 solver.cpp:404]     Test net output #0: loss = 0.189291 (* 1 = 0.189291 loss)
I0429 03:58:04.927348   966 solver.cpp:228] Iteration 35000, loss = 0.104633
I0429 03:58:04.927390   966 solver.cpp:244]     Train net output #0: loss = 0.104633 (* 1 = 0.104633 loss)
I0429 03:58:04.927397   966 sgd_solver.cpp:106] Iteration 35000, lr = 1e-06
I0429 03:58:54.451875   966 solver.cpp:228] Iteration 35100, loss = 0.152776
I0429 03:58:54.452042   966 solver.cpp:244]     Train net output #0: loss = 0.152776 (* 1 = 0.152776 loss)
I0429 03:58:54.452049   966 sgd_solver.cpp:106] Iteration 35100, lr = 1e-06
I0429 03:59:45.989164   966 solver.cpp:228] Iteration 35200, loss = 0.156356
I0429 03:59:45.989341   966 solver.cpp:244]     Train net output #0: loss = 0.156356 (* 1 = 0.156356 loss)
I0429 03:59:45.989349   966 sgd_solver.cpp:106] Iteration 35200, lr = 1e-06
I0429 04:00:35.514667   966 solver.cpp:228] Iteration 35300, loss = 0.112113
I0429 04:00:35.514854   966 solver.cpp:244]     Train net output #0: loss = 0.112113 (* 1 = 0.112113 loss)
I0429 04:00:35.514863   966 sgd_solver.cpp:106] Iteration 35300, lr = 1e-06
I0429 04:01:25.028585   966 solver.cpp:228] Iteration 35400, loss = 0.107744
I0429 04:01:25.028753   966 solver.cpp:244]     Train net output #0: loss = 0.107744 (* 1 = 0.107744 loss)
I0429 04:01:25.028759   966 sgd_solver.cpp:106] Iteration 35400, lr = 1e-06
I0429 04:02:16.486512   966 solver.cpp:228] Iteration 35500, loss = 0.198337
I0429 04:02:16.486681   966 solver.cpp:244]     Train net output #0: loss = 0.198337 (* 1 = 0.198337 loss)
I0429 04:02:16.486688   966 sgd_solver.cpp:106] Iteration 35500, lr = 1e-06
I0429 04:03:06.004926   966 solver.cpp:228] Iteration 35600, loss = 0.134273
I0429 04:03:06.005081   966 solver.cpp:244]     Train net output #0: loss = 0.134273 (* 1 = 0.134273 loss)
I0429 04:03:06.005089   966 sgd_solver.cpp:106] Iteration 35600, lr = 1e-06
I0429 04:03:55.518045   966 solver.cpp:228] Iteration 35700, loss = 0.0872939
I0429 04:03:55.518213   966 solver.cpp:244]     Train net output #0: loss = 0.0872939 (* 1 = 0.0872939 loss)
I0429 04:03:55.518221   966 sgd_solver.cpp:106] Iteration 35700, lr = 1e-06
I0429 04:04:45.050297   966 solver.cpp:228] Iteration 35800, loss = 0.0956193
I0429 04:04:45.050441   966 solver.cpp:244]     Train net output #0: loss = 0.0956193 (* 1 = 0.0956193 loss)
I0429 04:04:45.050447   966 sgd_solver.cpp:106] Iteration 35800, lr = 1e-06
I0429 04:05:36.660675   966 solver.cpp:228] Iteration 35900, loss = 0.162331
I0429 04:05:36.660836   966 solver.cpp:244]     Train net output #0: loss = 0.162331 (* 1 = 0.162331 loss)
I0429 04:05:36.660845   966 sgd_solver.cpp:106] Iteration 35900, lr = 1e-06
I0429 04:06:25.862370   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_36000.caffemodel
I0429 04:06:32.004197   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_36000.solverstate
I0429 04:06:32.198132   966 solver.cpp:337] Iteration 36000, Testing net (#0)
I0429 04:06:32.198253   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 04:06:32.198259   966 net.cpp:693] Ignoring source layer visualize
I0429 04:06:32.198261   966 net.cpp:693] Ignoring source layer fake
I0429 04:11:26.018201   966 solver.cpp:404]     Test net output #0: loss = 0.18292 (* 1 = 0.18292 loss)
I0429 04:11:26.327678   966 solver.cpp:228] Iteration 36000, loss = 0.171876
I0429 04:11:26.327718   966 solver.cpp:244]     Train net output #0: loss = 0.171876 (* 1 = 0.171876 loss)
I0429 04:11:26.327724   966 sgd_solver.cpp:106] Iteration 36000, lr = 1e-06
I0429 04:12:15.839824   966 solver.cpp:228] Iteration 36100, loss = 0.122373
I0429 04:12:15.839987   966 solver.cpp:244]     Train net output #0: loss = 0.122373 (* 1 = 0.122373 loss)
I0429 04:12:15.839993   966 sgd_solver.cpp:106] Iteration 36100, lr = 1e-06
I0429 04:13:07.253810   966 solver.cpp:228] Iteration 36200, loss = 0.185477
I0429 04:13:07.254951   966 solver.cpp:244]     Train net output #0: loss = 0.185477 (* 1 = 0.185477 loss)
I0429 04:13:07.254992   966 sgd_solver.cpp:106] Iteration 36200, lr = 1e-06
I0429 04:13:56.748270   966 solver.cpp:228] Iteration 36300, loss = 0.14524
I0429 04:13:56.748437   966 solver.cpp:244]     Train net output #0: loss = 0.14524 (* 1 = 0.14524 loss)
I0429 04:13:56.748445   966 sgd_solver.cpp:106] Iteration 36300, lr = 1e-06
I0429 04:14:46.275894   966 solver.cpp:228] Iteration 36400, loss = 0.0949798
I0429 04:14:46.276049   966 solver.cpp:244]     Train net output #0: loss = 0.0949798 (* 1 = 0.0949798 loss)
I0429 04:14:46.276056   966 sgd_solver.cpp:106] Iteration 36400, lr = 1e-06
I0429 04:15:35.800351   966 solver.cpp:228] Iteration 36500, loss = 0.105715
I0429 04:15:35.800524   966 solver.cpp:244]     Train net output #0: loss = 0.105715 (* 1 = 0.105715 loss)
I0429 04:15:35.800531   966 sgd_solver.cpp:106] Iteration 36500, lr = 1e-06
I0429 04:16:27.188592   966 solver.cpp:228] Iteration 36600, loss = 0.14797
I0429 04:16:27.188818   966 solver.cpp:244]     Train net output #0: loss = 0.14797 (* 1 = 0.14797 loss)
I0429 04:16:27.188839   966 sgd_solver.cpp:106] Iteration 36600, lr = 1e-06
I0429 04:17:16.690907   966 solver.cpp:228] Iteration 36700, loss = 0.177635
I0429 04:17:16.691067   966 solver.cpp:244]     Train net output #0: loss = 0.177635 (* 1 = 0.177635 loss)
I0429 04:17:16.691074   966 sgd_solver.cpp:106] Iteration 36700, lr = 1e-06
I0429 04:18:06.194589   966 solver.cpp:228] Iteration 36800, loss = 0.123306
I0429 04:18:06.194763   966 solver.cpp:244]     Train net output #0: loss = 0.123306 (* 1 = 0.123306 loss)
I0429 04:18:06.194772   966 sgd_solver.cpp:106] Iteration 36800, lr = 1e-06
I0429 04:18:57.486902   966 solver.cpp:228] Iteration 36900, loss = 0.136202
I0429 04:18:57.487076   966 solver.cpp:244]     Train net output #0: loss = 0.136202 (* 1 = 0.136202 loss)
I0429 04:18:57.487082   966 sgd_solver.cpp:106] Iteration 36900, lr = 1e-06
I0429 04:19:46.681306   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_37000.caffemodel
I0429 04:19:54.013478   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_37000.solverstate
I0429 04:19:54.204708   966 solver.cpp:337] Iteration 37000, Testing net (#0)
I0429 04:19:54.204809   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 04:19:54.204815   966 net.cpp:693] Ignoring source layer visualize
I0429 04:19:54.204818   966 net.cpp:693] Ignoring source layer fake
I0429 04:24:46.426514   966 solver.cpp:404]     Test net output #0: loss = 0.181735 (* 1 = 0.181735 loss)
I0429 04:24:46.735766   966 solver.cpp:228] Iteration 37000, loss = 0.159299
I0429 04:24:46.735811   966 solver.cpp:244]     Train net output #0: loss = 0.159299 (* 1 = 0.159299 loss)
I0429 04:24:46.735822   966 sgd_solver.cpp:106] Iteration 37000, lr = 1e-06
I0429 04:25:36.253834   966 solver.cpp:228] Iteration 37100, loss = 0.185041
I0429 04:25:36.254017   966 solver.cpp:244]     Train net output #0: loss = 0.185041 (* 1 = 0.185041 loss)
I0429 04:25:36.254025   966 sgd_solver.cpp:106] Iteration 37100, lr = 1e-06
I0429 04:26:27.396948   966 solver.cpp:228] Iteration 37200, loss = 0.351574
I0429 04:26:27.397109   966 solver.cpp:244]     Train net output #0: loss = 0.351574 (* 1 = 0.351574 loss)
I0429 04:26:27.397116   966 sgd_solver.cpp:106] Iteration 37200, lr = 1e-06
I0429 04:27:16.897959   966 solver.cpp:228] Iteration 37300, loss = 0.0871785
I0429 04:27:16.898118   966 solver.cpp:244]     Train net output #0: loss = 0.0871785 (* 1 = 0.0871785 loss)
I0429 04:27:16.898125   966 sgd_solver.cpp:106] Iteration 37300, lr = 1e-06
I0429 04:28:06.398874   966 solver.cpp:228] Iteration 37400, loss = 0.0807016
I0429 04:28:06.399046   966 solver.cpp:244]     Train net output #0: loss = 0.0807016 (* 1 = 0.0807016 loss)
I0429 04:28:06.399054   966 sgd_solver.cpp:106] Iteration 37400, lr = 1e-06
I0429 04:28:57.148879   966 solver.cpp:228] Iteration 37500, loss = 0.109388
I0429 04:28:57.149056   966 solver.cpp:244]     Train net output #0: loss = 0.109388 (* 1 = 0.109388 loss)
I0429 04:28:57.149063   966 sgd_solver.cpp:106] Iteration 37500, lr = 1e-07
I0429 04:29:46.659354   966 solver.cpp:228] Iteration 37600, loss = 0.128736
I0429 04:29:46.659530   966 solver.cpp:244]     Train net output #0: loss = 0.128736 (* 1 = 0.128736 loss)
I0429 04:29:46.659536   966 sgd_solver.cpp:106] Iteration 37600, lr = 1e-07
I0429 04:30:36.178048   966 solver.cpp:228] Iteration 37700, loss = 0.138843
I0429 04:30:36.178217   966 solver.cpp:244]     Train net output #0: loss = 0.138843 (* 1 = 0.138843 loss)
I0429 04:30:36.178223   966 sgd_solver.cpp:106] Iteration 37700, lr = 1e-07
I0429 04:31:27.016021   966 solver.cpp:228] Iteration 37800, loss = 0.128124
I0429 04:31:27.016176   966 solver.cpp:244]     Train net output #0: loss = 0.128124 (* 1 = 0.128124 loss)
I0429 04:31:27.016183   966 sgd_solver.cpp:106] Iteration 37800, lr = 1e-07
I0429 04:32:16.518780   966 solver.cpp:228] Iteration 37900, loss = 0.0956975
I0429 04:32:16.518939   966 solver.cpp:244]     Train net output #0: loss = 0.0956975 (* 1 = 0.0956975 loss)
I0429 04:32:16.518945   966 sgd_solver.cpp:106] Iteration 37900, lr = 1e-07
I0429 04:33:05.726646   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_38000.caffemodel
I0429 04:33:18.538527   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_38000.solverstate
I0429 04:33:18.731958   966 solver.cpp:337] Iteration 38000, Testing net (#0)
I0429 04:33:18.732100   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 04:33:18.732105   966 net.cpp:693] Ignoring source layer visualize
I0429 04:33:18.732108   966 net.cpp:693] Ignoring source layer fake
I0429 04:38:12.090414   966 solver.cpp:404]     Test net output #0: loss = 0.18027 (* 1 = 0.18027 loss)
I0429 04:38:12.401391   966 solver.cpp:228] Iteration 38000, loss = 0.165855
I0429 04:38:12.401408   966 solver.cpp:244]     Train net output #0: loss = 0.165855 (* 1 = 0.165855 loss)
I0429 04:38:12.401429   966 sgd_solver.cpp:106] Iteration 38000, lr = 1e-07
I0429 04:39:03.485816   966 solver.cpp:228] Iteration 38100, loss = 0.144836
I0429 04:39:03.485961   966 solver.cpp:244]     Train net output #0: loss = 0.144836 (* 1 = 0.144836 loss)
I0429 04:39:03.485968   966 sgd_solver.cpp:106] Iteration 38100, lr = 1e-07
I0429 04:39:52.998766   966 solver.cpp:228] Iteration 38200, loss = 0.0446466
I0429 04:39:52.998937   966 solver.cpp:244]     Train net output #0: loss = 0.0446466 (* 1 = 0.0446466 loss)
I0429 04:39:52.998944   966 sgd_solver.cpp:106] Iteration 38200, lr = 1e-07
I0429 04:40:44.256003   966 solver.cpp:228] Iteration 38300, loss = 0.133322
I0429 04:40:44.256157   966 solver.cpp:244]     Train net output #0: loss = 0.133322 (* 1 = 0.133322 loss)
I0429 04:40:44.256165   966 sgd_solver.cpp:106] Iteration 38300, lr = 1e-07
I0429 04:41:33.756307   966 solver.cpp:228] Iteration 38400, loss = 0.163494
I0429 04:41:33.756486   966 solver.cpp:244]     Train net output #0: loss = 0.163494 (* 1 = 0.163494 loss)
I0429 04:41:33.756494   966 sgd_solver.cpp:106] Iteration 38400, lr = 1e-07
I0429 04:42:23.273898   966 solver.cpp:228] Iteration 38500, loss = 0.11901
I0429 04:42:23.274060   966 solver.cpp:244]     Train net output #0: loss = 0.11901 (* 1 = 0.11901 loss)
I0429 04:42:23.274067   966 sgd_solver.cpp:106] Iteration 38500, lr = 1e-07
I0429 04:43:12.794589   966 solver.cpp:228] Iteration 38600, loss = 0.0930019
I0429 04:43:12.794744   966 solver.cpp:244]     Train net output #0: loss = 0.0930019 (* 1 = 0.0930019 loss)
I0429 04:43:12.794751   966 sgd_solver.cpp:106] Iteration 38600, lr = 1e-07
I0429 04:44:04.151844   966 solver.cpp:228] Iteration 38700, loss = 0.0777953
I0429 04:44:04.152009   966 solver.cpp:244]     Train net output #0: loss = 0.0777953 (* 1 = 0.0777953 loss)
I0429 04:44:04.152015   966 sgd_solver.cpp:106] Iteration 38700, lr = 1e-07
I0429 04:44:53.664789   966 solver.cpp:228] Iteration 38800, loss = 0.202379
I0429 04:44:53.664935   966 solver.cpp:244]     Train net output #0: loss = 0.202379 (* 1 = 0.202379 loss)
I0429 04:44:53.664942   966 sgd_solver.cpp:106] Iteration 38800, lr = 1e-07
I0429 04:45:43.183315   966 solver.cpp:228] Iteration 38900, loss = 0.133066
I0429 04:45:43.183465   966 solver.cpp:244]     Train net output #0: loss = 0.133066 (* 1 = 0.133066 loss)
I0429 04:45:43.183471   966 sgd_solver.cpp:106] Iteration 38900, lr = 1e-07
I0429 04:46:34.238234   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_39000.caffemodel
I0429 04:46:42.783138   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_39000.solverstate
I0429 04:46:42.981930   966 solver.cpp:337] Iteration 39000, Testing net (#0)
I0429 04:46:42.982064   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 04:46:42.982069   966 net.cpp:693] Ignoring source layer visualize
I0429 04:46:42.982071   966 net.cpp:693] Ignoring source layer fake
I0429 04:51:34.863613   966 solver.cpp:404]     Test net output #0: loss = 0.190606 (* 1 = 0.190606 loss)
I0429 04:51:35.174739   966 solver.cpp:228] Iteration 39000, loss = 0.314383
I0429 04:51:35.174775   966 solver.cpp:244]     Train net output #0: loss = 0.314383 (* 1 = 0.314383 loss)
I0429 04:51:35.174782   966 sgd_solver.cpp:106] Iteration 39000, lr = 1e-07
I0429 04:52:24.697326   966 solver.cpp:228] Iteration 39100, loss = 0.100781
I0429 04:52:24.697495   966 solver.cpp:244]     Train net output #0: loss = 0.100781 (* 1 = 0.100781 loss)
I0429 04:52:24.697502   966 sgd_solver.cpp:106] Iteration 39100, lr = 1e-07
I0429 04:53:14.212366   966 solver.cpp:228] Iteration 39200, loss = 0.134252
I0429 04:53:14.212529   966 solver.cpp:244]     Train net output #0: loss = 0.134252 (* 1 = 0.134252 loss)
I0429 04:53:14.212537   966 sgd_solver.cpp:106] Iteration 39200, lr = 1e-07
I0429 04:54:03.714303   966 solver.cpp:228] Iteration 39300, loss = 0.109073
I0429 04:54:03.714458   966 solver.cpp:244]     Train net output #0: loss = 0.109073 (* 1 = 0.109073 loss)
I0429 04:54:03.714465   966 sgd_solver.cpp:106] Iteration 39300, lr = 1e-07
I0429 04:54:55.038897   966 solver.cpp:228] Iteration 39400, loss = 0.0994454
I0429 04:54:55.039100   966 solver.cpp:244]     Train net output #0: loss = 0.0994454 (* 1 = 0.0994454 loss)
I0429 04:54:55.039110   966 sgd_solver.cpp:106] Iteration 39400, lr = 1e-07
I0429 04:55:44.528303   966 solver.cpp:228] Iteration 39500, loss = 0.150986
I0429 04:55:44.528468   966 solver.cpp:244]     Train net output #0: loss = 0.150986 (* 1 = 0.150986 loss)
I0429 04:55:44.528475   966 sgd_solver.cpp:106] Iteration 39500, lr = 1e-07
I0429 04:56:34.046665   966 solver.cpp:228] Iteration 39600, loss = 0.131676
I0429 04:56:34.046836   966 solver.cpp:244]     Train net output #0: loss = 0.131676 (* 1 = 0.131676 loss)
I0429 04:56:34.046844   966 sgd_solver.cpp:106] Iteration 39600, lr = 1e-07
I0429 04:57:25.416965   966 solver.cpp:228] Iteration 39700, loss = 0.19141
I0429 04:57:25.417326   966 solver.cpp:244]     Train net output #0: loss = 0.19141 (* 1 = 0.19141 loss)
I0429 04:57:25.417335   966 sgd_solver.cpp:106] Iteration 39700, lr = 1e-07
I0429 04:58:14.907075   966 solver.cpp:228] Iteration 39800, loss = 0.141872
I0429 04:58:14.907253   966 solver.cpp:244]     Train net output #0: loss = 0.141872 (* 1 = 0.141872 loss)
I0429 04:58:14.907274   966 sgd_solver.cpp:106] Iteration 39800, lr = 1e-07
I0429 04:59:04.406568   966 solver.cpp:228] Iteration 39900, loss = 0.138068
I0429 04:59:04.406735   966 solver.cpp:244]     Train net output #0: loss = 0.138068 (* 1 = 0.138068 loss)
I0429 04:59:04.406743   966 sgd_solver.cpp:106] Iteration 39900, lr = 1e-07
I0429 04:59:55.383123   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_40000.caffemodel
I0429 05:00:06.284817   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_40000.solverstate
I0429 05:00:06.490082   966 solver.cpp:337] Iteration 40000, Testing net (#0)
I0429 05:00:06.490206   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 05:00:06.490211   966 net.cpp:693] Ignoring source layer visualize
I0429 05:00:06.490229   966 net.cpp:693] Ignoring source layer fake
I0429 05:04:58.501945   966 solver.cpp:404]     Test net output #0: loss = 0.187738 (* 1 = 0.187738 loss)
I0429 05:04:58.810734   966 solver.cpp:228] Iteration 40000, loss = 0.12396
I0429 05:04:58.810781   966 solver.cpp:244]     Train net output #0: loss = 0.12396 (* 1 = 0.12396 loss)
I0429 05:04:58.810789   966 sgd_solver.cpp:106] Iteration 40000, lr = 1e-07
I0429 05:05:48.296391   966 solver.cpp:228] Iteration 40100, loss = 0.113864
I0429 05:05:48.297994   966 solver.cpp:244]     Train net output #0: loss = 0.113864 (* 1 = 0.113864 loss)
I0429 05:05:48.298009   966 sgd_solver.cpp:106] Iteration 40100, lr = 1e-07
I0429 05:06:37.804689   966 solver.cpp:228] Iteration 40200, loss = 0.112049
I0429 05:06:37.804872   966 solver.cpp:244]     Train net output #0: loss = 0.112049 (* 1 = 0.112049 loss)
I0429 05:06:37.804878   966 sgd_solver.cpp:106] Iteration 40200, lr = 1e-07
I0429 05:07:27.288923   966 solver.cpp:228] Iteration 40300, loss = 0.0928465
I0429 05:07:27.289078   966 solver.cpp:244]     Train net output #0: loss = 0.0928465 (* 1 = 0.0928465 loss)
I0429 05:07:27.289086   966 sgd_solver.cpp:106] Iteration 40300, lr = 1e-07
I0429 05:08:18.439254   966 solver.cpp:228] Iteration 40400, loss = 0.066009
I0429 05:08:18.439419   966 solver.cpp:244]     Train net output #0: loss = 0.066009 (* 1 = 0.066009 loss)
I0429 05:08:18.439425   966 sgd_solver.cpp:106] Iteration 40400, lr = 1e-07
I0429 05:09:07.931721   966 solver.cpp:228] Iteration 40500, loss = 0.404434
I0429 05:09:07.931885   966 solver.cpp:244]     Train net output #0: loss = 0.404434 (* 1 = 0.404434 loss)
I0429 05:09:07.931890   966 sgd_solver.cpp:106] Iteration 40500, lr = 1e-07
I0429 05:09:57.417933   966 solver.cpp:228] Iteration 40600, loss = 0.122948
I0429 05:09:57.418098   966 solver.cpp:244]     Train net output #0: loss = 0.122948 (* 1 = 0.122948 loss)
I0429 05:09:57.418105   966 sgd_solver.cpp:106] Iteration 40600, lr = 1e-07
I0429 05:10:48.194737   966 solver.cpp:228] Iteration 40700, loss = 0.13102
I0429 05:10:48.194905   966 solver.cpp:244]     Train net output #0: loss = 0.13102 (* 1 = 0.13102 loss)
I0429 05:10:48.194911   966 sgd_solver.cpp:106] Iteration 40700, lr = 1e-07
I0429 05:11:37.710726   966 solver.cpp:228] Iteration 40800, loss = 0.152592
I0429 05:11:37.710881   966 solver.cpp:244]     Train net output #0: loss = 0.152592 (* 1 = 0.152592 loss)
I0429 05:11:37.710887   966 sgd_solver.cpp:106] Iteration 40800, lr = 1e-07
I0429 05:12:28.558642   966 solver.cpp:228] Iteration 40900, loss = 0.328162
I0429 05:12:28.558809   966 solver.cpp:244]     Train net output #0: loss = 0.328162 (* 1 = 0.328162 loss)
I0429 05:12:28.558815   966 sgd_solver.cpp:106] Iteration 40900, lr = 1e-07
I0429 05:13:17.758193   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_41000.caffemodel
I0429 05:13:29.685144   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_41000.solverstate
I0429 05:13:29.888171   966 solver.cpp:337] Iteration 41000, Testing net (#0)
I0429 05:13:29.888324   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 05:13:29.888329   966 net.cpp:693] Ignoring source layer visualize
I0429 05:13:29.888332   966 net.cpp:693] Ignoring source layer fake
I0429 05:18:21.803381   966 solver.cpp:404]     Test net output #0: loss = 0.181861 (* 1 = 0.181861 loss)
I0429 05:18:22.112223   966 solver.cpp:228] Iteration 41000, loss = 0.328942
I0429 05:18:22.112244   966 solver.cpp:244]     Train net output #0: loss = 0.328942 (* 1 = 0.328942 loss)
I0429 05:18:22.112267   966 sgd_solver.cpp:106] Iteration 41000, lr = 1e-07
I0429 05:19:11.637506   966 solver.cpp:228] Iteration 41100, loss = 0.220069
I0429 05:19:11.637658   966 solver.cpp:244]     Train net output #0: loss = 0.220069 (* 1 = 0.220069 loss)
I0429 05:19:11.637665   966 sgd_solver.cpp:106] Iteration 41100, lr = 1e-07
I0429 05:20:02.742409   966 solver.cpp:228] Iteration 41200, loss = 0.0789141
I0429 05:20:02.742575   966 solver.cpp:244]     Train net output #0: loss = 0.0789141 (* 1 = 0.0789141 loss)
I0429 05:20:02.742583   966 sgd_solver.cpp:106] Iteration 41200, lr = 1e-07
I0429 05:20:52.251112   966 solver.cpp:228] Iteration 41300, loss = 0.102099
I0429 05:20:52.251271   966 solver.cpp:244]     Train net output #0: loss = 0.102099 (* 1 = 0.102099 loss)
I0429 05:20:52.251276   966 sgd_solver.cpp:106] Iteration 41300, lr = 1e-07
I0429 05:21:41.752210   966 solver.cpp:228] Iteration 41400, loss = 0.205827
I0429 05:21:41.752385   966 solver.cpp:244]     Train net output #0: loss = 0.205827 (* 1 = 0.205827 loss)
I0429 05:21:41.752394   966 sgd_solver.cpp:106] Iteration 41400, lr = 1e-07
I0429 05:22:33.054474   966 solver.cpp:228] Iteration 41500, loss = 0.180042
I0429 05:22:33.054636   966 solver.cpp:244]     Train net output #0: loss = 0.180042 (* 1 = 0.180042 loss)
I0429 05:22:33.054643   966 sgd_solver.cpp:106] Iteration 41500, lr = 1e-07
I0429 05:23:22.556085   966 solver.cpp:228] Iteration 41600, loss = 0.198926
I0429 05:23:22.556255   966 solver.cpp:244]     Train net output #0: loss = 0.198926 (* 1 = 0.198926 loss)
I0429 05:23:22.556262   966 sgd_solver.cpp:106] Iteration 41600, lr = 1e-07
I0429 05:24:12.076526   966 solver.cpp:228] Iteration 41700, loss = 0.133531
I0429 05:24:12.080955   966 solver.cpp:244]     Train net output #0: loss = 0.133531 (* 1 = 0.133531 loss)
I0429 05:24:12.080961   966 sgd_solver.cpp:106] Iteration 41700, lr = 1e-07
I0429 05:25:03.447198   966 solver.cpp:228] Iteration 41800, loss = 0.11852
I0429 05:25:03.447368   966 solver.cpp:244]     Train net output #0: loss = 0.11852 (* 1 = 0.11852 loss)
I0429 05:25:03.447376   966 sgd_solver.cpp:106] Iteration 41800, lr = 1e-07
I0429 05:25:52.961195   966 solver.cpp:228] Iteration 41900, loss = 0.0826668
I0429 05:25:52.961349   966 solver.cpp:244]     Train net output #0: loss = 0.0826668 (* 1 = 0.0826668 loss)
I0429 05:25:52.961354   966 sgd_solver.cpp:106] Iteration 41900, lr = 1e-07
I0429 05:26:42.172279   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_42000.caffemodel
I0429 05:26:55.593684   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_42000.solverstate
I0429 05:26:55.791750   966 solver.cpp:337] Iteration 42000, Testing net (#0)
I0429 05:26:55.791888   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 05:26:55.791894   966 net.cpp:693] Ignoring source layer visualize
I0429 05:26:55.791898   966 net.cpp:693] Ignoring source layer fake
I0429 05:31:48.291949   966 solver.cpp:404]     Test net output #0: loss = 0.180691 (* 1 = 0.180691 loss)
I0429 05:31:48.603775   966 solver.cpp:228] Iteration 42000, loss = 0.120298
I0429 05:31:48.603793   966 solver.cpp:244]     Train net output #0: loss = 0.120298 (* 1 = 0.120298 loss)
I0429 05:31:48.603816   966 sgd_solver.cpp:106] Iteration 42000, lr = 1e-07
I0429 05:32:40.042048   966 solver.cpp:228] Iteration 42100, loss = 0.151203
I0429 05:32:40.042196   966 solver.cpp:244]     Train net output #0: loss = 0.151203 (* 1 = 0.151203 loss)
I0429 05:32:40.042202   966 sgd_solver.cpp:106] Iteration 42100, lr = 1e-07
I0429 05:33:29.568137   966 solver.cpp:228] Iteration 42200, loss = 0.209814
I0429 05:33:29.568352   966 solver.cpp:244]     Train net output #0: loss = 0.209814 (* 1 = 0.209814 loss)
I0429 05:33:29.568367   966 sgd_solver.cpp:106] Iteration 42200, lr = 1e-07
I0429 05:34:19.107421   966 solver.cpp:228] Iteration 42300, loss = 0.110135
I0429 05:34:19.107592   966 solver.cpp:244]     Train net output #0: loss = 0.110135 (* 1 = 0.110135 loss)
I0429 05:34:19.107599   966 sgd_solver.cpp:106] Iteration 42300, lr = 1e-07
I0429 05:35:08.618793   966 solver.cpp:228] Iteration 42400, loss = 0.171808
I0429 05:35:08.618952   966 solver.cpp:244]     Train net output #0: loss = 0.171808 (* 1 = 0.171808 loss)
I0429 05:35:08.618958   966 sgd_solver.cpp:106] Iteration 42400, lr = 1e-07
I0429 05:35:59.998266   966 solver.cpp:228] Iteration 42500, loss = 0.113767
I0429 05:35:59.998437   966 solver.cpp:244]     Train net output #0: loss = 0.113767 (* 1 = 0.113767 loss)
I0429 05:35:59.998445   966 sgd_solver.cpp:106] Iteration 42500, lr = 1e-07
I0429 05:36:49.510845   966 solver.cpp:228] Iteration 42600, loss = 0.159634
I0429 05:36:49.511018   966 solver.cpp:244]     Train net output #0: loss = 0.159634 (* 1 = 0.159634 loss)
I0429 05:36:49.511024   966 sgd_solver.cpp:106] Iteration 42600, lr = 1e-07
I0429 05:37:39.017225   966 solver.cpp:228] Iteration 42700, loss = 0.145431
I0429 05:37:39.017371   966 solver.cpp:244]     Train net output #0: loss = 0.145431 (* 1 = 0.145431 loss)
I0429 05:37:39.017377   966 sgd_solver.cpp:106] Iteration 42700, lr = 1e-07
I0429 05:38:30.301028   966 solver.cpp:228] Iteration 42800, loss = 0.137863
I0429 05:38:30.303597   966 solver.cpp:244]     Train net output #0: loss = 0.137863 (* 1 = 0.137863 loss)
I0429 05:38:30.303602   966 sgd_solver.cpp:106] Iteration 42800, lr = 1e-07
I0429 05:39:19.789947   966 solver.cpp:228] Iteration 42900, loss = 0.100702
I0429 05:39:19.790103   966 solver.cpp:244]     Train net output #0: loss = 0.100702 (* 1 = 0.100702 loss)
I0429 05:39:19.790110   966 sgd_solver.cpp:106] Iteration 42900, lr = 1e-07
I0429 05:40:08.975208   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_43000.caffemodel
I0429 05:40:25.855345   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_43000.solverstate
I0429 05:40:26.050153   966 solver.cpp:337] Iteration 43000, Testing net (#0)
I0429 05:40:26.050276   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 05:40:26.050282   966 net.cpp:693] Ignoring source layer visualize
I0429 05:40:26.050284   966 net.cpp:693] Ignoring source layer fake
I0429 05:45:17.986615   966 solver.cpp:404]     Test net output #0: loss = 0.180689 (* 1 = 0.180689 loss)
I0429 05:45:18.298029   966 solver.cpp:228] Iteration 43000, loss = 0.107954
I0429 05:45:18.298055   966 solver.cpp:244]     Train net output #0: loss = 0.107954 (* 1 = 0.107954 loss)
I0429 05:45:18.298077   966 sgd_solver.cpp:106] Iteration 43000, lr = 1e-07
I0429 05:46:07.929401   966 solver.cpp:228] Iteration 43100, loss = 0.103292
I0429 05:46:07.929545   966 solver.cpp:244]     Train net output #0: loss = 0.103292 (* 1 = 0.103292 loss)
I0429 05:46:07.929553   966 sgd_solver.cpp:106] Iteration 43100, lr = 1e-07
I0429 05:46:59.351840   966 solver.cpp:228] Iteration 43200, loss = 0.0887012
I0429 05:46:59.354554   966 solver.cpp:244]     Train net output #0: loss = 0.0887012 (* 1 = 0.0887012 loss)
I0429 05:46:59.354579   966 sgd_solver.cpp:106] Iteration 43200, lr = 1e-07
I0429 05:47:48.991880   966 solver.cpp:228] Iteration 43300, loss = 0.0314195
I0429 05:47:48.992064   966 solver.cpp:244]     Train net output #0: loss = 0.0314195 (* 1 = 0.0314195 loss)
I0429 05:47:48.992072   966 sgd_solver.cpp:106] Iteration 43300, lr = 1e-07
I0429 05:48:38.622392   966 solver.cpp:228] Iteration 43400, loss = 0.147718
I0429 05:48:38.622558   966 solver.cpp:244]     Train net output #0: loss = 0.147718 (* 1 = 0.147718 loss)
I0429 05:48:38.622565   966 sgd_solver.cpp:106] Iteration 43400, lr = 1e-07
I0429 05:49:29.897481   966 solver.cpp:228] Iteration 43500, loss = 0.0728288
I0429 05:49:29.897650   966 solver.cpp:244]     Train net output #0: loss = 0.0728288 (* 1 = 0.0728288 loss)
I0429 05:49:29.897656   966 sgd_solver.cpp:106] Iteration 43500, lr = 1e-07
I0429 05:50:19.522251   966 solver.cpp:228] Iteration 43600, loss = 0.11878
I0429 05:50:19.522410   966 solver.cpp:244]     Train net output #0: loss = 0.11878 (* 1 = 0.11878 loss)
I0429 05:50:19.522418   966 sgd_solver.cpp:106] Iteration 43600, lr = 1e-07
I0429 05:51:09.141862   966 solver.cpp:228] Iteration 43700, loss = 0.415152
I0429 05:51:09.142022   966 solver.cpp:244]     Train net output #0: loss = 0.415152 (* 1 = 0.415152 loss)
I0429 05:51:09.142029   966 sgd_solver.cpp:106] Iteration 43700, lr = 1e-07
I0429 05:52:00.035688   966 solver.cpp:228] Iteration 43800, loss = 0.169063
I0429 05:52:00.035919   966 solver.cpp:244]     Train net output #0: loss = 0.169063 (* 1 = 0.169063 loss)
I0429 05:52:00.035936   966 sgd_solver.cpp:106] Iteration 43800, lr = 1e-07
I0429 05:52:49.660326   966 solver.cpp:228] Iteration 43900, loss = 0.0944669
I0429 05:52:49.661775   966 solver.cpp:244]     Train net output #0: loss = 0.0944669 (* 1 = 0.0944669 loss)
I0429 05:52:49.661789   966 sgd_solver.cpp:106] Iteration 43900, lr = 1e-07
I0429 05:53:40.328220   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_44000.caffemodel
I0429 05:54:00.963604   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_44000.solverstate
I0429 05:54:01.164901   966 solver.cpp:337] Iteration 44000, Testing net (#0)
I0429 05:54:01.165024   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 05:54:01.165030   966 net.cpp:693] Ignoring source layer visualize
I0429 05:54:01.165031   966 net.cpp:693] Ignoring source layer fake
I0429 05:58:53.318778   966 solver.cpp:404]     Test net output #0: loss = 0.192 (* 1 = 0.192 loss)
I0429 05:58:53.627575   966 solver.cpp:228] Iteration 44000, loss = 0.121482
I0429 05:58:53.627596   966 solver.cpp:244]     Train net output #0: loss = 0.121482 (* 1 = 0.121482 loss)
I0429 05:58:53.627619   966 sgd_solver.cpp:106] Iteration 44000, lr = 1e-07
I0429 05:59:43.154633   966 solver.cpp:228] Iteration 44100, loss = 0.0883108
I0429 05:59:43.154785   966 solver.cpp:244]     Train net output #0: loss = 0.0883108 (* 1 = 0.0883108 loss)
I0429 05:59:43.154793   966 sgd_solver.cpp:106] Iteration 44100, lr = 1e-07
I0429 06:00:32.671057   966 solver.cpp:228] Iteration 44200, loss = 0.0656606
I0429 06:00:32.671211   966 solver.cpp:244]     Train net output #0: loss = 0.0656606 (* 1 = 0.0656606 loss)
I0429 06:00:32.671218   966 sgd_solver.cpp:106] Iteration 44200, lr = 1e-07
I0429 06:01:23.841289   966 solver.cpp:228] Iteration 44300, loss = 0.144032
I0429 06:01:23.841486   966 solver.cpp:244]     Train net output #0: loss = 0.144032 (* 1 = 0.144032 loss)
I0429 06:01:23.841495   966 sgd_solver.cpp:106] Iteration 44300, lr = 1e-07
I0429 06:02:13.352944   966 solver.cpp:228] Iteration 44400, loss = 0.0931141
I0429 06:02:13.353116   966 solver.cpp:244]     Train net output #0: loss = 0.0931141 (* 1 = 0.0931141 loss)
I0429 06:02:13.353122   966 sgd_solver.cpp:106] Iteration 44400, lr = 1e-07
I0429 06:03:02.849491   966 solver.cpp:228] Iteration 44500, loss = 0.170232
I0429 06:03:02.849702   966 solver.cpp:244]     Train net output #0: loss = 0.170232 (* 1 = 0.170232 loss)
I0429 06:03:02.849711   966 sgd_solver.cpp:106] Iteration 44500, lr = 1e-07
I0429 06:03:54.088126   966 solver.cpp:228] Iteration 44600, loss = 0.252267
I0429 06:03:54.088294   966 solver.cpp:244]     Train net output #0: loss = 0.252267 (* 1 = 0.252267 loss)
I0429 06:03:54.088300   966 sgd_solver.cpp:106] Iteration 44600, lr = 1e-07
I0429 06:04:43.598565   966 solver.cpp:228] Iteration 44700, loss = 0.196471
I0429 06:04:43.598737   966 solver.cpp:244]     Train net output #0: loss = 0.196471 (* 1 = 0.196471 loss)
I0429 06:04:43.598743   966 sgd_solver.cpp:106] Iteration 44700, lr = 1e-07
I0429 06:05:33.109491   966 solver.cpp:228] Iteration 44800, loss = 0.124463
I0429 06:05:33.109658   966 solver.cpp:244]     Train net output #0: loss = 0.124463 (* 1 = 0.124463 loss)
I0429 06:05:33.109664   966 sgd_solver.cpp:106] Iteration 44800, lr = 1e-07
I0429 06:06:24.468283   966 solver.cpp:228] Iteration 44900, loss = 0.0964287
I0429 06:06:24.468422   966 solver.cpp:244]     Train net output #0: loss = 0.0964287 (* 1 = 0.0964287 loss)
I0429 06:06:24.468430   966 sgd_solver.cpp:106] Iteration 44900, lr = 1e-07
I0429 06:07:13.665474   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_45000.caffemodel
I0429 06:08:18.304930   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_45000.solverstate
I0429 06:08:18.505863   966 solver.cpp:337] Iteration 45000, Testing net (#0)
I0429 06:08:18.505983   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 06:08:18.505988   966 net.cpp:693] Ignoring source layer visualize
I0429 06:08:18.505991   966 net.cpp:693] Ignoring source layer fake
I0429 06:13:10.348713   966 solver.cpp:404]     Test net output #0: loss = 0.186008 (* 1 = 0.186008 loss)
I0429 06:13:10.658103   966 solver.cpp:228] Iteration 45000, loss = 0.144188
I0429 06:13:10.658138   966 solver.cpp:244]     Train net output #0: loss = 0.144188 (* 1 = 0.144188 loss)
I0429 06:13:10.658145   966 sgd_solver.cpp:106] Iteration 45000, lr = 1e-08
I0429 06:14:00.176391   966 solver.cpp:228] Iteration 45100, loss = 0.169755
I0429 06:14:00.176544   966 solver.cpp:244]     Train net output #0: loss = 0.169755 (* 1 = 0.169755 loss)
I0429 06:14:00.176551   966 sgd_solver.cpp:106] Iteration 45100, lr = 1e-08
I0429 06:14:49.707911   966 solver.cpp:228] Iteration 45200, loss = 0.168346
I0429 06:14:49.708070   966 solver.cpp:244]     Train net output #0: loss = 0.168346 (* 1 = 0.168346 loss)
I0429 06:14:49.708076   966 sgd_solver.cpp:106] Iteration 45200, lr = 1e-08
I0429 06:15:41.520041   966 solver.cpp:228] Iteration 45300, loss = 0.13248
I0429 06:15:41.520205   966 solver.cpp:244]     Train net output #0: loss = 0.13248 (* 1 = 0.13248 loss)
I0429 06:15:41.520211   966 sgd_solver.cpp:106] Iteration 45300, lr = 1e-08
I0429 06:16:31.013478   966 solver.cpp:228] Iteration 45400, loss = 0.0829847
I0429 06:16:31.013664   966 solver.cpp:244]     Train net output #0: loss = 0.0829847 (* 1 = 0.0829847 loss)
I0429 06:16:31.013672   966 sgd_solver.cpp:106] Iteration 45400, lr = 1e-08
I0429 06:17:20.534268   966 solver.cpp:228] Iteration 45500, loss = 0.111476
I0429 06:17:20.534445   966 solver.cpp:244]     Train net output #0: loss = 0.111476 (* 1 = 0.111476 loss)
I0429 06:17:20.534451   966 sgd_solver.cpp:106] Iteration 45500, lr = 1e-08
I0429 06:18:11.903553   966 solver.cpp:228] Iteration 45600, loss = 0.0967958
I0429 06:18:11.903708   966 solver.cpp:244]     Train net output #0: loss = 0.0967958 (* 1 = 0.0967958 loss)
I0429 06:18:11.903715   966 sgd_solver.cpp:106] Iteration 45600, lr = 1e-08
I0429 06:19:01.415426   966 solver.cpp:228] Iteration 45700, loss = 0.0764671
I0429 06:19:01.415597   966 solver.cpp:244]     Train net output #0: loss = 0.0764671 (* 1 = 0.0764671 loss)
I0429 06:19:01.415604   966 sgd_solver.cpp:106] Iteration 45700, lr = 1e-08
I0429 06:19:50.934909   966 solver.cpp:228] Iteration 45800, loss = 0.117209
I0429 06:19:50.935091   966 solver.cpp:244]     Train net output #0: loss = 0.117209 (* 1 = 0.117209 loss)
I0429 06:19:50.935098   966 sgd_solver.cpp:106] Iteration 45800, lr = 1e-08
I0429 06:20:42.248834   966 solver.cpp:228] Iteration 45900, loss = 0.11486
I0429 06:20:42.248986   966 solver.cpp:244]     Train net output #0: loss = 0.11486 (* 1 = 0.11486 loss)
I0429 06:20:42.248991   966 sgd_solver.cpp:106] Iteration 45900, lr = 1e-08
I0429 06:21:31.435576   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_46000.caffemodel
I0429 06:21:51.733860   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_46000.solverstate
I0429 06:21:51.980283   966 solver.cpp:337] Iteration 46000, Testing net (#0)
I0429 06:21:51.980412   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 06:21:51.980437   966 net.cpp:693] Ignoring source layer visualize
I0429 06:21:51.980439   966 net.cpp:693] Ignoring source layer fake
I0429 06:26:43.919764   966 solver.cpp:404]     Test net output #0: loss = 0.182049 (* 1 = 0.182049 loss)
I0429 06:26:44.229571   966 solver.cpp:228] Iteration 46000, loss = 0.108035
I0429 06:26:44.229609   966 solver.cpp:244]     Train net output #0: loss = 0.108035 (* 1 = 0.108035 loss)
I0429 06:26:44.229615   966 sgd_solver.cpp:106] Iteration 46000, lr = 1e-08
I0429 06:27:33.860456   966 solver.cpp:228] Iteration 46100, loss = 0.104762
I0429 06:27:33.860615   966 solver.cpp:244]     Train net output #0: loss = 0.104762 (* 1 = 0.104762 loss)
I0429 06:27:33.860622   966 sgd_solver.cpp:106] Iteration 46100, lr = 1e-08
I0429 06:28:23.482018   966 solver.cpp:228] Iteration 46200, loss = 0.150628
I0429 06:28:23.482182   966 solver.cpp:244]     Train net output #0: loss = 0.150628 (* 1 = 0.150628 loss)
I0429 06:28:23.482189   966 sgd_solver.cpp:106] Iteration 46200, lr = 1e-08
I0429 06:29:14.912695   966 solver.cpp:228] Iteration 46300, loss = 0.143233
I0429 06:29:14.912839   966 solver.cpp:244]     Train net output #0: loss = 0.143233 (* 1 = 0.143233 loss)
I0429 06:29:14.912847   966 sgd_solver.cpp:106] Iteration 46300, lr = 1e-08
I0429 06:30:04.533835   966 solver.cpp:228] Iteration 46400, loss = 0.0476327
I0429 06:30:04.533989   966 solver.cpp:244]     Train net output #0: loss = 0.0476327 (* 1 = 0.0476327 loss)
I0429 06:30:04.533996   966 sgd_solver.cpp:106] Iteration 46400, lr = 1e-08
I0429 06:30:54.157274   966 solver.cpp:228] Iteration 46500, loss = 0.0877298
I0429 06:30:54.157449   966 solver.cpp:244]     Train net output #0: loss = 0.0877298 (* 1 = 0.0877298 loss)
I0429 06:30:54.157457   966 sgd_solver.cpp:106] Iteration 46500, lr = 1e-08
I0429 06:31:45.453248   966 solver.cpp:228] Iteration 46600, loss = 0.31515
I0429 06:31:45.453418   966 solver.cpp:244]     Train net output #0: loss = 0.31515 (* 1 = 0.31515 loss)
I0429 06:31:45.453424   966 sgd_solver.cpp:106] Iteration 46600, lr = 1e-08
I0429 06:32:35.078904   966 solver.cpp:228] Iteration 46700, loss = 0.0831815
I0429 06:32:35.079075   966 solver.cpp:244]     Train net output #0: loss = 0.0831815 (* 1 = 0.0831815 loss)
I0429 06:32:35.079082   966 sgd_solver.cpp:106] Iteration 46700, lr = 1e-08
I0429 06:33:24.700800   966 solver.cpp:228] Iteration 46800, loss = 0.123923
I0429 06:33:24.700966   966 solver.cpp:244]     Train net output #0: loss = 0.123923 (* 1 = 0.123923 loss)
I0429 06:33:24.700973   966 sgd_solver.cpp:106] Iteration 46800, lr = 1e-08
I0429 06:34:15.632194   966 solver.cpp:228] Iteration 46900, loss = 0.104324
I0429 06:34:15.632350   966 solver.cpp:244]     Train net output #0: loss = 0.104324 (* 1 = 0.104324 loss)
I0429 06:34:15.632357   966 sgd_solver.cpp:106] Iteration 46900, lr = 1e-08
I0429 06:35:04.948397   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_47000.caffemodel
I0429 06:35:16.600790   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_47000.solverstate
I0429 06:35:16.789234   966 solver.cpp:337] Iteration 47000, Testing net (#0)
I0429 06:35:16.789366   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 06:35:16.789387   966 net.cpp:693] Ignoring source layer visualize
I0429 06:35:16.789389   966 net.cpp:693] Ignoring source layer fake
I0429 06:40:08.779064   966 solver.cpp:404]     Test net output #0: loss = 0.180164 (* 1 = 0.180164 loss)
I0429 06:40:09.088824   966 solver.cpp:228] Iteration 47000, loss = 0.129538
I0429 06:40:09.088862   966 solver.cpp:244]     Train net output #0: loss = 0.129538 (* 1 = 0.129538 loss)
I0429 06:40:09.088868   966 sgd_solver.cpp:106] Iteration 47000, lr = 1e-08
I0429 06:40:58.723289   966 solver.cpp:228] Iteration 47100, loss = 0.131871
I0429 06:40:58.723444   966 solver.cpp:244]     Train net output #0: loss = 0.131871 (* 1 = 0.131871 loss)
I0429 06:40:58.723455   966 sgd_solver.cpp:106] Iteration 47100, lr = 1e-08
I0429 06:41:49.681939   966 solver.cpp:228] Iteration 47200, loss = 0.130682
I0429 06:41:49.682090   966 solver.cpp:244]     Train net output #0: loss = 0.130682 (* 1 = 0.130682 loss)
I0429 06:41:49.682097   966 sgd_solver.cpp:106] Iteration 47200, lr = 1e-08
I0429 06:42:39.341214   966 solver.cpp:228] Iteration 47300, loss = 0.127424
I0429 06:42:39.341574   966 solver.cpp:244]     Train net output #0: loss = 0.127424 (* 1 = 0.127424 loss)
I0429 06:42:39.341589   966 sgd_solver.cpp:106] Iteration 47300, lr = 1e-08
I0429 06:43:30.517395   966 solver.cpp:228] Iteration 47400, loss = 0.0577986
I0429 06:43:30.517563   966 solver.cpp:244]     Train net output #0: loss = 0.0577986 (* 1 = 0.0577986 loss)
I0429 06:43:30.517570   966 sgd_solver.cpp:106] Iteration 47400, lr = 1e-08
I0429 06:44:20.143357   966 solver.cpp:228] Iteration 47500, loss = 0.156636
I0429 06:44:20.143519   966 solver.cpp:244]     Train net output #0: loss = 0.156636 (* 1 = 0.156636 loss)
I0429 06:44:20.143527   966 sgd_solver.cpp:106] Iteration 47500, lr = 1e-08
I0429 06:45:09.785676   966 solver.cpp:228] Iteration 47600, loss = 0.0755473
I0429 06:45:09.785837   966 solver.cpp:244]     Train net output #0: loss = 0.0755473 (* 1 = 0.0755473 loss)
I0429 06:45:09.785845   966 sgd_solver.cpp:106] Iteration 47600, lr = 1e-08
I0429 06:46:01.184653   966 solver.cpp:228] Iteration 47700, loss = 0.128596
I0429 06:46:01.184871   966 solver.cpp:244]     Train net output #0: loss = 0.128596 (* 1 = 0.128596 loss)
I0429 06:46:01.184880   966 sgd_solver.cpp:106] Iteration 47700, lr = 1e-08
I0429 06:46:50.814910   966 solver.cpp:228] Iteration 47800, loss = 0.160737
I0429 06:46:50.815074   966 solver.cpp:244]     Train net output #0: loss = 0.160737 (* 1 = 0.160737 loss)
I0429 06:46:50.815081   966 sgd_solver.cpp:106] Iteration 47800, lr = 1e-08
I0429 06:47:40.444108   966 solver.cpp:228] Iteration 47900, loss = 0.24221
I0429 06:47:40.444281   966 solver.cpp:244]     Train net output #0: loss = 0.24221 (* 1 = 0.24221 loss)
I0429 06:47:40.444288   966 sgd_solver.cpp:106] Iteration 47900, lr = 1e-08
I0429 06:48:31.610405   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_48000.caffemodel
I0429 06:48:34.321723   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_48000.solverstate
I0429 06:48:34.512596   966 solver.cpp:337] Iteration 48000, Testing net (#0)
I0429 06:48:34.512718   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 06:48:34.512723   966 net.cpp:693] Ignoring source layer visualize
I0429 06:48:34.512725   966 net.cpp:693] Ignoring source layer fake
I0429 06:53:26.502060   966 solver.cpp:404]     Test net output #0: loss = 0.181514 (* 1 = 0.181514 loss)
I0429 06:53:26.814046   966 solver.cpp:228] Iteration 48000, loss = 0.147113
I0429 06:53:26.814086   966 solver.cpp:244]     Train net output #0: loss = 0.147113 (* 1 = 0.147113 loss)
I0429 06:53:26.814092   966 sgd_solver.cpp:106] Iteration 48000, lr = 1e-08
I0429 06:54:16.456197   966 solver.cpp:228] Iteration 48100, loss = 0.0938196
I0429 06:54:16.456377   966 solver.cpp:244]     Train net output #0: loss = 0.0938196 (* 1 = 0.0938196 loss)
I0429 06:54:16.456384   966 sgd_solver.cpp:106] Iteration 48100, lr = 1e-08
I0429 06:55:06.089933   966 solver.cpp:228] Iteration 48200, loss = 0.179014
I0429 06:55:06.090090   966 solver.cpp:244]     Train net output #0: loss = 0.179014 (* 1 = 0.179014 loss)
I0429 06:55:06.090097   966 sgd_solver.cpp:106] Iteration 48200, lr = 1e-08
I0429 06:55:55.725253   966 solver.cpp:228] Iteration 48300, loss = 0.140469
I0429 06:55:55.725419   966 solver.cpp:244]     Train net output #0: loss = 0.140469 (* 1 = 0.140469 loss)
I0429 06:55:55.725427   966 sgd_solver.cpp:106] Iteration 48300, lr = 1e-08
I0429 06:56:47.239946   966 solver.cpp:228] Iteration 48400, loss = 0.236582
I0429 06:56:47.240298   966 solver.cpp:244]     Train net output #0: loss = 0.236582 (* 1 = 0.236582 loss)
I0429 06:56:47.240306   966 sgd_solver.cpp:106] Iteration 48400, lr = 1e-08
I0429 06:57:36.863765   966 solver.cpp:228] Iteration 48500, loss = 0.15579
I0429 06:57:36.863906   966 solver.cpp:244]     Train net output #0: loss = 0.15579 (* 1 = 0.15579 loss)
I0429 06:57:36.863914   966 sgd_solver.cpp:106] Iteration 48500, lr = 1e-08
I0429 06:58:26.496429   966 solver.cpp:228] Iteration 48600, loss = 0.11079
I0429 06:58:26.496599   966 solver.cpp:244]     Train net output #0: loss = 0.11079 (* 1 = 0.11079 loss)
I0429 06:58:26.496605   966 sgd_solver.cpp:106] Iteration 48600, lr = 1e-08
I0429 06:59:17.965522   966 solver.cpp:228] Iteration 48700, loss = 0.138674
I0429 06:59:17.968299   966 solver.cpp:244]     Train net output #0: loss = 0.138674 (* 1 = 0.138674 loss)
I0429 06:59:17.968305   966 sgd_solver.cpp:106] Iteration 48700, lr = 1e-08
I0429 07:00:07.596595   966 solver.cpp:228] Iteration 48800, loss = 0.168397
I0429 07:00:07.596770   966 solver.cpp:244]     Train net output #0: loss = 0.168397 (* 1 = 0.168397 loss)
I0429 07:00:07.596776   966 sgd_solver.cpp:106] Iteration 48800, lr = 1e-08
I0429 07:00:57.222602   966 solver.cpp:228] Iteration 48900, loss = 0.234199
I0429 07:00:57.222769   966 solver.cpp:244]     Train net output #0: loss = 0.234199 (* 1 = 0.234199 loss)
I0429 07:00:57.222775   966 sgd_solver.cpp:106] Iteration 48900, lr = 1e-08
I0429 07:01:46.541106   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_49000.caffemodel
I0429 07:01:59.260541   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_49000.solverstate
I0429 07:01:59.460912   966 solver.cpp:337] Iteration 49000, Testing net (#0)
I0429 07:01:59.461035   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 07:01:59.461040   966 net.cpp:693] Ignoring source layer visualize
I0429 07:01:59.461043   966 net.cpp:693] Ignoring source layer fake
I0429 07:06:52.018944   966 solver.cpp:404]     Test net output #0: loss = 0.191449 (* 1 = 0.191449 loss)
I0429 07:06:52.329143   966 solver.cpp:228] Iteration 49000, loss = 0.141857
I0429 07:06:52.329164   966 solver.cpp:244]     Train net output #0: loss = 0.141857 (* 1 = 0.141857 loss)
I0429 07:06:52.329186   966 sgd_solver.cpp:106] Iteration 49000, lr = 1e-08
I0429 07:07:43.781152   966 solver.cpp:228] Iteration 49100, loss = 0.215831
I0429 07:07:43.781325   966 solver.cpp:244]     Train net output #0: loss = 0.215831 (* 1 = 0.215831 loss)
I0429 07:07:43.781333   966 sgd_solver.cpp:106] Iteration 49100, lr = 1e-08
I0429 07:08:33.404793   966 solver.cpp:228] Iteration 49200, loss = 0.165114
I0429 07:08:33.404947   966 solver.cpp:244]     Train net output #0: loss = 0.165114 (* 1 = 0.165114 loss)
I0429 07:08:33.404953   966 sgd_solver.cpp:106] Iteration 49200, lr = 1e-08
I0429 07:09:23.024611   966 solver.cpp:228] Iteration 49300, loss = 0.12
I0429 07:09:23.024765   966 solver.cpp:244]     Train net output #0: loss = 0.12 (* 1 = 0.12 loss)
I0429 07:09:23.024772   966 sgd_solver.cpp:106] Iteration 49300, lr = 1e-08
I0429 07:10:14.452481   966 solver.cpp:228] Iteration 49400, loss = 0.0998879
I0429 07:10:14.452672   966 solver.cpp:244]     Train net output #0: loss = 0.0998879 (* 1 = 0.0998879 loss)
I0429 07:10:14.452680   966 sgd_solver.cpp:106] Iteration 49400, lr = 1e-08
I0429 07:11:04.062500   966 solver.cpp:228] Iteration 49500, loss = 0.111453
I0429 07:11:04.062667   966 solver.cpp:244]     Train net output #0: loss = 0.111453 (* 1 = 0.111453 loss)
I0429 07:11:04.062674   966 sgd_solver.cpp:106] Iteration 49500, lr = 1e-08
I0429 07:11:53.679500   966 solver.cpp:228] Iteration 49600, loss = 0.137081
I0429 07:11:53.679657   966 solver.cpp:244]     Train net output #0: loss = 0.137081 (* 1 = 0.137081 loss)
I0429 07:11:53.679663   966 sgd_solver.cpp:106] Iteration 49600, lr = 1e-08
I0429 07:12:44.957460   966 solver.cpp:228] Iteration 49700, loss = 0.135289
I0429 07:12:44.957641   966 solver.cpp:244]     Train net output #0: loss = 0.135289 (* 1 = 0.135289 loss)
I0429 07:12:44.957648   966 sgd_solver.cpp:106] Iteration 49700, lr = 1e-08
I0429 07:13:34.566260   966 solver.cpp:228] Iteration 49800, loss = 0.575418
I0429 07:13:34.566416   966 solver.cpp:244]     Train net output #0: loss = 0.575418 (* 1 = 0.575418 loss)
I0429 07:13:34.566423   966 sgd_solver.cpp:106] Iteration 49800, lr = 1e-08
I0429 07:14:24.177369   966 solver.cpp:228] Iteration 49900, loss = 0.126546
I0429 07:14:24.177532   966 solver.cpp:244]     Train net output #0: loss = 0.126546 (* 1 = 0.126546 loss)
I0429 07:14:24.177538   966 sgd_solver.cpp:106] Iteration 49900, lr = 1e-08
I0429 07:15:14.759168   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_50000.caffemodel
I0429 07:15:24.486881   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_50000.solverstate
I0429 07:15:24.689543   966 solver.cpp:337] Iteration 50000, Testing net (#0)
I0429 07:15:24.689653   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 07:15:24.689661   966 net.cpp:693] Ignoring source layer visualize
I0429 07:15:24.689662   966 net.cpp:693] Ignoring source layer fake
I0429 07:20:17.112324   966 solver.cpp:404]     Test net output #0: loss = 0.185859 (* 1 = 0.185859 loss)
I0429 07:20:17.424247   966 solver.cpp:228] Iteration 50000, loss = 0.107282
I0429 07:20:17.424285   966 solver.cpp:244]     Train net output #0: loss = 0.107282 (* 1 = 0.107282 loss)
I0429 07:20:17.424293   966 sgd_solver.cpp:106] Iteration 50000, lr = 1e-08
I0429 07:21:07.048698   966 solver.cpp:228] Iteration 50100, loss = 0.138209
I0429 07:21:07.048882   966 solver.cpp:244]     Train net output #0: loss = 0.138209 (* 1 = 0.138209 loss)
I0429 07:21:07.048892   966 sgd_solver.cpp:106] Iteration 50100, lr = 1e-08
I0429 07:21:56.678738   966 solver.cpp:228] Iteration 50200, loss = 0.128625
I0429 07:21:56.678900   966 solver.cpp:244]     Train net output #0: loss = 0.128625 (* 1 = 0.128625 loss)
I0429 07:21:56.678907   966 sgd_solver.cpp:106] Iteration 50200, lr = 1e-08
I0429 07:22:47.686679   966 solver.cpp:228] Iteration 50300, loss = 0.139685
I0429 07:22:47.686836   966 solver.cpp:244]     Train net output #0: loss = 0.139685 (* 1 = 0.139685 loss)
I0429 07:22:47.686842   966 sgd_solver.cpp:106] Iteration 50300, lr = 1e-08
I0429 07:23:37.314136   966 solver.cpp:228] Iteration 50400, loss = 0.207985
I0429 07:23:37.314324   966 solver.cpp:244]     Train net output #0: loss = 0.207985 (* 1 = 0.207985 loss)
I0429 07:23:37.314337   966 sgd_solver.cpp:106] Iteration 50400, lr = 1e-08
I0429 07:24:28.547863   966 solver.cpp:228] Iteration 50500, loss = 0.116182
I0429 07:24:28.548689   966 solver.cpp:244]     Train net output #0: loss = 0.116182 (* 1 = 0.116182 loss)
I0429 07:24:28.548698   966 sgd_solver.cpp:106] Iteration 50500, lr = 1e-08
I0429 07:25:18.190749   966 solver.cpp:228] Iteration 50600, loss = 0.147607
I0429 07:25:18.191118   966 solver.cpp:244]     Train net output #0: loss = 0.147607 (* 1 = 0.147607 loss)
I0429 07:25:18.191126   966 sgd_solver.cpp:106] Iteration 50600, lr = 1e-08
I0429 07:26:07.829515   966 solver.cpp:228] Iteration 50700, loss = 0.227684
I0429 07:26:07.829687   966 solver.cpp:244]     Train net output #0: loss = 0.227684 (* 1 = 0.227684 loss)
I0429 07:26:07.829694   966 sgd_solver.cpp:106] Iteration 50700, lr = 1e-08
I0429 07:26:59.420168   966 solver.cpp:228] Iteration 50800, loss = 0.145718
I0429 07:26:59.420354   966 solver.cpp:244]     Train net output #0: loss = 0.145718 (* 1 = 0.145718 loss)
I0429 07:26:59.420363   966 sgd_solver.cpp:106] Iteration 50800, lr = 1e-08
I0429 07:27:49.060612   966 solver.cpp:228] Iteration 50900, loss = 0.176854
I0429 07:27:49.062196   966 solver.cpp:244]     Train net output #0: loss = 0.176854 (* 1 = 0.176854 loss)
I0429 07:27:49.062221   966 sgd_solver.cpp:106] Iteration 50900, lr = 1e-08
I0429 07:28:38.386975   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_51000.caffemodel
I0429 07:28:46.549046   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_51000.solverstate
I0429 07:28:46.754976   966 solver.cpp:337] Iteration 51000, Testing net (#0)
I0429 07:28:46.755117   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 07:28:46.755125   966 net.cpp:693] Ignoring source layer visualize
I0429 07:28:46.755126   966 net.cpp:693] Ignoring source layer fake
I0429 07:33:39.866535   966 solver.cpp:404]     Test net output #0: loss = 0.182376 (* 1 = 0.182376 loss)
I0429 07:33:40.177263   966 solver.cpp:228] Iteration 51000, loss = 0.0346507
I0429 07:33:40.177305   966 solver.cpp:244]     Train net output #0: loss = 0.0346507 (* 1 = 0.0346507 loss)
I0429 07:33:40.177312   966 sgd_solver.cpp:106] Iteration 51000, lr = 1e-08
I0429 07:34:29.813997   966 solver.cpp:228] Iteration 51100, loss = 0.1598
I0429 07:34:29.814170   966 solver.cpp:244]     Train net output #0: loss = 0.1598 (* 1 = 0.1598 loss)
I0429 07:34:29.814177   966 sgd_solver.cpp:106] Iteration 51100, lr = 1e-08
I0429 07:35:21.373894   966 solver.cpp:228] Iteration 51200, loss = 0.135589
I0429 07:35:21.374061   966 solver.cpp:244]     Train net output #0: loss = 0.135589 (* 1 = 0.135589 loss)
I0429 07:35:21.374069   966 sgd_solver.cpp:106] Iteration 51200, lr = 1e-08
I0429 07:36:10.996628   966 solver.cpp:228] Iteration 51300, loss = 0.229488
I0429 07:36:10.996786   966 solver.cpp:244]     Train net output #0: loss = 0.229488 (* 1 = 0.229488 loss)
I0429 07:36:10.996793   966 sgd_solver.cpp:106] Iteration 51300, lr = 1e-08
I0429 07:37:00.633062   966 solver.cpp:228] Iteration 51400, loss = 0.118423
I0429 07:37:00.633205   966 solver.cpp:244]     Train net output #0: loss = 0.118423 (* 1 = 0.118423 loss)
I0429 07:37:00.633213   966 sgd_solver.cpp:106] Iteration 51400, lr = 1e-08
I0429 07:37:52.198838   966 solver.cpp:228] Iteration 51500, loss = 0.23066
I0429 07:37:52.198999   966 solver.cpp:244]     Train net output #0: loss = 0.23066 (* 1 = 0.23066 loss)
I0429 07:37:52.199005   966 sgd_solver.cpp:106] Iteration 51500, lr = 1e-08
I0429 07:38:41.821110   966 solver.cpp:228] Iteration 51600, loss = 0.153674
I0429 07:38:41.821261   966 solver.cpp:244]     Train net output #0: loss = 0.153674 (* 1 = 0.153674 loss)
I0429 07:38:41.821269   966 sgd_solver.cpp:106] Iteration 51600, lr = 1e-08
I0429 07:39:31.465704   966 solver.cpp:228] Iteration 51700, loss = 0.202856
I0429 07:39:31.465885   966 solver.cpp:244]     Train net output #0: loss = 0.202856 (* 1 = 0.202856 loss)
I0429 07:39:31.465893   966 sgd_solver.cpp:106] Iteration 51700, lr = 1e-08
I0429 07:40:21.090768   966 solver.cpp:228] Iteration 51800, loss = 0.133846
I0429 07:40:21.090948   966 solver.cpp:244]     Train net output #0: loss = 0.133846 (* 1 = 0.133846 loss)
I0429 07:40:21.090955   966 sgd_solver.cpp:106] Iteration 51800, lr = 1e-08
I0429 07:41:12.625540   966 solver.cpp:228] Iteration 51900, loss = 0.148035
I0429 07:41:12.625728   966 solver.cpp:244]     Train net output #0: loss = 0.148035 (* 1 = 0.148035 loss)
I0429 07:41:12.625735   966 sgd_solver.cpp:106] Iteration 51900, lr = 1e-08
I0429 07:42:01.939286   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_52000.caffemodel
I0429 07:42:13.050730   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_52000.solverstate
I0429 07:42:13.258831   966 solver.cpp:337] Iteration 52000, Testing net (#0)
I0429 07:42:13.258955   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 07:42:13.258977   966 net.cpp:693] Ignoring source layer visualize
I0429 07:42:13.258980   966 net.cpp:693] Ignoring source layer fake
I0429 07:47:06.791941   966 solver.cpp:404]     Test net output #0: loss = 0.179914 (* 1 = 0.179914 loss)
I0429 07:47:07.103348   966 solver.cpp:228] Iteration 52000, loss = 0.0838777
I0429 07:47:07.103402   966 solver.cpp:244]     Train net output #0: loss = 0.0838777 (* 1 = 0.0838777 loss)
I0429 07:47:07.103410   966 sgd_solver.cpp:106] Iteration 52000, lr = 1e-08
I0429 07:47:56.731019   966 solver.cpp:228] Iteration 52100, loss = 0.231744
I0429 07:47:56.731173   966 solver.cpp:244]     Train net output #0: loss = 0.231744 (* 1 = 0.231744 loss)
I0429 07:47:56.731182   966 sgd_solver.cpp:106] Iteration 52100, lr = 1e-08
I0429 07:48:48.342803   966 solver.cpp:228] Iteration 52200, loss = 0.242489
I0429 07:48:48.342970   966 solver.cpp:244]     Train net output #0: loss = 0.242489 (* 1 = 0.242489 loss)
I0429 07:48:48.342978   966 sgd_solver.cpp:106] Iteration 52200, lr = 1e-08
I0429 07:49:37.953268   966 solver.cpp:228] Iteration 52300, loss = 0.0950688
I0429 07:49:37.953429   966 solver.cpp:244]     Train net output #0: loss = 0.0950688 (* 1 = 0.0950688 loss)
I0429 07:49:37.953445   966 sgd_solver.cpp:106] Iteration 52300, lr = 1e-08
I0429 07:50:27.576867   966 solver.cpp:228] Iteration 52400, loss = 0.128482
I0429 07:50:27.577055   966 solver.cpp:244]     Train net output #0: loss = 0.128482 (* 1 = 0.128482 loss)
I0429 07:50:27.577065   966 sgd_solver.cpp:106] Iteration 52400, lr = 1e-08
I0429 07:51:19.832921   966 solver.cpp:228] Iteration 52500, loss = 0.0904777
I0429 07:51:19.833081   966 solver.cpp:244]     Train net output #0: loss = 0.0904777 (* 1 = 0.0904777 loss)
I0429 07:51:19.833088   966 sgd_solver.cpp:106] Iteration 52500, lr = 1e-09
I0429 07:52:09.452064   966 solver.cpp:228] Iteration 52600, loss = 0.0520048
I0429 07:52:09.452246   966 solver.cpp:244]     Train net output #0: loss = 0.0520048 (* 1 = 0.0520048 loss)
I0429 07:52:09.452253   966 sgd_solver.cpp:106] Iteration 52600, lr = 1e-09
I0429 07:52:59.081751   966 solver.cpp:228] Iteration 52700, loss = 0.0974492
I0429 07:52:59.081969   966 solver.cpp:244]     Train net output #0: loss = 0.0974492 (* 1 = 0.0974492 loss)
I0429 07:52:59.081979   966 sgd_solver.cpp:106] Iteration 52700, lr = 1e-09
I0429 07:53:48.705895   966 solver.cpp:228] Iteration 52800, loss = 0.151744
I0429 07:53:48.706082   966 solver.cpp:244]     Train net output #0: loss = 0.151744 (* 1 = 0.151744 loss)
I0429 07:53:48.706089   966 sgd_solver.cpp:106] Iteration 52800, lr = 1e-09
I0429 07:54:40.044013   966 solver.cpp:228] Iteration 52900, loss = 0.0578965
I0429 07:54:40.044199   966 solver.cpp:244]     Train net output #0: loss = 0.0578965 (* 1 = 0.0578965 loss)
I0429 07:54:40.044206   966 sgd_solver.cpp:106] Iteration 52900, lr = 1e-09
I0429 07:55:29.361443   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_53000.caffemodel
I0429 07:55:39.986114   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_53000.solverstate
I0429 07:55:40.181334   966 solver.cpp:337] Iteration 53000, Testing net (#0)
I0429 07:55:40.181493   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 07:55:40.181499   966 net.cpp:693] Ignoring source layer visualize
I0429 07:55:40.181502   966 net.cpp:693] Ignoring source layer fake
I0429 08:00:32.793895   966 solver.cpp:404]     Test net output #0: loss = 0.182143 (* 1 = 0.182143 loss)
I0429 08:00:33.103757   966 solver.cpp:228] Iteration 53000, loss = 0.0262152
I0429 08:00:33.103801   966 solver.cpp:244]     Train net output #0: loss = 0.0262152 (* 1 = 0.0262152 loss)
I0429 08:00:33.103808   966 sgd_solver.cpp:106] Iteration 53000, lr = 1e-09
I0429 08:01:22.727955   966 solver.cpp:228] Iteration 53100, loss = 0.134266
I0429 08:01:22.728127   966 solver.cpp:244]     Train net output #0: loss = 0.134266 (* 1 = 0.134266 loss)
I0429 08:01:22.728133   966 sgd_solver.cpp:106] Iteration 53100, lr = 1e-09
I0429 08:02:13.672672   966 solver.cpp:228] Iteration 53200, loss = 0.129637
I0429 08:02:13.672830   966 solver.cpp:244]     Train net output #0: loss = 0.129637 (* 1 = 0.129637 loss)
I0429 08:02:13.672837   966 sgd_solver.cpp:106] Iteration 53200, lr = 1e-09
I0429 08:03:03.307512   966 solver.cpp:228] Iteration 53300, loss = 0.149412
I0429 08:03:03.307658   966 solver.cpp:244]     Train net output #0: loss = 0.149412 (* 1 = 0.149412 loss)
I0429 08:03:03.307664   966 sgd_solver.cpp:106] Iteration 53300, lr = 1e-09
I0429 08:03:54.316258   966 solver.cpp:228] Iteration 53400, loss = 0.0880892
I0429 08:03:54.316440   966 solver.cpp:244]     Train net output #0: loss = 0.0880892 (* 1 = 0.0880892 loss)
I0429 08:03:54.316448   966 sgd_solver.cpp:106] Iteration 53400, lr = 1e-09
I0429 08:04:43.954666   966 solver.cpp:228] Iteration 53500, loss = 0.191688
I0429 08:04:43.954867   966 solver.cpp:244]     Train net output #0: loss = 0.191688 (* 1 = 0.191688 loss)
I0429 08:04:43.954879   966 sgd_solver.cpp:106] Iteration 53500, lr = 1e-09
I0429 08:05:33.612685   966 solver.cpp:228] Iteration 53600, loss = 0.238592
I0429 08:05:33.612835   966 solver.cpp:244]     Train net output #0: loss = 0.238592 (* 1 = 0.238592 loss)
I0429 08:05:33.612843   966 sgd_solver.cpp:106] Iteration 53600, lr = 1e-09
I0429 08:06:24.956748   966 solver.cpp:228] Iteration 53700, loss = 0.135384
I0429 08:06:24.956985   966 solver.cpp:244]     Train net output #0: loss = 0.135384 (* 1 = 0.135384 loss)
I0429 08:06:24.957006   966 sgd_solver.cpp:106] Iteration 53700, lr = 1e-09
I0429 08:07:14.586230   966 solver.cpp:228] Iteration 53800, loss = 0.108791
I0429 08:07:14.586391   966 solver.cpp:244]     Train net output #0: loss = 0.108791 (* 1 = 0.108791 loss)
I0429 08:07:14.586400   966 sgd_solver.cpp:106] Iteration 53800, lr = 1e-09
I0429 08:08:04.229552   966 solver.cpp:228] Iteration 53900, loss = 0.0224996
I0429 08:08:04.229722   966 solver.cpp:244]     Train net output #0: loss = 0.0224996 (* 1 = 0.0224996 loss)
I0429 08:08:04.229729   966 sgd_solver.cpp:106] Iteration 53900, lr = 1e-09
I0429 08:08:55.323725   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_54000.caffemodel
I0429 08:09:15.468554   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_54000.solverstate
I0429 08:09:15.663249   966 solver.cpp:337] Iteration 54000, Testing net (#0)
I0429 08:09:15.663372   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 08:09:15.663378   966 net.cpp:693] Ignoring source layer visualize
I0429 08:09:15.663396   966 net.cpp:693] Ignoring source layer fake
I0429 08:14:07.954836   966 solver.cpp:404]     Test net output #0: loss = 0.191812 (* 1 = 0.191812 loss)
I0429 08:14:08.264708   966 solver.cpp:228] Iteration 54000, loss = 0.0990689
I0429 08:14:08.264727   966 solver.cpp:244]     Train net output #0: loss = 0.0990689 (* 1 = 0.0990689 loss)
I0429 08:14:08.264749   966 sgd_solver.cpp:106] Iteration 54000, lr = 1e-09
I0429 08:14:57.898011   966 solver.cpp:228] Iteration 54100, loss = 0.119295
I0429 08:14:57.898164   966 solver.cpp:244]     Train net output #0: loss = 0.119295 (* 1 = 0.119295 loss)
I0429 08:14:57.898170   966 sgd_solver.cpp:106] Iteration 54100, lr = 1e-09
I0429 08:15:47.534308   966 solver.cpp:228] Iteration 54200, loss = 0.16044
I0429 08:15:47.534497   966 solver.cpp:244]     Train net output #0: loss = 0.16044 (* 1 = 0.16044 loss)
I0429 08:15:47.534504   966 sgd_solver.cpp:106] Iteration 54200, lr = 1e-09
I0429 08:16:39.021459   966 solver.cpp:228] Iteration 54300, loss = 0.153822
I0429 08:16:39.021637   966 solver.cpp:244]     Train net output #0: loss = 0.153822 (* 1 = 0.153822 loss)
I0429 08:16:39.021644   966 sgd_solver.cpp:106] Iteration 54300, lr = 1e-09
I0429 08:17:28.658490   966 solver.cpp:228] Iteration 54400, loss = 0.14487
I0429 08:17:28.658649   966 solver.cpp:244]     Train net output #0: loss = 0.14487 (* 1 = 0.14487 loss)
I0429 08:17:28.658658   966 sgd_solver.cpp:106] Iteration 54400, lr = 1e-09
I0429 08:18:18.299721   966 solver.cpp:228] Iteration 54500, loss = 0.143276
I0429 08:18:18.299887   966 solver.cpp:244]     Train net output #0: loss = 0.143276 (* 1 = 0.143276 loss)
I0429 08:18:18.299896   966 sgd_solver.cpp:106] Iteration 54500, lr = 1e-09
I0429 08:19:09.859750   966 solver.cpp:228] Iteration 54600, loss = 0.181096
I0429 08:19:09.859930   966 solver.cpp:244]     Train net output #0: loss = 0.181096 (* 1 = 0.181096 loss)
I0429 08:19:09.859936   966 sgd_solver.cpp:106] Iteration 54600, lr = 1e-09
I0429 08:19:59.494957   966 solver.cpp:228] Iteration 54700, loss = 0.213423
I0429 08:19:59.495146   966 solver.cpp:244]     Train net output #0: loss = 0.213423 (* 1 = 0.213423 loss)
I0429 08:19:59.495153   966 sgd_solver.cpp:106] Iteration 54700, lr = 1e-09
I0429 08:20:49.122370   966 solver.cpp:228] Iteration 54800, loss = 0.0971995
I0429 08:20:49.122534   966 solver.cpp:244]     Train net output #0: loss = 0.0971995 (* 1 = 0.0971995 loss)
I0429 08:20:49.122541   966 sgd_solver.cpp:106] Iteration 54800, lr = 1e-09
I0429 08:21:38.756608   966 solver.cpp:228] Iteration 54900, loss = 0.1984
I0429 08:21:38.756783   966 solver.cpp:244]     Train net output #0: loss = 0.1984 (* 1 = 0.1984 loss)
I0429 08:21:38.756789   966 sgd_solver.cpp:106] Iteration 54900, lr = 1e-09
I0429 08:22:29.969677   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_55000.caffemodel
I0429 08:22:39.827780   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_55000.solverstate
I0429 08:22:40.018887   966 solver.cpp:337] Iteration 55000, Testing net (#0)
I0429 08:22:40.019012   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 08:22:40.019033   966 net.cpp:693] Ignoring source layer visualize
I0429 08:22:40.019035   966 net.cpp:693] Ignoring source layer fake
I0429 08:27:32.384507   966 solver.cpp:404]     Test net output #0: loss = 0.185001 (* 1 = 0.185001 loss)
I0429 08:27:32.694097   966 solver.cpp:228] Iteration 55000, loss = 0.143234
I0429 08:27:32.694114   966 solver.cpp:244]     Train net output #0: loss = 0.143234 (* 1 = 0.143234 loss)
I0429 08:27:32.694136   966 sgd_solver.cpp:106] Iteration 55000, lr = 1e-09
I0429 08:28:22.329576   966 solver.cpp:228] Iteration 55100, loss = 0.10744
I0429 08:28:22.329736   966 solver.cpp:244]     Train net output #0: loss = 0.10744 (* 1 = 0.10744 loss)
I0429 08:28:22.329743   966 sgd_solver.cpp:106] Iteration 55100, lr = 1e-09
I0429 08:29:11.964565   966 solver.cpp:228] Iteration 55200, loss = 0.18071
I0429 08:29:11.964715   966 solver.cpp:244]     Train net output #0: loss = 0.18071 (* 1 = 0.18071 loss)
I0429 08:29:11.964723   966 sgd_solver.cpp:106] Iteration 55200, lr = 1e-09
I0429 08:30:03.378715   966 solver.cpp:228] Iteration 55300, loss = 0.0824325
I0429 08:30:03.378883   966 solver.cpp:244]     Train net output #0: loss = 0.0824325 (* 1 = 0.0824325 loss)
I0429 08:30:03.378890   966 sgd_solver.cpp:106] Iteration 55300, lr = 1e-09
I0429 08:30:52.987251   966 solver.cpp:228] Iteration 55400, loss = 0.129787
I0429 08:30:52.987399   966 solver.cpp:244]     Train net output #0: loss = 0.129787 (* 1 = 0.129787 loss)
I0429 08:30:52.987406   966 sgd_solver.cpp:106] Iteration 55400, lr = 1e-09
I0429 08:31:42.605723   966 solver.cpp:228] Iteration 55500, loss = 0.110436
I0429 08:31:42.605911   966 solver.cpp:244]     Train net output #0: loss = 0.110436 (* 1 = 0.110436 loss)
I0429 08:31:42.605918   966 sgd_solver.cpp:106] Iteration 55500, lr = 1e-09
I0429 08:32:32.258740   966 solver.cpp:228] Iteration 55600, loss = 0.0981403
I0429 08:32:32.258891   966 solver.cpp:244]     Train net output #0: loss = 0.0981403 (* 1 = 0.0981403 loss)
I0429 08:32:32.258900   966 sgd_solver.cpp:106] Iteration 55600, lr = 1e-09
I0429 08:33:23.665603   966 solver.cpp:228] Iteration 55700, loss = 0.0699992
I0429 08:33:23.665773   966 solver.cpp:244]     Train net output #0: loss = 0.0699992 (* 1 = 0.0699992 loss)
I0429 08:33:23.665781   966 sgd_solver.cpp:106] Iteration 55700, lr = 1e-09
I0429 08:34:13.277659   966 solver.cpp:228] Iteration 55800, loss = 0.111628
I0429 08:34:13.277817   966 solver.cpp:244]     Train net output #0: loss = 0.111628 (* 1 = 0.111628 loss)
I0429 08:34:13.277823   966 sgd_solver.cpp:106] Iteration 55800, lr = 1e-09
I0429 08:35:02.906038   966 solver.cpp:228] Iteration 55900, loss = 0.0926859
I0429 08:35:02.906170   966 solver.cpp:244]     Train net output #0: loss = 0.0926859 (* 1 = 0.0926859 loss)
I0429 08:35:02.906177   966 sgd_solver.cpp:106] Iteration 55900, lr = 1e-09
I0429 08:35:53.862226   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_56000.caffemodel
I0429 08:36:05.906675   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_56000.solverstate
I0429 08:36:06.099038   966 solver.cpp:337] Iteration 56000, Testing net (#0)
I0429 08:36:06.099164   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 08:36:06.099169   966 net.cpp:693] Ignoring source layer visualize
I0429 08:36:06.099171   966 net.cpp:693] Ignoring source layer fake
I0429 08:40:58.870126   966 solver.cpp:404]     Test net output #0: loss = 0.182398 (* 1 = 0.182398 loss)
I0429 08:40:59.181529   966 solver.cpp:228] Iteration 56000, loss = 0.084158
I0429 08:40:59.181550   966 solver.cpp:244]     Train net output #0: loss = 0.084158 (* 1 = 0.084158 loss)
I0429 08:40:59.181571   966 sgd_solver.cpp:106] Iteration 56000, lr = 1e-09
I0429 08:41:48.794297   966 solver.cpp:228] Iteration 56100, loss = 0.12617
I0429 08:41:48.794471   966 solver.cpp:244]     Train net output #0: loss = 0.12617 (* 1 = 0.12617 loss)
I0429 08:41:48.794477   966 sgd_solver.cpp:106] Iteration 56100, lr = 1e-09
I0429 08:42:38.407474   966 solver.cpp:228] Iteration 56200, loss = 0.114531
I0429 08:42:38.407645   966 solver.cpp:244]     Train net output #0: loss = 0.114531 (* 1 = 0.114531 loss)
I0429 08:42:38.407652   966 sgd_solver.cpp:106] Iteration 56200, lr = 1e-09
I0429 08:43:29.309372   966 solver.cpp:228] Iteration 56300, loss = 0.107576
I0429 08:43:29.309536   966 solver.cpp:244]     Train net output #0: loss = 0.107576 (* 1 = 0.107576 loss)
I0429 08:43:29.309545   966 sgd_solver.cpp:106] Iteration 56300, lr = 1e-09
I0429 08:44:18.929024   966 solver.cpp:228] Iteration 56400, loss = 0.0841341
I0429 08:44:18.929181   966 solver.cpp:244]     Train net output #0: loss = 0.0841341 (* 1 = 0.0841341 loss)
I0429 08:44:18.929195   966 sgd_solver.cpp:106] Iteration 56400, lr = 1e-09
I0429 08:45:09.912503   966 solver.cpp:228] Iteration 56500, loss = 0.0950368
I0429 08:45:09.912648   966 solver.cpp:244]     Train net output #0: loss = 0.0950368 (* 1 = 0.0950368 loss)
I0429 08:45:09.912655   966 sgd_solver.cpp:106] Iteration 56500, lr = 1e-09
I0429 08:45:59.547116   966 solver.cpp:228] Iteration 56600, loss = 0.122082
I0429 08:45:59.547335   966 solver.cpp:244]     Train net output #0: loss = 0.122082 (* 1 = 0.122082 loss)
I0429 08:45:59.547343   966 sgd_solver.cpp:106] Iteration 56600, lr = 1e-09
I0429 08:46:49.170778   966 solver.cpp:228] Iteration 56700, loss = 0.112725
I0429 08:46:49.170933   966 solver.cpp:244]     Train net output #0: loss = 0.112725 (* 1 = 0.112725 loss)
I0429 08:46:49.170940   966 sgd_solver.cpp:106] Iteration 56700, lr = 1e-09
I0429 08:47:40.354290   966 solver.cpp:228] Iteration 56800, loss = 0.154336
I0429 08:47:40.354480   966 solver.cpp:244]     Train net output #0: loss = 0.154336 (* 1 = 0.154336 loss)
I0429 08:47:40.354487   966 sgd_solver.cpp:106] Iteration 56800, lr = 1e-09
I0429 08:48:29.984663   966 solver.cpp:228] Iteration 56900, loss = 0.142667
I0429 08:48:29.984810   966 solver.cpp:244]     Train net output #0: loss = 0.142667 (* 1 = 0.142667 loss)
I0429 08:48:29.984817   966 sgd_solver.cpp:106] Iteration 56900, lr = 1e-09
I0429 08:49:19.294526   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_57000.caffemodel
I0429 08:49:32.678915   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_57000.solverstate
I0429 08:49:32.872398   966 solver.cpp:337] Iteration 57000, Testing net (#0)
I0429 08:49:32.872529   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 08:49:32.872550   966 net.cpp:693] Ignoring source layer visualize
I0429 08:49:32.872552   966 net.cpp:693] Ignoring source layer fake
I0429 08:54:25.367861   966 solver.cpp:404]     Test net output #0: loss = 0.179659 (* 1 = 0.179659 loss)
I0429 08:54:25.677562   966 solver.cpp:228] Iteration 57000, loss = 0.262997
I0429 08:54:25.677580   966 solver.cpp:244]     Train net output #0: loss = 0.262997 (* 1 = 0.262997 loss)
I0429 08:54:25.677603   966 sgd_solver.cpp:106] Iteration 57000, lr = 1e-09
I0429 08:55:17.104468   966 solver.cpp:228] Iteration 57100, loss = 0.12508
I0429 08:55:17.104652   966 solver.cpp:244]     Train net output #0: loss = 0.12508 (* 1 = 0.12508 loss)
I0429 08:55:17.104660   966 sgd_solver.cpp:106] Iteration 57100, lr = 1e-09
I0429 08:56:06.731755   966 solver.cpp:228] Iteration 57200, loss = 0.0981034
I0429 08:56:06.731920   966 solver.cpp:244]     Train net output #0: loss = 0.0981034 (* 1 = 0.0981034 loss)
I0429 08:56:06.731926   966 sgd_solver.cpp:106] Iteration 57200, lr = 1e-09
I0429 08:56:56.367224   966 solver.cpp:228] Iteration 57300, loss = 0.163004
I0429 08:56:56.367630   966 solver.cpp:244]     Train net output #0: loss = 0.163004 (* 1 = 0.163004 loss)
I0429 08:56:56.367640   966 sgd_solver.cpp:106] Iteration 57300, lr = 1e-09
I0429 08:57:47.882180   966 solver.cpp:228] Iteration 57400, loss = 0.205166
I0429 08:57:47.882352   966 solver.cpp:244]     Train net output #0: loss = 0.205166 (* 1 = 0.205166 loss)
I0429 08:57:47.882359   966 sgd_solver.cpp:106] Iteration 57400, lr = 1e-09
I0429 08:58:37.510717   966 solver.cpp:228] Iteration 57500, loss = 0.156848
I0429 08:58:37.510881   966 solver.cpp:244]     Train net output #0: loss = 0.156848 (* 1 = 0.156848 loss)
I0429 08:58:37.510888   966 sgd_solver.cpp:106] Iteration 57500, lr = 1e-09
I0429 08:59:27.143731   966 solver.cpp:228] Iteration 57600, loss = 0.0925204
I0429 08:59:27.145517   966 solver.cpp:244]     Train net output #0: loss = 0.0925204 (* 1 = 0.0925204 loss)
I0429 08:59:27.145541   966 sgd_solver.cpp:106] Iteration 57600, lr = 1e-09
I0429 09:00:16.791708   966 solver.cpp:228] Iteration 57700, loss = 0.0713121
I0429 09:00:16.791874   966 solver.cpp:244]     Train net output #0: loss = 0.0713121 (* 1 = 0.0713121 loss)
I0429 09:00:16.791882   966 sgd_solver.cpp:106] Iteration 57700, lr = 1e-09
I0429 09:01:08.256712   966 solver.cpp:228] Iteration 57800, loss = 0.200657
I0429 09:01:08.256875   966 solver.cpp:244]     Train net output #0: loss = 0.200657 (* 1 = 0.200657 loss)
I0429 09:01:08.256882   966 sgd_solver.cpp:106] Iteration 57800, lr = 1e-09
I0429 09:01:57.888634   966 solver.cpp:228] Iteration 57900, loss = 0.196693
I0429 09:01:57.888780   966 solver.cpp:244]     Train net output #0: loss = 0.196693 (* 1 = 0.196693 loss)
I0429 09:01:57.888788   966 sgd_solver.cpp:106] Iteration 57900, lr = 1e-09
I0429 09:02:47.213899   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_58000.caffemodel
I0429 09:02:53.465912   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_58000.solverstate
I0429 09:02:53.664819   966 solver.cpp:337] Iteration 58000, Testing net (#0)
I0429 09:02:53.664938   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 09:02:53.664945   966 net.cpp:693] Ignoring source layer visualize
I0429 09:02:53.664947   966 net.cpp:693] Ignoring source layer fake
I0429 09:07:46.606268   966 solver.cpp:404]     Test net output #0: loss = 0.183064 (* 1 = 0.183064 loss)
I0429 09:07:46.918176   966 solver.cpp:228] Iteration 58000, loss = 0.223976
I0429 09:07:46.918195   966 solver.cpp:244]     Train net output #0: loss = 0.223976 (* 1 = 0.223976 loss)
I0429 09:07:46.918215   966 sgd_solver.cpp:106] Iteration 58000, lr = 1e-09
I0429 09:08:38.363090   966 solver.cpp:228] Iteration 58100, loss = 0.113916
I0429 09:08:38.363255   966 solver.cpp:244]     Train net output #0: loss = 0.113916 (* 1 = 0.113916 loss)
I0429 09:08:38.363263   966 sgd_solver.cpp:106] Iteration 58100, lr = 1e-09
I0429 09:09:27.994514   966 solver.cpp:228] Iteration 58200, loss = 0.11622
I0429 09:09:27.996500   966 solver.cpp:244]     Train net output #0: loss = 0.11622 (* 1 = 0.11622 loss)
I0429 09:09:27.996507   966 sgd_solver.cpp:106] Iteration 58200, lr = 1e-09
I0429 09:10:17.624595   966 solver.cpp:228] Iteration 58300, loss = 0.140139
I0429 09:10:17.624749   966 solver.cpp:244]     Train net output #0: loss = 0.140139 (* 1 = 0.140139 loss)
I0429 09:10:17.624755   966 sgd_solver.cpp:106] Iteration 58300, lr = 1e-09
I0429 09:11:09.055382   966 solver.cpp:228] Iteration 58400, loss = 0.12675
I0429 09:11:09.055546   966 solver.cpp:244]     Train net output #0: loss = 0.12675 (* 1 = 0.12675 loss)
I0429 09:11:09.055553   966 sgd_solver.cpp:106] Iteration 58400, lr = 1e-09
I0429 09:11:58.664989   966 solver.cpp:228] Iteration 58500, loss = 0.0628432
I0429 09:11:58.665144   966 solver.cpp:244]     Train net output #0: loss = 0.0628432 (* 1 = 0.0628432 loss)
I0429 09:11:58.665151   966 sgd_solver.cpp:106] Iteration 58500, lr = 1e-09
I0429 09:12:48.284858   966 solver.cpp:228] Iteration 58600, loss = 0.134626
I0429 09:12:48.285010   966 solver.cpp:244]     Train net output #0: loss = 0.134626 (* 1 = 0.134626 loss)
I0429 09:12:48.285017   966 sgd_solver.cpp:106] Iteration 58600, lr = 1e-09
I0429 09:13:37.927594   966 solver.cpp:228] Iteration 58700, loss = 0.130029
I0429 09:13:37.927794   966 solver.cpp:244]     Train net output #0: loss = 0.130029 (* 1 = 0.130029 loss)
I0429 09:13:37.927808   966 sgd_solver.cpp:106] Iteration 58700, lr = 1e-09
I0429 09:14:29.324738   966 solver.cpp:228] Iteration 58800, loss = 0.196466
I0429 09:14:29.324893   966 solver.cpp:244]     Train net output #0: loss = 0.196466 (* 1 = 0.196466 loss)
I0429 09:14:29.324899   966 sgd_solver.cpp:106] Iteration 58800, lr = 1e-09
I0429 09:15:18.948137   966 solver.cpp:228] Iteration 58900, loss = 0.0987779
I0429 09:15:18.948284   966 solver.cpp:244]     Train net output #0: loss = 0.0987779 (* 1 = 0.0987779 loss)
I0429 09:15:18.948292   966 sgd_solver.cpp:106] Iteration 58900, lr = 1e-09
I0429 09:16:08.253955   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_59000.caffemodel
I0429 09:16:21.850859   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_59000.solverstate
I0429 09:16:22.053094   966 solver.cpp:337] Iteration 59000, Testing net (#0)
I0429 09:16:22.053232   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 09:16:22.053238   966 net.cpp:693] Ignoring source layer visualize
I0429 09:16:22.053241   966 net.cpp:693] Ignoring source layer fake
I0429 09:21:14.461580   966 solver.cpp:404]     Test net output #0: loss = 0.19235 (* 1 = 0.19235 loss)
I0429 09:21:14.771620   966 solver.cpp:228] Iteration 59000, loss = 0.125055
I0429 09:21:14.771639   966 solver.cpp:244]     Train net output #0: loss = 0.125055 (* 1 = 0.125055 loss)
I0429 09:21:14.771661   966 sgd_solver.cpp:106] Iteration 59000, lr = 1e-09
I0429 09:22:06.060586   966 solver.cpp:228] Iteration 59100, loss = 0.173292
I0429 09:22:06.060752   966 solver.cpp:244]     Train net output #0: loss = 0.173292 (* 1 = 0.173292 loss)
I0429 09:22:06.060760   966 sgd_solver.cpp:106] Iteration 59100, lr = 1e-09
I0429 09:22:55.678267   966 solver.cpp:228] Iteration 59200, loss = 0.201918
I0429 09:22:55.678421   966 solver.cpp:244]     Train net output #0: loss = 0.201918 (* 1 = 0.201918 loss)
I0429 09:22:55.678428   966 sgd_solver.cpp:106] Iteration 59200, lr = 1e-09
I0429 09:23:45.301566   966 solver.cpp:228] Iteration 59300, loss = 0.110959
I0429 09:23:45.301713   966 solver.cpp:244]     Train net output #0: loss = 0.110959 (* 1 = 0.110959 loss)
I0429 09:23:45.301720   966 sgd_solver.cpp:106] Iteration 59300, lr = 1e-09
I0429 09:24:36.223357   966 solver.cpp:228] Iteration 59400, loss = 0.0953506
I0429 09:24:36.223498   966 solver.cpp:244]     Train net output #0: loss = 0.0953506 (* 1 = 0.0953506 loss)
I0429 09:24:36.223506   966 sgd_solver.cpp:106] Iteration 59400, lr = 1e-09
I0429 09:25:25.844612   966 solver.cpp:228] Iteration 59500, loss = 0.0861574
I0429 09:25:25.844758   966 solver.cpp:244]     Train net output #0: loss = 0.0861574 (* 1 = 0.0861574 loss)
I0429 09:25:25.844764   966 sgd_solver.cpp:106] Iteration 59500, lr = 1e-09
I0429 09:26:15.477365   966 solver.cpp:228] Iteration 59600, loss = 0.127836
I0429 09:26:15.477581   966 solver.cpp:244]     Train net output #0: loss = 0.127836 (* 1 = 0.127836 loss)
I0429 09:26:15.477589   966 sgd_solver.cpp:106] Iteration 59600, lr = 1e-09
I0429 09:27:06.411806   966 solver.cpp:228] Iteration 59700, loss = 0.12635
I0429 09:27:06.411975   966 solver.cpp:244]     Train net output #0: loss = 0.12635 (* 1 = 0.12635 loss)
I0429 09:27:06.411984   966 sgd_solver.cpp:106] Iteration 59700, lr = 1e-09
I0429 09:27:56.055372   966 solver.cpp:228] Iteration 59800, loss = 0.142431
I0429 09:27:56.055531   966 solver.cpp:244]     Train net output #0: loss = 0.142431 (* 1 = 0.142431 loss)
I0429 09:27:56.055536   966 sgd_solver.cpp:106] Iteration 59800, lr = 1e-09
I0429 09:28:47.230044   966 solver.cpp:228] Iteration 59900, loss = 0.251303
I0429 09:28:47.230208   966 solver.cpp:244]     Train net output #0: loss = 0.251303 (* 1 = 0.251303 loss)
I0429 09:28:47.230216   966 sgd_solver.cpp:106] Iteration 59900, lr = 1e-09
I0429 09:29:36.549829   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_60000.caffemodel
I0429 09:30:14.869680   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_60000.solverstate
I0429 09:30:15.066213   966 solver.cpp:337] Iteration 60000, Testing net (#0)
I0429 09:30:15.066339   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 09:30:15.066345   966 net.cpp:693] Ignoring source layer visualize
I0429 09:30:15.066347   966 net.cpp:693] Ignoring source layer fake
I0429 09:35:07.222700   966 solver.cpp:404]     Test net output #0: loss = 0.183715 (* 1 = 0.183715 loss)
I0429 09:35:07.532624   966 solver.cpp:228] Iteration 60000, loss = 0.213367
I0429 09:35:07.532657   966 solver.cpp:244]     Train net output #0: loss = 0.213367 (* 1 = 0.213367 loss)
I0429 09:35:07.532678   966 sgd_solver.cpp:106] Iteration 60000, lr = 1e-10
I0429 09:35:57.162447   966 solver.cpp:228] Iteration 60100, loss = 0.125321
I0429 09:35:57.162609   966 solver.cpp:244]     Train net output #0: loss = 0.125321 (* 1 = 0.125321 loss)
I0429 09:35:57.162616   966 sgd_solver.cpp:106] Iteration 60100, lr = 1e-10
I0429 09:36:48.547580   966 solver.cpp:228] Iteration 60200, loss = 0.184351
I0429 09:36:48.547739   966 solver.cpp:244]     Train net output #0: loss = 0.184351 (* 1 = 0.184351 loss)
I0429 09:36:48.547745   966 sgd_solver.cpp:106] Iteration 60200, lr = 1e-10
I0429 09:37:38.167281   966 solver.cpp:228] Iteration 60300, loss = 0.180046
I0429 09:37:38.167464   966 solver.cpp:244]     Train net output #0: loss = 0.180046 (* 1 = 0.180046 loss)
I0429 09:37:38.167471   966 sgd_solver.cpp:106] Iteration 60300, lr = 1e-10
I0429 09:38:27.789790   966 solver.cpp:228] Iteration 60400, loss = 0.13365
I0429 09:38:27.789942   966 solver.cpp:244]     Train net output #0: loss = 0.13365 (* 1 = 0.13365 loss)
I0429 09:38:27.789948   966 sgd_solver.cpp:106] Iteration 60400, lr = 1e-10
I0429 09:39:19.270807   966 solver.cpp:228] Iteration 60500, loss = 0.145187
I0429 09:39:19.270969   966 solver.cpp:244]     Train net output #0: loss = 0.145187 (* 1 = 0.145187 loss)
I0429 09:39:19.270977   966 sgd_solver.cpp:106] Iteration 60500, lr = 1e-10
I0429 09:40:08.887274   966 solver.cpp:228] Iteration 60600, loss = 0.140563
I0429 09:40:08.887426   966 solver.cpp:244]     Train net output #0: loss = 0.140563 (* 1 = 0.140563 loss)
I0429 09:40:08.887432   966 sgd_solver.cpp:106] Iteration 60600, lr = 1e-10
I0429 09:40:58.517594   966 solver.cpp:228] Iteration 60700, loss = 0.14802
I0429 09:40:58.517753   966 solver.cpp:244]     Train net output #0: loss = 0.14802 (* 1 = 0.14802 loss)
I0429 09:40:58.517760   966 sgd_solver.cpp:106] Iteration 60700, lr = 1e-10
I0429 09:41:48.152217   966 solver.cpp:228] Iteration 60800, loss = 0.216649
I0429 09:41:48.152375   966 solver.cpp:244]     Train net output #0: loss = 0.216649 (* 1 = 0.216649 loss)
I0429 09:41:48.152381   966 sgd_solver.cpp:106] Iteration 60800, lr = 1e-10
I0429 09:42:39.673094   966 solver.cpp:228] Iteration 60900, loss = 0.136489
I0429 09:42:39.673271   966 solver.cpp:244]     Train net output #0: loss = 0.136489 (* 1 = 0.136489 loss)
I0429 09:42:39.673279   966 sgd_solver.cpp:106] Iteration 60900, lr = 1e-10
I0429 09:43:28.976002   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_61000.caffemodel
I0429 09:43:40.184355   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_61000.solverstate
I0429 09:43:40.378439   966 solver.cpp:337] Iteration 61000, Testing net (#0)
I0429 09:43:40.378558   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 09:43:40.378563   966 net.cpp:693] Ignoring source layer visualize
I0429 09:43:40.378566   966 net.cpp:693] Ignoring source layer fake
I0429 09:48:32.999300   966 solver.cpp:404]     Test net output #0: loss = 0.182524 (* 1 = 0.182524 loss)
I0429 09:48:33.310956   966 solver.cpp:228] Iteration 61000, loss = 0.140662
I0429 09:48:33.310974   966 solver.cpp:244]     Train net output #0: loss = 0.140662 (* 1 = 0.140662 loss)
I0429 09:48:33.310997   966 sgd_solver.cpp:106] Iteration 61000, lr = 1e-10
I0429 09:49:22.934921   966 solver.cpp:228] Iteration 61100, loss = 0.222622
I0429 09:49:22.935103   966 solver.cpp:244]     Train net output #0: loss = 0.222622 (* 1 = 0.222622 loss)
I0429 09:49:22.935111   966 sgd_solver.cpp:106] Iteration 61100, lr = 1e-10
I0429 09:50:14.352282   966 solver.cpp:228] Iteration 61200, loss = 0.120705
I0429 09:50:14.352444   966 solver.cpp:244]     Train net output #0: loss = 0.120705 (* 1 = 0.120705 loss)
I0429 09:50:14.352452   966 sgd_solver.cpp:106] Iteration 61200, lr = 1e-10
I0429 09:51:03.963853   966 solver.cpp:228] Iteration 61300, loss = 0.288562
I0429 09:51:03.964007   966 solver.cpp:244]     Train net output #0: loss = 0.288562 (* 1 = 0.288562 loss)
I0429 09:51:03.964015   966 sgd_solver.cpp:106] Iteration 61300, lr = 1e-10
I0429 09:51:53.568246   966 solver.cpp:228] Iteration 61400, loss = 0.12335
I0429 09:51:53.570111   966 solver.cpp:244]     Train net output #0: loss = 0.12335 (* 1 = 0.12335 loss)
I0429 09:51:53.570133   966 sgd_solver.cpp:106] Iteration 61400, lr = 1e-10
I0429 09:52:43.187736   966 solver.cpp:228] Iteration 61500, loss = 0.111348
I0429 09:52:43.187897   966 solver.cpp:244]     Train net output #0: loss = 0.111348 (* 1 = 0.111348 loss)
I0429 09:52:43.187904   966 sgd_solver.cpp:106] Iteration 61500, lr = 1e-10
I0429 09:53:34.552898   966 solver.cpp:228] Iteration 61600, loss = 0.149435
I0429 09:53:34.554211   966 solver.cpp:244]     Train net output #0: loss = 0.149435 (* 1 = 0.149435 loss)
I0429 09:53:34.554219   966 sgd_solver.cpp:106] Iteration 61600, lr = 1e-10
I0429 09:54:24.167598   966 solver.cpp:228] Iteration 61700, loss = 0.106675
I0429 09:54:24.167809   966 solver.cpp:244]     Train net output #0: loss = 0.106675 (* 1 = 0.106675 loss)
I0429 09:54:24.167824   966 sgd_solver.cpp:106] Iteration 61700, lr = 1e-10
I0429 09:55:13.785879   966 solver.cpp:228] Iteration 61800, loss = 0.0946621
I0429 09:55:13.786041   966 solver.cpp:244]     Train net output #0: loss = 0.0946621 (* 1 = 0.0946621 loss)
I0429 09:55:13.786048   966 sgd_solver.cpp:106] Iteration 61800, lr = 1e-10
I0429 09:56:05.204339   966 solver.cpp:228] Iteration 61900, loss = 0.102097
I0429 09:56:05.204551   966 solver.cpp:244]     Train net output #0: loss = 0.102097 (* 1 = 0.102097 loss)
I0429 09:56:05.204560   966 sgd_solver.cpp:106] Iteration 61900, lr = 1e-10
I0429 09:56:54.493505   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_62000.caffemodel
I0429 09:57:05.291206   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_62000.solverstate
I0429 09:57:05.496692   966 solver.cpp:337] Iteration 62000, Testing net (#0)
I0429 09:57:05.496815   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 09:57:05.496821   966 net.cpp:693] Ignoring source layer visualize
I0429 09:57:05.496840   966 net.cpp:693] Ignoring source layer fake
I0429 10:01:58.229828   966 solver.cpp:404]     Test net output #0: loss = 0.17923 (* 1 = 0.17923 loss)
I0429 10:01:58.542732   966 solver.cpp:228] Iteration 62000, loss = 0.173197
I0429 10:01:58.542773   966 solver.cpp:244]     Train net output #0: loss = 0.173197 (* 1 = 0.173197 loss)
I0429 10:01:58.542778   966 sgd_solver.cpp:106] Iteration 62000, lr = 1e-10
I0429 10:02:48.137032   966 solver.cpp:228] Iteration 62100, loss = 0.163714
I0429 10:02:48.137197   966 solver.cpp:244]     Train net output #0: loss = 0.163714 (* 1 = 0.163714 loss)
I0429 10:02:48.137204   966 sgd_solver.cpp:106] Iteration 62100, lr = 1e-10
I0429 10:03:39.358047   966 solver.cpp:228] Iteration 62200, loss = 0.103484
I0429 10:03:39.358213   966 solver.cpp:244]     Train net output #0: loss = 0.103484 (* 1 = 0.103484 loss)
I0429 10:03:39.358220   966 sgd_solver.cpp:106] Iteration 62200, lr = 1e-10
I0429 10:04:28.961539   966 solver.cpp:228] Iteration 62300, loss = 0.125573
I0429 10:04:28.961700   966 solver.cpp:244]     Train net output #0: loss = 0.125573 (* 1 = 0.125573 loss)
I0429 10:04:28.961707   966 sgd_solver.cpp:106] Iteration 62300, lr = 1e-10
I0429 10:05:18.565834   966 solver.cpp:228] Iteration 62400, loss = 0.158582
I0429 10:05:18.565987   966 solver.cpp:244]     Train net output #0: loss = 0.158582 (* 1 = 0.158582 loss)
I0429 10:05:18.565994   966 sgd_solver.cpp:106] Iteration 62400, lr = 1e-10
I0429 10:06:09.411124   966 solver.cpp:228] Iteration 62500, loss = 0.132556
I0429 10:06:09.411293   966 solver.cpp:244]     Train net output #0: loss = 0.132556 (* 1 = 0.132556 loss)
I0429 10:06:09.411301   966 sgd_solver.cpp:106] Iteration 62500, lr = 1e-10
I0429 10:06:59.017897   966 solver.cpp:228] Iteration 62600, loss = 0.0999742
I0429 10:06:59.018060   966 solver.cpp:244]     Train net output #0: loss = 0.0999742 (* 1 = 0.0999742 loss)
I0429 10:06:59.018067   966 sgd_solver.cpp:106] Iteration 62600, lr = 1e-10
I0429 10:07:48.622329   966 solver.cpp:228] Iteration 62700, loss = 0.102395
I0429 10:07:48.622496   966 solver.cpp:244]     Train net output #0: loss = 0.102395 (* 1 = 0.102395 loss)
I0429 10:07:48.622503   966 sgd_solver.cpp:106] Iteration 62700, lr = 1e-10
I0429 10:08:39.573343   966 solver.cpp:228] Iteration 62800, loss = 0.178346
I0429 10:08:39.574518   966 solver.cpp:244]     Train net output #0: loss = 0.178346 (* 1 = 0.178346 loss)
I0429 10:08:39.574539   966 sgd_solver.cpp:106] Iteration 62800, lr = 1e-10
I0429 10:09:29.184232   966 solver.cpp:228] Iteration 62900, loss = 0.219942
I0429 10:09:29.184423   966 solver.cpp:244]     Train net output #0: loss = 0.219942 (* 1 = 0.219942 loss)
I0429 10:09:29.184430   966 sgd_solver.cpp:106] Iteration 62900, lr = 1e-10
I0429 10:10:20.008957   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_63000.caffemodel
I0429 10:10:45.736990   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_63000.solverstate
I0429 10:10:45.942998   966 solver.cpp:337] Iteration 63000, Testing net (#0)
I0429 10:10:45.943106   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 10:10:45.943112   966 net.cpp:693] Ignoring source layer visualize
I0429 10:10:45.943115   966 net.cpp:693] Ignoring source layer fake
I0429 10:15:38.092965   966 solver.cpp:404]     Test net output #0: loss = 0.183682 (* 1 = 0.183682 loss)
I0429 10:15:38.402935   966 solver.cpp:228] Iteration 63000, loss = 0.303622
I0429 10:15:38.402954   966 solver.cpp:244]     Train net output #0: loss = 0.303622 (* 1 = 0.303622 loss)
I0429 10:15:38.402976   966 sgd_solver.cpp:106] Iteration 63000, lr = 1e-10
I0429 10:16:28.017230   966 solver.cpp:228] Iteration 63100, loss = 0.103276
I0429 10:16:28.017468   966 solver.cpp:244]     Train net output #0: loss = 0.103276 (* 1 = 0.103276 loss)
I0429 10:16:28.017490   966 sgd_solver.cpp:106] Iteration 63100, lr = 1e-10
I0429 10:17:17.630028   966 solver.cpp:228] Iteration 63200, loss = 0.154872
I0429 10:17:17.630201   966 solver.cpp:244]     Train net output #0: loss = 0.154872 (* 1 = 0.154872 loss)
I0429 10:17:17.630208   966 sgd_solver.cpp:106] Iteration 63200, lr = 1e-10
I0429 10:18:09.022768   966 solver.cpp:228] Iteration 63300, loss = 0.216522
I0429 10:18:09.022931   966 solver.cpp:244]     Train net output #0: loss = 0.216522 (* 1 = 0.216522 loss)
I0429 10:18:09.022938   966 sgd_solver.cpp:106] Iteration 63300, lr = 1e-10
I0429 10:18:58.631527   966 solver.cpp:228] Iteration 63400, loss = 0.130556
I0429 10:18:58.631693   966 solver.cpp:244]     Train net output #0: loss = 0.130556 (* 1 = 0.130556 loss)
I0429 10:18:58.631700   966 sgd_solver.cpp:106] Iteration 63400, lr = 1e-10
I0429 10:19:48.242928   966 solver.cpp:228] Iteration 63500, loss = 0.103539
I0429 10:19:48.243110   966 solver.cpp:244]     Train net output #0: loss = 0.103539 (* 1 = 0.103539 loss)
I0429 10:19:48.243118   966 sgd_solver.cpp:106] Iteration 63500, lr = 1e-10
I0429 10:20:37.854586   966 solver.cpp:228] Iteration 63600, loss = 0.110122
I0429 10:20:37.854763   966 solver.cpp:244]     Train net output #0: loss = 0.110122 (* 1 = 0.110122 loss)
I0429 10:20:37.854769   966 sgd_solver.cpp:106] Iteration 63600, lr = 1e-10
I0429 10:21:29.320147   966 solver.cpp:228] Iteration 63700, loss = 0.113599
I0429 10:21:29.320313   966 solver.cpp:244]     Train net output #0: loss = 0.113599 (* 1 = 0.113599 loss)
I0429 10:21:29.320320   966 sgd_solver.cpp:106] Iteration 63700, lr = 1e-10
I0429 10:22:18.941913   966 solver.cpp:228] Iteration 63800, loss = 0.0534964
I0429 10:22:18.942078   966 solver.cpp:244]     Train net output #0: loss = 0.0534964 (* 1 = 0.0534964 loss)
I0429 10:22:18.942085   966 sgd_solver.cpp:106] Iteration 63800, lr = 1e-10
I0429 10:23:08.571779   966 solver.cpp:228] Iteration 63900, loss = 0.111159
I0429 10:23:08.571954   966 solver.cpp:244]     Train net output #0: loss = 0.111159 (* 1 = 0.111159 loss)
I0429 10:23:08.571961   966 sgd_solver.cpp:106] Iteration 63900, lr = 1e-10
I0429 10:23:59.784848   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_64000.caffemodel
I0429 10:24:10.282393   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_64000.solverstate
I0429 10:24:10.480268   966 solver.cpp:337] Iteration 64000, Testing net (#0)
I0429 10:24:10.480396   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 10:24:10.480401   966 net.cpp:693] Ignoring source layer visualize
I0429 10:24:10.480403   966 net.cpp:693] Ignoring source layer fake
I0429 10:29:03.344162   966 solver.cpp:404]     Test net output #0: loss = 0.191943 (* 1 = 0.191943 loss)
I0429 10:29:03.654193   966 solver.cpp:228] Iteration 64000, loss = 0.122417
I0429 10:29:03.654211   966 solver.cpp:244]     Train net output #0: loss = 0.122417 (* 1 = 0.122417 loss)
I0429 10:29:03.654232   966 sgd_solver.cpp:106] Iteration 64000, lr = 1e-10
I0429 10:29:53.272460   966 solver.cpp:228] Iteration 64100, loss = 0.214421
I0429 10:29:53.272634   966 solver.cpp:244]     Train net output #0: loss = 0.214421 (* 1 = 0.214421 loss)
I0429 10:29:53.272640   966 sgd_solver.cpp:106] Iteration 64100, lr = 1e-10
I0429 10:30:42.881125   966 solver.cpp:228] Iteration 64200, loss = 0.227402
I0429 10:30:42.881290   966 solver.cpp:244]     Train net output #0: loss = 0.227402 (* 1 = 0.227402 loss)
I0429 10:30:42.881297   966 sgd_solver.cpp:106] Iteration 64200, lr = 1e-10
I0429 10:31:32.492185   966 solver.cpp:228] Iteration 64300, loss = 0.231242
I0429 10:31:32.493428   966 solver.cpp:244]     Train net output #0: loss = 0.231242 (* 1 = 0.231242 loss)
I0429 10:31:32.493455   966 sgd_solver.cpp:106] Iteration 64300, lr = 1e-10
I0429 10:32:23.952337   966 solver.cpp:228] Iteration 64400, loss = 0.149653
I0429 10:32:23.952522   966 solver.cpp:244]     Train net output #0: loss = 0.149653 (* 1 = 0.149653 loss)
I0429 10:32:23.952529   966 sgd_solver.cpp:106] Iteration 64400, lr = 1e-10
I0429 10:33:13.561233   966 solver.cpp:228] Iteration 64500, loss = 0.108013
I0429 10:33:13.561391   966 solver.cpp:244]     Train net output #0: loss = 0.108013 (* 1 = 0.108013 loss)
I0429 10:33:13.561398   966 sgd_solver.cpp:106] Iteration 64500, lr = 1e-10
I0429 10:34:03.163605   966 solver.cpp:228] Iteration 64600, loss = 0.0721916
I0429 10:34:03.163758   966 solver.cpp:244]     Train net output #0: loss = 0.0721916 (* 1 = 0.0721916 loss)
I0429 10:34:03.163764   966 sgd_solver.cpp:106] Iteration 64600, lr = 1e-10
I0429 10:34:54.601276   966 solver.cpp:228] Iteration 64700, loss = 0.102463
I0429 10:34:54.601449   966 solver.cpp:244]     Train net output #0: loss = 0.102463 (* 1 = 0.102463 loss)
I0429 10:34:54.601457   966 sgd_solver.cpp:106] Iteration 64700, lr = 1e-10
I0429 10:35:44.218952   966 solver.cpp:228] Iteration 64800, loss = 0.101227
I0429 10:35:44.219451   966 solver.cpp:244]     Train net output #0: loss = 0.101227 (* 1 = 0.101227 loss)
I0429 10:35:44.219466   966 sgd_solver.cpp:106] Iteration 64800, lr = 1e-10
I0429 10:36:33.829396   966 solver.cpp:228] Iteration 64900, loss = 0.074938
I0429 10:36:33.829598   966 solver.cpp:244]     Train net output #0: loss = 0.074938 (* 1 = 0.074938 loss)
I0429 10:36:33.829605   966 sgd_solver.cpp:106] Iteration 64900, lr = 1e-10
I0429 10:37:24.946812   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_65000.caffemodel
I0429 10:37:30.851853   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_65000.solverstate
I0429 10:37:31.058907   966 solver.cpp:337] Iteration 65000, Testing net (#0)
I0429 10:37:31.059051   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 10:37:31.059056   966 net.cpp:693] Ignoring source layer visualize
I0429 10:37:31.059059   966 net.cpp:693] Ignoring source layer fake
I0429 10:42:23.070333   966 solver.cpp:404]     Test net output #0: loss = 0.183669 (* 1 = 0.183669 loss)
I0429 10:42:23.379906   966 solver.cpp:228] Iteration 65000, loss = 0.0265425
I0429 10:42:23.379923   966 solver.cpp:244]     Train net output #0: loss = 0.0265425 (* 1 = 0.0265425 loss)
I0429 10:42:23.379945   966 sgd_solver.cpp:106] Iteration 65000, lr = 1e-10
I0429 10:43:12.983758   966 solver.cpp:228] Iteration 65100, loss = 0.152625
I0429 10:43:12.983944   966 solver.cpp:244]     Train net output #0: loss = 0.152625 (* 1 = 0.152625 loss)
I0429 10:43:12.983952   966 sgd_solver.cpp:106] Iteration 65100, lr = 1e-10
I0429 10:44:02.590466   966 solver.cpp:228] Iteration 65200, loss = 0.112523
I0429 10:44:02.590631   966 solver.cpp:244]     Train net output #0: loss = 0.112523 (* 1 = 0.112523 loss)
I0429 10:44:02.590638   966 sgd_solver.cpp:106] Iteration 65200, lr = 1e-10
I0429 10:44:52.187096   966 solver.cpp:228] Iteration 65300, loss = 0.0990193
I0429 10:44:52.187257   966 solver.cpp:244]     Train net output #0: loss = 0.0990193 (* 1 = 0.0990193 loss)
I0429 10:44:52.187263   966 sgd_solver.cpp:106] Iteration 65300, lr = 1e-10
I0429 10:45:43.480320   966 solver.cpp:228] Iteration 65400, loss = 0.0926605
I0429 10:45:43.480450   966 solver.cpp:244]     Train net output #0: loss = 0.0926605 (* 1 = 0.0926605 loss)
I0429 10:45:43.480458   966 sgd_solver.cpp:106] Iteration 65400, lr = 1e-10
I0429 10:46:33.083201   966 solver.cpp:228] Iteration 65500, loss = 0.0958204
I0429 10:46:33.083356   966 solver.cpp:244]     Train net output #0: loss = 0.0958204 (* 1 = 0.0958204 loss)
I0429 10:46:33.083364   966 sgd_solver.cpp:106] Iteration 65500, lr = 1e-10
I0429 10:47:22.674659   966 solver.cpp:228] Iteration 65600, loss = 0.100318
I0429 10:47:22.674821   966 solver.cpp:244]     Train net output #0: loss = 0.100318 (* 1 = 0.100318 loss)
I0429 10:47:22.674829   966 sgd_solver.cpp:106] Iteration 65600, lr = 1e-10
I0429 10:48:13.579591   966 solver.cpp:228] Iteration 65700, loss = 0.112939
I0429 10:48:13.580317   966 solver.cpp:244]     Train net output #0: loss = 0.112939 (* 1 = 0.112939 loss)
I0429 10:48:13.580324   966 sgd_solver.cpp:106] Iteration 65700, lr = 1e-10
I0429 10:49:03.172216   966 solver.cpp:228] Iteration 65800, loss = 0.0864042
I0429 10:49:03.172435   966 solver.cpp:244]     Train net output #0: loss = 0.0864042 (* 1 = 0.0864042 loss)
I0429 10:49:03.172451   966 sgd_solver.cpp:106] Iteration 65800, lr = 1e-10
I0429 10:49:54.554006   966 solver.cpp:228] Iteration 65900, loss = 0.194701
I0429 10:49:54.554172   966 solver.cpp:244]     Train net output #0: loss = 0.194701 (* 1 = 0.194701 loss)
I0429 10:49:54.554177   966 sgd_solver.cpp:106] Iteration 65900, lr = 1e-10
I0429 10:50:43.845219   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_66000.caffemodel
I0429 10:50:49.544541   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_66000.solverstate
I0429 10:50:49.734598   966 solver.cpp:337] Iteration 66000, Testing net (#0)
I0429 10:50:49.734721   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 10:50:49.734727   966 net.cpp:693] Ignoring source layer visualize
I0429 10:50:49.734730   966 net.cpp:693] Ignoring source layer fake
I0429 10:55:42.735497   966 solver.cpp:404]     Test net output #0: loss = 0.182968 (* 1 = 0.182968 loss)
I0429 10:55:43.047222   966 solver.cpp:228] Iteration 66000, loss = 0.174791
I0429 10:55:43.047245   966 solver.cpp:244]     Train net output #0: loss = 0.174791 (* 1 = 0.174791 loss)
I0429 10:55:43.047266   966 sgd_solver.cpp:106] Iteration 66000, lr = 1e-10
I0429 10:56:32.652089   966 solver.cpp:228] Iteration 66100, loss = 0.0816162
I0429 10:56:32.652813   966 solver.cpp:244]     Train net output #0: loss = 0.0816162 (* 1 = 0.0816162 loss)
I0429 10:56:32.652820   966 sgd_solver.cpp:106] Iteration 66100, lr = 1e-10
I0429 10:57:23.846791   966 solver.cpp:228] Iteration 66200, loss = 0.119769
I0429 10:57:23.846954   966 solver.cpp:244]     Train net output #0: loss = 0.119769 (* 1 = 0.119769 loss)
I0429 10:57:23.846961   966 sgd_solver.cpp:106] Iteration 66200, lr = 1e-10
I0429 10:58:13.444147   966 solver.cpp:228] Iteration 66300, loss = 0.173303
I0429 10:58:13.444300   966 solver.cpp:244]     Train net output #0: loss = 0.173303 (* 1 = 0.173303 loss)
I0429 10:58:13.444306   966 sgd_solver.cpp:106] Iteration 66300, lr = 1e-10
I0429 10:59:03.045177   966 solver.cpp:228] Iteration 66400, loss = 0.193828
I0429 10:59:03.045361   966 solver.cpp:244]     Train net output #0: loss = 0.193828 (* 1 = 0.193828 loss)
I0429 10:59:03.045368   966 sgd_solver.cpp:106] Iteration 66400, lr = 1e-10
I0429 10:59:54.463654   966 solver.cpp:228] Iteration 66500, loss = 0.144675
I0429 10:59:54.463815   966 solver.cpp:244]     Train net output #0: loss = 0.144675 (* 1 = 0.144675 loss)
I0429 10:59:54.463824   966 sgd_solver.cpp:106] Iteration 66500, lr = 1e-10
I0429 11:00:44.079749   966 solver.cpp:228] Iteration 66600, loss = 0.210514
I0429 11:00:44.079911   966 solver.cpp:244]     Train net output #0: loss = 0.210514 (* 1 = 0.210514 loss)
I0429 11:00:44.079917   966 sgd_solver.cpp:106] Iteration 66600, lr = 1e-10
I0429 11:01:33.684365   966 solver.cpp:228] Iteration 66700, loss = 0.0931448
I0429 11:01:33.684528   966 solver.cpp:244]     Train net output #0: loss = 0.0931448 (* 1 = 0.0931448 loss)
I0429 11:01:33.684536   966 sgd_solver.cpp:106] Iteration 66700, lr = 1e-10
I0429 11:02:25.165798   966 solver.cpp:228] Iteration 66800, loss = 0.19291
I0429 11:02:25.165947   966 solver.cpp:244]     Train net output #0: loss = 0.19291 (* 1 = 0.19291 loss)
I0429 11:02:25.165954   966 sgd_solver.cpp:106] Iteration 66800, lr = 1e-10
I0429 11:03:14.794746   966 solver.cpp:228] Iteration 66900, loss = 0.160764
I0429 11:03:14.794998   966 solver.cpp:244]     Train net output #0: loss = 0.160764 (* 1 = 0.160764 loss)
I0429 11:03:14.795016   966 sgd_solver.cpp:106] Iteration 66900, lr = 1e-10
I0429 11:04:04.154891   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_67000.caffemodel
I0429 11:04:35.898463   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_67000.solverstate
I0429 11:04:36.104020   966 solver.cpp:337] Iteration 67000, Testing net (#0)
I0429 11:04:36.104145   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 11:04:36.104164   966 net.cpp:693] Ignoring source layer visualize
I0429 11:04:36.104167   966 net.cpp:693] Ignoring source layer fake
I0429 11:09:29.312666   966 solver.cpp:404]     Test net output #0: loss = 0.178835 (* 1 = 0.178835 loss)
I0429 11:09:29.622426   966 solver.cpp:228] Iteration 67000, loss = 0.107428
I0429 11:09:29.622462   966 solver.cpp:244]     Train net output #0: loss = 0.107428 (* 1 = 0.107428 loss)
I0429 11:09:29.622468   966 sgd_solver.cpp:106] Iteration 67000, lr = 1e-10
I0429 11:10:21.107161   966 solver.cpp:228] Iteration 67100, loss = 0.199514
I0429 11:10:21.107345   966 solver.cpp:244]     Train net output #0: loss = 0.199514 (* 1 = 0.199514 loss)
I0429 11:10:21.107353   966 sgd_solver.cpp:106] Iteration 67100, lr = 1e-10
I0429 11:11:10.712869   966 solver.cpp:228] Iteration 67200, loss = 0.172031
I0429 11:11:10.713032   966 solver.cpp:244]     Train net output #0: loss = 0.172031 (* 1 = 0.172031 loss)
I0429 11:11:10.713038   966 sgd_solver.cpp:106] Iteration 67200, lr = 1e-10
I0429 11:12:00.312927   966 solver.cpp:228] Iteration 67300, loss = 0.13635
I0429 11:12:00.313086   966 solver.cpp:244]     Train net output #0: loss = 0.13635 (* 1 = 0.13635 loss)
I0429 11:12:00.313091   966 sgd_solver.cpp:106] Iteration 67300, lr = 1e-10
I0429 11:12:49.928941   966 solver.cpp:228] Iteration 67400, loss = 0.258398
I0429 11:12:49.929133   966 solver.cpp:244]     Train net output #0: loss = 0.258398 (* 1 = 0.258398 loss)
I0429 11:12:49.929141   966 sgd_solver.cpp:106] Iteration 67400, lr = 1e-10
I0429 11:13:41.381947   966 solver.cpp:228] Iteration 67500, loss = 0.139307
I0429 11:13:41.382107   966 solver.cpp:244]     Train net output #0: loss = 0.139307 (* 1 = 0.139307 loss)
I0429 11:13:41.382113   966 sgd_solver.cpp:106] Iteration 67500, lr = 1e-11
I0429 11:14:30.975282   966 solver.cpp:228] Iteration 67600, loss = 0.0974355
I0429 11:14:30.975450   966 solver.cpp:244]     Train net output #0: loss = 0.0974355 (* 1 = 0.0974355 loss)
I0429 11:14:30.975457   966 sgd_solver.cpp:106] Iteration 67600, lr = 1e-11
I0429 11:15:20.584131   966 solver.cpp:228] Iteration 67700, loss = 0.0997639
I0429 11:15:20.584306   966 solver.cpp:244]     Train net output #0: loss = 0.0997639 (* 1 = 0.0997639 loss)
I0429 11:15:20.584313   966 sgd_solver.cpp:106] Iteration 67700, lr = 1e-11
I0429 11:16:11.993388   966 solver.cpp:228] Iteration 67800, loss = 0.079841
I0429 11:16:11.993551   966 solver.cpp:244]     Train net output #0: loss = 0.079841 (* 1 = 0.079841 loss)
I0429 11:16:11.993556   966 sgd_solver.cpp:106] Iteration 67800, lr = 1e-11
I0429 11:17:01.592988   966 solver.cpp:228] Iteration 67900, loss = 0.101677
I0429 11:17:01.593124   966 solver.cpp:244]     Train net output #0: loss = 0.101677 (* 1 = 0.101677 loss)
I0429 11:17:01.593132   966 sgd_solver.cpp:106] Iteration 67900, lr = 1e-11
I0429 11:17:50.930737   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_68000.caffemodel
I0429 11:17:59.040202   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_68000.solverstate
I0429 11:17:59.243309   966 solver.cpp:337] Iteration 68000, Testing net (#0)
I0429 11:17:59.243448   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 11:17:59.243453   966 net.cpp:693] Ignoring source layer visualize
I0429 11:17:59.243455   966 net.cpp:693] Ignoring source layer fake
I0429 11:22:52.077246   966 solver.cpp:404]     Test net output #0: loss = 0.184777 (* 1 = 0.184777 loss)
I0429 11:22:52.388240   966 solver.cpp:228] Iteration 68000, loss = 0.120108
I0429 11:22:52.388279   966 solver.cpp:244]     Train net output #0: loss = 0.120108 (* 1 = 0.120108 loss)
I0429 11:22:52.388285   966 sgd_solver.cpp:106] Iteration 68000, lr = 1e-11
I0429 11:23:41.990877   966 solver.cpp:228] Iteration 68100, loss = 0.204275
I0429 11:23:41.991041   966 solver.cpp:244]     Train net output #0: loss = 0.204275 (* 1 = 0.204275 loss)
I0429 11:23:41.991047   966 sgd_solver.cpp:106] Iteration 68100, lr = 1e-11
I0429 11:24:34.657214   966 solver.cpp:228] Iteration 68200, loss = 0.108806
I0429 11:24:34.657366   966 solver.cpp:244]     Train net output #0: loss = 0.108806 (* 1 = 0.108806 loss)
I0429 11:24:34.657373   966 sgd_solver.cpp:106] Iteration 68200, lr = 1e-11
I0429 11:25:24.244298   966 solver.cpp:228] Iteration 68300, loss = 0.104443
I0429 11:25:24.244457   966 solver.cpp:244]     Train net output #0: loss = 0.104443 (* 1 = 0.104443 loss)
I0429 11:25:24.244463   966 sgd_solver.cpp:106] Iteration 68300, lr = 1e-11
I0429 11:26:13.841025   966 solver.cpp:228] Iteration 68400, loss = 0.134028
I0429 11:26:13.841172   966 solver.cpp:244]     Train net output #0: loss = 0.134028 (* 1 = 0.134028 loss)
I0429 11:26:13.841179   966 sgd_solver.cpp:106] Iteration 68400, lr = 1e-11
I0429 11:27:05.783362   966 solver.cpp:228] Iteration 68500, loss = 0.2065
I0429 11:27:05.783515   966 solver.cpp:244]     Train net output #0: loss = 0.2065 (* 1 = 0.2065 loss)
I0429 11:27:05.783522   966 sgd_solver.cpp:106] Iteration 68500, lr = 1e-11
I0429 11:27:55.379973   966 solver.cpp:228] Iteration 68600, loss = 0.13639
I0429 11:27:55.380101   966 solver.cpp:244]     Train net output #0: loss = 0.13639 (* 1 = 0.13639 loss)
I0429 11:27:55.380107   966 sgd_solver.cpp:106] Iteration 68600, lr = 1e-11
I0429 11:28:44.981276   966 solver.cpp:228] Iteration 68700, loss = 0.140979
I0429 11:28:44.981448   966 solver.cpp:244]     Train net output #0: loss = 0.140979 (* 1 = 0.140979 loss)
I0429 11:28:44.981456   966 sgd_solver.cpp:106] Iteration 68700, lr = 1e-11
I0429 11:29:36.425117   966 solver.cpp:228] Iteration 68800, loss = 0.105537
I0429 11:29:36.425283   966 solver.cpp:244]     Train net output #0: loss = 0.105537 (* 1 = 0.105537 loss)
I0429 11:29:36.425292   966 sgd_solver.cpp:106] Iteration 68800, lr = 1e-11
I0429 11:30:26.022095   966 solver.cpp:228] Iteration 68900, loss = 0.0926647
I0429 11:30:26.022256   966 solver.cpp:244]     Train net output #0: loss = 0.0926647 (* 1 = 0.0926647 loss)
I0429 11:30:26.022264   966 sgd_solver.cpp:106] Iteration 68900, lr = 1e-11
I0429 11:31:17.396064   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_69000.caffemodel
I0429 11:31:27.849125   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_69000.solverstate
I0429 11:31:28.052744   966 solver.cpp:337] Iteration 69000, Testing net (#0)
I0429 11:31:28.052875   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 11:31:28.052881   966 net.cpp:693] Ignoring source layer visualize
I0429 11:31:28.052899   966 net.cpp:693] Ignoring source layer fake
I0429 11:36:21.006961   966 solver.cpp:404]     Test net output #0: loss = 0.191728 (* 1 = 0.191728 loss)
I0429 11:36:21.316766   966 solver.cpp:228] Iteration 69000, loss = 0.141448
I0429 11:36:21.316786   966 solver.cpp:244]     Train net output #0: loss = 0.141448 (* 1 = 0.141448 loss)
I0429 11:36:21.316812   966 sgd_solver.cpp:106] Iteration 69000, lr = 1e-11
I0429 11:37:10.925554   966 solver.cpp:228] Iteration 69100, loss = 0.233673
I0429 11:37:10.925719   966 solver.cpp:244]     Train net output #0: loss = 0.233673 (* 1 = 0.233673 loss)
I0429 11:37:10.925725   966 sgd_solver.cpp:106] Iteration 69100, lr = 1e-11
I0429 11:38:00.526944   966 solver.cpp:228] Iteration 69200, loss = 0.226423
I0429 11:38:00.527107   966 solver.cpp:244]     Train net output #0: loss = 0.226423 (* 1 = 0.226423 loss)
I0429 11:38:00.527112   966 sgd_solver.cpp:106] Iteration 69200, lr = 1e-11
I0429 11:38:52.858245   966 solver.cpp:228] Iteration 69300, loss = 0.13339
I0429 11:38:52.858402   966 solver.cpp:244]     Train net output #0: loss = 0.13339 (* 1 = 0.13339 loss)
I0429 11:38:52.858409   966 sgd_solver.cpp:106] Iteration 69300, lr = 1e-11
I0429 11:39:42.457106   966 solver.cpp:228] Iteration 69400, loss = 0.0668606
I0429 11:39:42.457286   966 solver.cpp:244]     Train net output #0: loss = 0.0668606 (* 1 = 0.0668606 loss)
I0429 11:39:42.457294   966 sgd_solver.cpp:106] Iteration 69400, lr = 1e-11
I0429 11:40:32.057070   966 solver.cpp:228] Iteration 69500, loss = 0.120951
I0429 11:40:32.057235   966 solver.cpp:244]     Train net output #0: loss = 0.120951 (* 1 = 0.120951 loss)
I0429 11:40:32.057241   966 sgd_solver.cpp:106] Iteration 69500, lr = 1e-11
I0429 11:41:25.212584   966 solver.cpp:228] Iteration 69600, loss = 0.216962
I0429 11:41:25.212735   966 solver.cpp:244]     Train net output #0: loss = 0.216962 (* 1 = 0.216962 loss)
I0429 11:41:25.212743   966 sgd_solver.cpp:106] Iteration 69600, lr = 1e-11
I0429 11:42:14.814121   966 solver.cpp:228] Iteration 69700, loss = 0.124149
I0429 11:42:14.816166   966 solver.cpp:244]     Train net output #0: loss = 0.124149 (* 1 = 0.124149 loss)
I0429 11:42:14.816172   966 sgd_solver.cpp:106] Iteration 69700, lr = 1e-11
I0429 11:43:04.418599   966 solver.cpp:228] Iteration 69800, loss = 0.168128
I0429 11:43:04.418758   966 solver.cpp:244]     Train net output #0: loss = 0.168128 (* 1 = 0.168128 loss)
I0429 11:43:04.418766   966 sgd_solver.cpp:106] Iteration 69800, lr = 1e-11
I0429 11:43:56.964663   966 solver.cpp:228] Iteration 69900, loss = 0.0980778
I0429 11:43:56.964831   966 solver.cpp:244]     Train net output #0: loss = 0.0980778 (* 1 = 0.0980778 loss)
I0429 11:43:56.964838   966 sgd_solver.cpp:106] Iteration 69900, lr = 1e-11
I0429 11:44:46.256516   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_70000.caffemodel
I0429 11:45:00.832785   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_70000.solverstate
I0429 11:45:01.030025   966 solver.cpp:337] Iteration 70000, Testing net (#0)
I0429 11:45:01.030153   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 11:45:01.030175   966 net.cpp:693] Ignoring source layer visualize
I0429 11:45:01.030179   966 net.cpp:693] Ignoring source layer fake
I0429 11:49:53.833109   966 solver.cpp:404]     Test net output #0: loss = 0.183164 (* 1 = 0.183164 loss)
I0429 11:49:54.142009   966 solver.cpp:228] Iteration 70000, loss = 0.0850562
I0429 11:49:54.142047   966 solver.cpp:244]     Train net output #0: loss = 0.0850562 (* 1 = 0.0850562 loss)
I0429 11:49:54.142055   966 sgd_solver.cpp:106] Iteration 70000, lr = 1e-11
I0429 11:50:43.690418   966 solver.cpp:228] Iteration 70100, loss = 0.164223
I0429 11:50:43.690589   966 solver.cpp:244]     Train net output #0: loss = 0.164223 (* 1 = 0.164223 loss)
I0429 11:50:43.690598   966 sgd_solver.cpp:106] Iteration 70100, lr = 1e-11
I0429 11:51:33.244346   966 solver.cpp:228] Iteration 70200, loss = 0.148055
I0429 11:51:33.244513   966 solver.cpp:244]     Train net output #0: loss = 0.148055 (* 1 = 0.148055 loss)
I0429 11:51:33.244519   966 sgd_solver.cpp:106] Iteration 70200, lr = 1e-11
I0429 11:52:27.726471   966 solver.cpp:228] Iteration 70300, loss = 0.226977
I0429 11:52:27.726627   966 solver.cpp:244]     Train net output #0: loss = 0.226977 (* 1 = 0.226977 loss)
I0429 11:52:27.726634   966 sgd_solver.cpp:106] Iteration 70300, lr = 1e-11
I0429 11:53:17.261364   966 solver.cpp:228] Iteration 70400, loss = 0.0755377
I0429 11:53:17.261529   966 solver.cpp:244]     Train net output #0: loss = 0.0755377 (* 1 = 0.0755377 loss)
I0429 11:53:17.261538   966 sgd_solver.cpp:106] Iteration 70400, lr = 1e-11
I0429 11:54:06.807646   966 solver.cpp:228] Iteration 70500, loss = 0.0999985
I0429 11:54:06.807806   966 solver.cpp:244]     Train net output #0: loss = 0.0999985 (* 1 = 0.0999985 loss)
I0429 11:54:06.807814   966 sgd_solver.cpp:106] Iteration 70500, lr = 1e-11
I0429 11:54:59.263243   966 solver.cpp:228] Iteration 70600, loss = 0.121068
I0429 11:54:59.263406   966 solver.cpp:244]     Train net output #0: loss = 0.121068 (* 1 = 0.121068 loss)
I0429 11:54:59.263415   966 sgd_solver.cpp:106] Iteration 70600, lr = 1e-11
I0429 11:55:48.795586   966 solver.cpp:228] Iteration 70700, loss = 0.0841881
I0429 11:55:48.795747   966 solver.cpp:244]     Train net output #0: loss = 0.0841881 (* 1 = 0.0841881 loss)
I0429 11:55:48.795754   966 sgd_solver.cpp:106] Iteration 70700, lr = 1e-11
I0429 11:56:38.345551   966 solver.cpp:228] Iteration 70800, loss = 0.166026
I0429 11:56:38.345676   966 solver.cpp:244]     Train net output #0: loss = 0.166026 (* 1 = 0.166026 loss)
I0429 11:56:38.345684   966 sgd_solver.cpp:106] Iteration 70800, lr = 1e-11
I0429 11:57:30.567359   966 solver.cpp:228] Iteration 70900, loss = 0.167021
I0429 11:57:30.567518   966 solver.cpp:244]     Train net output #0: loss = 0.167021 (* 1 = 0.167021 loss)
I0429 11:57:30.567525   966 sgd_solver.cpp:106] Iteration 70900, lr = 1e-11
I0429 11:58:19.778098   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_71000.caffemodel
I0429 11:58:52.254231   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_71000.solverstate
I0429 11:58:52.556787   966 solver.cpp:337] Iteration 71000, Testing net (#0)
I0429 11:58:52.556910   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 11:58:52.556932   966 net.cpp:693] Ignoring source layer visualize
I0429 11:58:52.556934   966 net.cpp:693] Ignoring source layer fake
I0429 12:03:45.448207   966 solver.cpp:404]     Test net output #0: loss = 0.182656 (* 1 = 0.182656 loss)
I0429 12:03:45.759793   966 solver.cpp:228] Iteration 71000, loss = 0.127491
I0429 12:03:45.759812   966 solver.cpp:244]     Train net output #0: loss = 0.127491 (* 1 = 0.127491 loss)
I0429 12:03:45.759835   966 sgd_solver.cpp:106] Iteration 71000, lr = 1e-11
I0429 12:04:35.284157   966 solver.cpp:228] Iteration 71100, loss = 0.109604
I0429 12:04:35.284345   966 solver.cpp:244]     Train net output #0: loss = 0.109604 (* 1 = 0.109604 loss)
I0429 12:04:35.284354   966 sgd_solver.cpp:106] Iteration 71100, lr = 1e-11
I0429 12:05:24.810344   966 solver.cpp:228] Iteration 71200, loss = 0.0714576
I0429 12:05:24.810540   966 solver.cpp:244]     Train net output #0: loss = 0.0714576 (* 1 = 0.0714576 loss)
I0429 12:05:24.810549   966 sgd_solver.cpp:106] Iteration 71200, lr = 1e-11
I0429 12:06:17.431452   966 solver.cpp:228] Iteration 71300, loss = 0.108442
I0429 12:06:17.431609   966 solver.cpp:244]     Train net output #0: loss = 0.108442 (* 1 = 0.108442 loss)
I0429 12:06:17.431617   966 sgd_solver.cpp:106] Iteration 71300, lr = 1e-11
I0429 12:07:06.940788   966 solver.cpp:228] Iteration 71400, loss = 0.0908053
I0429 12:07:06.940956   966 solver.cpp:244]     Train net output #0: loss = 0.0908053 (* 1 = 0.0908053 loss)
I0429 12:07:06.940964   966 sgd_solver.cpp:106] Iteration 71400, lr = 1e-11
I0429 12:07:56.453629   966 solver.cpp:228] Iteration 71500, loss = 0.122238
I0429 12:07:56.453758   966 solver.cpp:244]     Train net output #0: loss = 0.122238 (* 1 = 0.122238 loss)
I0429 12:07:56.453765   966 sgd_solver.cpp:106] Iteration 71500, lr = 1e-11
I0429 12:08:48.441985   966 solver.cpp:228] Iteration 71600, loss = 0.11467
I0429 12:08:48.442167   966 solver.cpp:244]     Train net output #0: loss = 0.11467 (* 1 = 0.11467 loss)
I0429 12:08:48.442174   966 sgd_solver.cpp:106] Iteration 71600, lr = 1e-11
I0429 12:09:37.942005   966 solver.cpp:228] Iteration 71700, loss = 0.181978
I0429 12:09:37.942172   966 solver.cpp:244]     Train net output #0: loss = 0.181978 (* 1 = 0.181978 loss)
I0429 12:09:37.942178   966 sgd_solver.cpp:106] Iteration 71700, lr = 1e-11
I0429 12:10:27.467161   966 solver.cpp:228] Iteration 71800, loss = 0.094188
I0429 12:10:27.467311   966 solver.cpp:244]     Train net output #0: loss = 0.094188 (* 1 = 0.094188 loss)
I0429 12:10:27.467319   966 sgd_solver.cpp:106] Iteration 71800, lr = 1e-11
I0429 12:11:19.042100   966 solver.cpp:228] Iteration 71900, loss = 0.165169
I0429 12:11:19.042273   966 solver.cpp:244]     Train net output #0: loss = 0.165169 (* 1 = 0.165169 loss)
I0429 12:11:19.042279   966 sgd_solver.cpp:106] Iteration 71900, lr = 1e-11
I0429 12:12:08.243576   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_72000.caffemodel
I0429 12:12:32.790391   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_72000.solverstate
I0429 12:12:33.087460   966 solver.cpp:337] Iteration 72000, Testing net (#0)
I0429 12:12:33.087586   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 12:12:33.087607   966 net.cpp:693] Ignoring source layer visualize
I0429 12:12:33.087610   966 net.cpp:693] Ignoring source layer fake
I0429 12:17:25.181177   966 solver.cpp:404]     Test net output #0: loss = 0.178669 (* 1 = 0.178669 loss)
I0429 12:17:25.490517   966 solver.cpp:228] Iteration 72000, loss = 0.0884815
I0429 12:17:25.490555   966 solver.cpp:244]     Train net output #0: loss = 0.0884815 (* 1 = 0.0884815 loss)
I0429 12:17:25.490562   966 sgd_solver.cpp:106] Iteration 72000, lr = 1e-11
I0429 12:18:15.019804   966 solver.cpp:228] Iteration 72100, loss = 0.131382
I0429 12:18:15.019971   966 solver.cpp:244]     Train net output #0: loss = 0.131382 (* 1 = 0.131382 loss)
I0429 12:18:15.019979   966 sgd_solver.cpp:106] Iteration 72100, lr = 1e-11
I0429 12:19:07.612504   966 solver.cpp:228] Iteration 72200, loss = 0.0854711
I0429 12:19:07.612711   966 solver.cpp:244]     Train net output #0: loss = 0.0854711 (* 1 = 0.0854711 loss)
I0429 12:19:07.612720   966 sgd_solver.cpp:106] Iteration 72200, lr = 1e-11
I0429 12:19:57.137904   966 solver.cpp:228] Iteration 72300, loss = 0.169112
I0429 12:19:57.138068   966 solver.cpp:244]     Train net output #0: loss = 0.169112 (* 1 = 0.169112 loss)
I0429 12:19:57.138077   966 sgd_solver.cpp:106] Iteration 72300, lr = 1e-11
I0429 12:20:55.891675   966 solver.cpp:228] Iteration 72400, loss = 0.108064
I0429 12:20:55.891878   966 solver.cpp:244]     Train net output #0: loss = 0.108064 (* 1 = 0.108064 loss)
I0429 12:20:55.891887   966 sgd_solver.cpp:106] Iteration 72400, lr = 1e-11
I0429 12:21:45.338069   966 solver.cpp:228] Iteration 72500, loss = 0.226037
I0429 12:21:45.338289   966 solver.cpp:244]     Train net output #0: loss = 0.226037 (* 1 = 0.226037 loss)
I0429 12:21:45.338297   966 sgd_solver.cpp:106] Iteration 72500, lr = 1e-11
I0429 12:22:34.882342   966 solver.cpp:228] Iteration 72600, loss = 0.0803338
I0429 12:22:34.882504   966 solver.cpp:244]     Train net output #0: loss = 0.0803338 (* 1 = 0.0803338 loss)
I0429 12:22:34.882513   966 sgd_solver.cpp:106] Iteration 72600, lr = 1e-11
I0429 12:23:34.538404   966 solver.cpp:228] Iteration 72700, loss = 0.173585
I0429 12:23:34.538556   966 solver.cpp:244]     Train net output #0: loss = 0.173585 (* 1 = 0.173585 loss)
I0429 12:23:34.538564   966 sgd_solver.cpp:106] Iteration 72700, lr = 1e-11
I0429 12:24:23.990526   966 solver.cpp:228] Iteration 72800, loss = 0.144147
I0429 12:24:23.990694   966 solver.cpp:244]     Train net output #0: loss = 0.144147 (* 1 = 0.144147 loss)
I0429 12:24:23.990701   966 sgd_solver.cpp:106] Iteration 72800, lr = 1e-11
I0429 12:25:13.537907   966 solver.cpp:228] Iteration 72900, loss = 0.247861
I0429 12:25:13.538082   966 solver.cpp:244]     Train net output #0: loss = 0.247861 (* 1 = 0.247861 loss)
I0429 12:25:13.538091   966 sgd_solver.cpp:106] Iteration 72900, lr = 1e-11
I0429 12:26:13.863785   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_73000.caffemodel
I0429 12:26:24.097697   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_73000.solverstate
I0429 12:26:24.321465   966 solver.cpp:337] Iteration 73000, Testing net (#0)
I0429 12:26:24.321560   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 12:26:24.321568   966 net.cpp:693] Ignoring source layer visualize
I0429 12:26:24.321570   966 net.cpp:693] Ignoring source layer fake
I0429 12:31:16.831866   966 solver.cpp:404]     Test net output #0: loss = 0.18676 (* 1 = 0.18676 loss)
I0429 12:31:17.142675   966 solver.cpp:228] Iteration 73000, loss = 0.264064
I0429 12:31:17.142714   966 solver.cpp:244]     Train net output #0: loss = 0.264064 (* 1 = 0.264064 loss)
I0429 12:31:17.142721   966 sgd_solver.cpp:106] Iteration 73000, lr = 1e-11
I0429 12:32:06.704743   966 solver.cpp:228] Iteration 73100, loss = 0.0902529
I0429 12:32:06.704907   966 solver.cpp:244]     Train net output #0: loss = 0.0902529 (* 1 = 0.0902529 loss)
I0429 12:32:06.704915   966 sgd_solver.cpp:106] Iteration 73100, lr = 1e-11
I0429 12:32:56.259110   966 solver.cpp:228] Iteration 73200, loss = 0.0643971
I0429 12:32:56.259272   966 solver.cpp:244]     Train net output #0: loss = 0.0643971 (* 1 = 0.0643971 loss)
I0429 12:32:56.259279   966 sgd_solver.cpp:106] Iteration 73200, lr = 1e-11
I0429 12:33:45.811843   966 solver.cpp:228] Iteration 73300, loss = 0.144549
I0429 12:33:45.812013   966 solver.cpp:244]     Train net output #0: loss = 0.144549 (* 1 = 0.144549 loss)
I0429 12:33:45.812021   966 sgd_solver.cpp:106] Iteration 73300, lr = 1e-11
I0429 12:34:45.626870   966 solver.cpp:228] Iteration 73400, loss = 0.198709
I0429 12:34:45.627065   966 solver.cpp:244]     Train net output #0: loss = 0.198709 (* 1 = 0.198709 loss)
I0429 12:34:45.627076   966 sgd_solver.cpp:106] Iteration 73400, lr = 1e-11
I0429 12:35:35.084796   966 solver.cpp:228] Iteration 73500, loss = 0.160934
I0429 12:35:35.084947   966 solver.cpp:244]     Train net output #0: loss = 0.160934 (* 1 = 0.160934 loss)
I0429 12:35:35.084955   966 sgd_solver.cpp:106] Iteration 73500, lr = 1e-11
I0429 12:36:24.610460   966 solver.cpp:228] Iteration 73600, loss = 0.224837
I0429 12:36:24.610610   966 solver.cpp:244]     Train net output #0: loss = 0.224837 (* 1 = 0.224837 loss)
I0429 12:36:24.610616   966 sgd_solver.cpp:106] Iteration 73600, lr = 1e-11
I0429 12:37:24.506639   966 solver.cpp:228] Iteration 73700, loss = 0.110567
I0429 12:37:24.506799   966 solver.cpp:244]     Train net output #0: loss = 0.110567 (* 1 = 0.110567 loss)
I0429 12:37:24.506806   966 sgd_solver.cpp:106] Iteration 73700, lr = 1e-11
I0429 12:38:13.949092   966 solver.cpp:228] Iteration 73800, loss = 0.102106
I0429 12:38:13.949275   966 solver.cpp:244]     Train net output #0: loss = 0.102106 (* 1 = 0.102106 loss)
I0429 12:38:13.949283   966 sgd_solver.cpp:106] Iteration 73800, lr = 1e-11
I0429 12:39:03.447556   966 solver.cpp:228] Iteration 73900, loss = 0.112907
I0429 12:39:03.447718   966 solver.cpp:244]     Train net output #0: loss = 0.112907 (* 1 = 0.112907 loss)
I0429 12:39:03.447726   966 sgd_solver.cpp:106] Iteration 73900, lr = 1e-11
I0429 12:39:52.663548   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_74000.caffemodel
I0429 12:39:58.765272   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_74000.solverstate
I0429 12:39:58.961102   966 solver.cpp:337] Iteration 74000, Testing net (#0)
I0429 12:39:58.961230   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 12:39:58.961237   966 net.cpp:693] Ignoring source layer visualize
I0429 12:39:58.961239   966 net.cpp:693] Ignoring source layer fake
I0429 12:44:51.340216   966 solver.cpp:404]     Test net output #0: loss = 0.190824 (* 1 = 0.190824 loss)
I0429 12:44:51.651706   966 solver.cpp:228] Iteration 74000, loss = 0.134209
I0429 12:44:51.651744   966 solver.cpp:244]     Train net output #0: loss = 0.134209 (* 1 = 0.134209 loss)
I0429 12:44:51.651752   966 sgd_solver.cpp:106] Iteration 74000, lr = 1e-11
I0429 12:45:51.187777   966 solver.cpp:228] Iteration 74100, loss = 0.110176
I0429 12:45:51.187963   966 solver.cpp:244]     Train net output #0: loss = 0.110176 (* 1 = 0.110176 loss)
I0429 12:45:51.187970   966 sgd_solver.cpp:106] Iteration 74100, lr = 1e-11
I0429 12:46:40.669427   966 solver.cpp:228] Iteration 74200, loss = 0.12719
I0429 12:46:40.669809   966 solver.cpp:244]     Train net output #0: loss = 0.12719 (* 1 = 0.12719 loss)
I0429 12:46:40.669818   966 sgd_solver.cpp:106] Iteration 74200, lr = 1e-11
I0429 12:47:30.185688   966 solver.cpp:228] Iteration 74300, loss = 0.101261
I0429 12:47:30.185864   966 solver.cpp:244]     Train net output #0: loss = 0.101261 (* 1 = 0.101261 loss)
I0429 12:47:30.185871   966 sgd_solver.cpp:106] Iteration 74300, lr = 1e-11
I0429 12:48:29.653239   966 solver.cpp:228] Iteration 74400, loss = 0.147044
I0429 12:48:29.653410   966 solver.cpp:244]     Train net output #0: loss = 0.147044 (* 1 = 0.147044 loss)
I0429 12:48:29.653417   966 sgd_solver.cpp:106] Iteration 74400, lr = 1e-11
I0429 12:49:19.112916   966 solver.cpp:228] Iteration 74500, loss = 0.223217
I0429 12:49:19.113078   966 solver.cpp:244]     Train net output #0: loss = 0.223217 (* 1 = 0.223217 loss)
I0429 12:49:19.113086   966 sgd_solver.cpp:106] Iteration 74500, lr = 1e-11
I0429 12:50:08.607540   966 solver.cpp:228] Iteration 74600, loss = 0.101461
I0429 12:50:08.607699   966 solver.cpp:244]     Train net output #0: loss = 0.101461 (* 1 = 0.101461 loss)
I0429 12:50:08.607707   966 sgd_solver.cpp:106] Iteration 74600, lr = 1e-11
I0429 12:51:07.384069   966 solver.cpp:228] Iteration 74700, loss = 0.158027
I0429 12:51:07.384259   966 solver.cpp:244]     Train net output #0: loss = 0.158027 (* 1 = 0.158027 loss)
I0429 12:51:07.384268   966 sgd_solver.cpp:106] Iteration 74700, lr = 1e-11
I0429 12:51:56.872364   966 solver.cpp:228] Iteration 74800, loss = 0.206759
I0429 12:51:56.872526   966 solver.cpp:244]     Train net output #0: loss = 0.206759 (* 1 = 0.206759 loss)
I0429 12:51:56.872534   966 sgd_solver.cpp:106] Iteration 74800, lr = 1e-11
I0429 12:52:46.388332   966 solver.cpp:228] Iteration 74900, loss = 0.203184
I0429 12:52:46.388492   966 solver.cpp:244]     Train net output #0: loss = 0.203184 (* 1 = 0.203184 loss)
I0429 12:52:46.388500   966 sgd_solver.cpp:106] Iteration 74900, lr = 1e-11
I0429 12:53:37.477684   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_75000.caffemodel
I0429 12:53:46.693040   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_75000.solverstate
I0429 12:53:46.880296   966 solver.cpp:337] Iteration 75000, Testing net (#0)
I0429 12:53:46.880424   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 12:53:46.880429   966 net.cpp:693] Ignoring source layer visualize
I0429 12:53:46.880432   966 net.cpp:693] Ignoring source layer fake
I0429 12:58:39.831940   966 solver.cpp:404]     Test net output #0: loss = 0.182456 (* 1 = 0.182456 loss)
I0429 12:58:40.143280   966 solver.cpp:228] Iteration 75000, loss = 0.0744891
I0429 12:58:40.143299   966 solver.cpp:244]     Train net output #0: loss = 0.0744891 (* 1 = 0.0744891 loss)
I0429 12:58:40.143322   966 sgd_solver.cpp:106] Iteration 75000, lr = 1e-12
I0429 12:59:29.680557   966 solver.cpp:228] Iteration 75100, loss = 0.132859
I0429 12:59:29.680724   966 solver.cpp:244]     Train net output #0: loss = 0.132859 (* 1 = 0.132859 loss)
I0429 12:59:29.680732   966 sgd_solver.cpp:106] Iteration 75100, lr = 1e-12
I0429 13:00:19.212234   966 solver.cpp:228] Iteration 75200, loss = 0.103001
I0429 13:00:19.212417   966 solver.cpp:244]     Train net output #0: loss = 0.103001 (* 1 = 0.103001 loss)
I0429 13:00:19.212426   966 sgd_solver.cpp:106] Iteration 75200, lr = 1e-12
I0429 13:01:11.135107   966 solver.cpp:228] Iteration 75300, loss = 0.208406
I0429 13:01:11.135272   966 solver.cpp:244]     Train net output #0: loss = 0.208406 (* 1 = 0.208406 loss)
I0429 13:01:11.135279   966 sgd_solver.cpp:106] Iteration 75300, lr = 1e-12
I0429 13:02:00.659441   966 solver.cpp:228] Iteration 75400, loss = 0.134319
I0429 13:02:00.660246   966 solver.cpp:244]     Train net output #0: loss = 0.134319 (* 1 = 0.134319 loss)
I0429 13:02:00.660254   966 sgd_solver.cpp:106] Iteration 75400, lr = 1e-12
I0429 13:02:51.875757   966 solver.cpp:228] Iteration 75500, loss = 0.0935773
I0429 13:02:51.875928   966 solver.cpp:244]     Train net output #0: loss = 0.0935773 (* 1 = 0.0935773 loss)
I0429 13:02:51.875936   966 sgd_solver.cpp:106] Iteration 75500, lr = 1e-12
I0429 13:03:41.397193   966 solver.cpp:228] Iteration 75600, loss = 0.159045
I0429 13:03:41.397367   966 solver.cpp:244]     Train net output #0: loss = 0.159045 (* 1 = 0.159045 loss)
I0429 13:03:41.397374   966 sgd_solver.cpp:106] Iteration 75600, lr = 1e-12
I0429 13:04:30.927918   966 solver.cpp:228] Iteration 75700, loss = 0.0766836
I0429 13:04:30.928102   966 solver.cpp:244]     Train net output #0: loss = 0.0766836 (* 1 = 0.0766836 loss)
I0429 13:04:30.928114   966 sgd_solver.cpp:106] Iteration 75700, lr = 1e-12
I0429 13:05:22.401659   966 solver.cpp:228] Iteration 75800, loss = 0.123451
I0429 13:05:22.402335   966 solver.cpp:244]     Train net output #0: loss = 0.123451 (* 1 = 0.123451 loss)
I0429 13:05:22.402343   966 sgd_solver.cpp:106] Iteration 75800, lr = 1e-12
I0429 13:06:11.930297   966 solver.cpp:228] Iteration 75900, loss = 0.14253
I0429 13:06:11.930474   966 solver.cpp:244]     Train net output #0: loss = 0.14253 (* 1 = 0.14253 loss)
I0429 13:06:11.930480   966 sgd_solver.cpp:106] Iteration 75900, lr = 1e-12
I0429 13:07:01.150611   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_76000.caffemodel
I0429 13:07:08.018064   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_76000.solverstate
I0429 13:07:08.210958   966 solver.cpp:337] Iteration 76000, Testing net (#0)
I0429 13:07:08.211064   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 13:07:08.211071   966 net.cpp:693] Ignoring source layer visualize
I0429 13:07:08.211072   966 net.cpp:693] Ignoring source layer fake
I0429 13:12:01.166775   966 solver.cpp:404]     Test net output #0: loss = 0.182189 (* 1 = 0.182189 loss)
I0429 13:12:01.478245   966 solver.cpp:228] Iteration 76000, loss = 0.207336
I0429 13:12:01.478283   966 solver.cpp:244]     Train net output #0: loss = 0.207336 (* 1 = 0.207336 loss)
I0429 13:12:01.478291   966 sgd_solver.cpp:106] Iteration 76000, lr = 1e-12
I0429 13:12:51.018591   966 solver.cpp:228] Iteration 76100, loss = 0.0717293
I0429 13:12:51.018776   966 solver.cpp:244]     Train net output #0: loss = 0.0717293 (* 1 = 0.0717293 loss)
I0429 13:12:51.018784   966 sgd_solver.cpp:106] Iteration 76100, lr = 1e-12
I0429 13:13:43.008576   966 solver.cpp:228] Iteration 76200, loss = 0.149629
I0429 13:13:43.008734   966 solver.cpp:244]     Train net output #0: loss = 0.149629 (* 1 = 0.149629 loss)
I0429 13:13:43.008743   966 sgd_solver.cpp:106] Iteration 76200, lr = 1e-12
I0429 13:14:32.540197   966 solver.cpp:228] Iteration 76300, loss = 0.162484
I0429 13:14:32.540349   966 solver.cpp:244]     Train net output #0: loss = 0.162484 (* 1 = 0.162484 loss)
I0429 13:14:32.540371   966 sgd_solver.cpp:106] Iteration 76300, lr = 1e-12
I0429 13:15:22.072423   966 solver.cpp:228] Iteration 76400, loss = 0.165242
I0429 13:15:22.072576   966 solver.cpp:244]     Train net output #0: loss = 0.165242 (* 1 = 0.165242 loss)
I0429 13:15:22.072582   966 sgd_solver.cpp:106] Iteration 76400, lr = 1e-12
I0429 13:16:14.056447   966 solver.cpp:228] Iteration 76500, loss = 0.190258
I0429 13:16:14.056612   966 solver.cpp:244]     Train net output #0: loss = 0.190258 (* 1 = 0.190258 loss)
I0429 13:16:14.056619   966 sgd_solver.cpp:106] Iteration 76500, lr = 1e-12
I0429 13:17:03.580245   966 solver.cpp:228] Iteration 76600, loss = 0.0919432
I0429 13:17:03.581965   966 solver.cpp:244]     Train net output #0: loss = 0.0919432 (* 1 = 0.0919432 loss)
I0429 13:17:03.581975   966 sgd_solver.cpp:106] Iteration 76600, lr = 1e-12
I0429 13:17:53.114985   966 solver.cpp:228] Iteration 76700, loss = 0.1092
I0429 13:17:53.115145   966 solver.cpp:244]     Train net output #0: loss = 0.1092 (* 1 = 0.1092 loss)
I0429 13:17:53.115154   966 sgd_solver.cpp:106] Iteration 76700, lr = 1e-12
I0429 13:18:42.639637   966 solver.cpp:228] Iteration 76800, loss = 0.106694
I0429 13:18:42.639796   966 solver.cpp:244]     Train net output #0: loss = 0.106694 (* 1 = 0.106694 loss)
I0429 13:18:42.639803   966 sgd_solver.cpp:106] Iteration 76800, lr = 1e-12
I0429 13:19:34.394279   966 solver.cpp:228] Iteration 76900, loss = 0.0789474
I0429 13:19:34.394444   966 solver.cpp:244]     Train net output #0: loss = 0.0789474 (* 1 = 0.0789474 loss)
I0429 13:19:34.394451   966 sgd_solver.cpp:106] Iteration 76900, lr = 1e-12
I0429 13:20:23.611616   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_77000.caffemodel
I0429 13:20:34.847180   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_77000.solverstate
I0429 13:20:35.039530   966 solver.cpp:337] Iteration 77000, Testing net (#0)
I0429 13:20:35.039652   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 13:20:35.039674   966 net.cpp:693] Ignoring source layer visualize
I0429 13:20:35.039677   966 net.cpp:693] Ignoring source layer fake
I0429 13:25:27.897114   966 solver.cpp:404]     Test net output #0: loss = 0.179068 (* 1 = 0.179068 loss)
I0429 13:25:28.208353   966 solver.cpp:228] Iteration 77000, loss = 0.126763
I0429 13:25:28.208389   966 solver.cpp:244]     Train net output #0: loss = 0.126763 (* 1 = 0.126763 loss)
I0429 13:25:28.208396   966 sgd_solver.cpp:106] Iteration 77000, lr = 1e-12
I0429 13:26:17.737179   966 solver.cpp:228] Iteration 77100, loss = 0.116134
I0429 13:26:17.737345   966 solver.cpp:244]     Train net output #0: loss = 0.116134 (* 1 = 0.116134 loss)
I0429 13:26:17.737352   966 sgd_solver.cpp:106] Iteration 77100, lr = 1e-12
I0429 13:27:09.431435   966 solver.cpp:228] Iteration 77200, loss = 0.17028
I0429 13:27:09.431591   966 solver.cpp:244]     Train net output #0: loss = 0.17028 (* 1 = 0.17028 loss)
I0429 13:27:09.431599   966 sgd_solver.cpp:106] Iteration 77200, lr = 1e-12
I0429 13:27:58.938199   966 solver.cpp:228] Iteration 77300, loss = 0.127719
I0429 13:27:58.938349   966 solver.cpp:244]     Train net output #0: loss = 0.127719 (* 1 = 0.127719 loss)
I0429 13:27:58.938356   966 sgd_solver.cpp:106] Iteration 77300, lr = 1e-12
I0429 13:28:48.444131   966 solver.cpp:228] Iteration 77400, loss = 0.12951
I0429 13:28:48.444335   966 solver.cpp:244]     Train net output #0: loss = 0.12951 (* 1 = 0.12951 loss)
I0429 13:28:48.444344   966 sgd_solver.cpp:106] Iteration 77400, lr = 1e-12
I0429 13:29:39.935637   966 solver.cpp:228] Iteration 77500, loss = 0.0899169
I0429 13:29:39.935791   966 solver.cpp:244]     Train net output #0: loss = 0.0899169 (* 1 = 0.0899169 loss)
I0429 13:29:39.935799   966 sgd_solver.cpp:106] Iteration 77500, lr = 1e-12
I0429 13:30:29.442132   966 solver.cpp:228] Iteration 77600, loss = 0.0927565
I0429 13:30:29.442284   966 solver.cpp:244]     Train net output #0: loss = 0.0927565 (* 1 = 0.0927565 loss)
I0429 13:30:29.442291   966 sgd_solver.cpp:106] Iteration 77600, lr = 1e-12
I0429 13:31:18.957448   966 solver.cpp:228] Iteration 77700, loss = 0.138378
I0429 13:31:18.957613   966 solver.cpp:244]     Train net output #0: loss = 0.138378 (* 1 = 0.138378 loss)
I0429 13:31:18.957620   966 sgd_solver.cpp:106] Iteration 77700, lr = 1e-12
I0429 13:32:08.481165   966 solver.cpp:228] Iteration 77800, loss = 0.0990464
I0429 13:32:08.481329   966 solver.cpp:244]     Train net output #0: loss = 0.0990464 (* 1 = 0.0990464 loss)
I0429 13:32:08.481338   966 sgd_solver.cpp:106] Iteration 77800, lr = 1e-12
I0429 13:32:59.831759   966 solver.cpp:228] Iteration 77900, loss = 0.193643
I0429 13:32:59.834931   966 solver.cpp:244]     Train net output #0: loss = 0.193643 (* 1 = 0.193643 loss)
I0429 13:32:59.834949   966 sgd_solver.cpp:106] Iteration 77900, lr = 1e-12
I0429 13:33:49.040030   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_78000.caffemodel
I0429 13:34:13.772593   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_78000.solverstate
I0429 13:34:14.085644   966 solver.cpp:337] Iteration 78000, Testing net (#0)
I0429 13:34:14.085772   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 13:34:14.085785   966 net.cpp:693] Ignoring source layer visualize
I0429 13:34:14.085788   966 net.cpp:693] Ignoring source layer fake
I0429 13:39:06.299085   966 solver.cpp:404]     Test net output #0: loss = 0.187928 (* 1 = 0.187928 loss)
I0429 13:39:06.608120   966 solver.cpp:228] Iteration 78000, loss = 0.287282
I0429 13:39:06.608139   966 solver.cpp:244]     Train net output #0: loss = 0.287282 (* 1 = 0.287282 loss)
I0429 13:39:06.608160   966 sgd_solver.cpp:106] Iteration 78000, lr = 1e-12
I0429 13:39:56.152279   966 solver.cpp:228] Iteration 78100, loss = 0.135047
I0429 13:39:56.152439   966 solver.cpp:244]     Train net output #0: loss = 0.135047 (* 1 = 0.135047 loss)
I0429 13:39:56.152447   966 sgd_solver.cpp:106] Iteration 78100, lr = 1e-12
I0429 13:40:47.884222   966 solver.cpp:228] Iteration 78200, loss = 0.11818
I0429 13:40:47.884373   966 solver.cpp:244]     Train net output #0: loss = 0.11818 (* 1 = 0.11818 loss)
I0429 13:40:47.884382   966 sgd_solver.cpp:106] Iteration 78200, lr = 1e-12
I0429 13:41:37.444653   966 solver.cpp:228] Iteration 78300, loss = 0.129443
I0429 13:41:37.444814   966 solver.cpp:244]     Train net output #0: loss = 0.129443 (* 1 = 0.129443 loss)
I0429 13:41:37.444821   966 sgd_solver.cpp:106] Iteration 78300, lr = 1e-12
I0429 13:42:29.580020   966 solver.cpp:228] Iteration 78400, loss = 0.2984
I0429 13:42:29.580183   966 solver.cpp:244]     Train net output #0: loss = 0.2984 (* 1 = 0.2984 loss)
I0429 13:42:29.580189   966 sgd_solver.cpp:106] Iteration 78400, lr = 1e-12
I0429 13:43:19.128904   966 solver.cpp:228] Iteration 78500, loss = 0.144634
I0429 13:43:19.129068   966 solver.cpp:244]     Train net output #0: loss = 0.144634 (* 1 = 0.144634 loss)
I0429 13:43:19.129087   966 sgd_solver.cpp:106] Iteration 78500, lr = 1e-12
I0429 13:44:08.695019   966 solver.cpp:228] Iteration 78600, loss = 0.157611
I0429 13:44:08.695195   966 solver.cpp:244]     Train net output #0: loss = 0.157611 (* 1 = 0.157611 loss)
I0429 13:44:08.695204   966 sgd_solver.cpp:106] Iteration 78600, lr = 1e-12
I0429 13:45:00.172991   966 solver.cpp:228] Iteration 78700, loss = 0.113831
I0429 13:45:00.173137   966 solver.cpp:244]     Train net output #0: loss = 0.113831 (* 1 = 0.113831 loss)
I0429 13:45:00.173144   966 sgd_solver.cpp:106] Iteration 78700, lr = 1e-12
I0429 13:45:49.715728   966 solver.cpp:228] Iteration 78800, loss = 0.130555
I0429 13:45:49.715908   966 solver.cpp:244]     Train net output #0: loss = 0.130555 (* 1 = 0.130555 loss)
I0429 13:45:49.715916   966 sgd_solver.cpp:106] Iteration 78800, lr = 1e-12
I0429 13:46:39.272027   966 solver.cpp:228] Iteration 78900, loss = 0.131228
I0429 13:46:39.272207   966 solver.cpp:244]     Train net output #0: loss = 0.131228 (* 1 = 0.131228 loss)
I0429 13:46:39.272215   966 sgd_solver.cpp:106] Iteration 78900, lr = 1e-12
I0429 13:47:30.851899   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_79000.caffemodel
I0429 13:47:54.115603   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_79000.solverstate
I0429 13:47:54.307837   966 solver.cpp:337] Iteration 79000, Testing net (#0)
I0429 13:47:54.307962   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 13:47:54.307968   966 net.cpp:693] Ignoring source layer visualize
I0429 13:47:54.307971   966 net.cpp:693] Ignoring source layer fake
I0429 13:52:47.939501   966 solver.cpp:404]     Test net output #0: loss = 0.190137 (* 1 = 0.190137 loss)
I0429 13:52:48.251044   966 solver.cpp:228] Iteration 79000, loss = 0.211145
I0429 13:52:48.251086   966 solver.cpp:244]     Train net output #0: loss = 0.211145 (* 1 = 0.211145 loss)
I0429 13:52:48.251094   966 sgd_solver.cpp:106] Iteration 79000, lr = 1e-12
I0429 13:53:37.789999   966 solver.cpp:228] Iteration 79100, loss = 0.175589
I0429 13:53:37.790179   966 solver.cpp:244]     Train net output #0: loss = 0.175589 (* 1 = 0.175589 loss)
I0429 13:53:37.790186   966 sgd_solver.cpp:106] Iteration 79100, lr = 1e-12
I0429 13:54:27.328269   966 solver.cpp:228] Iteration 79200, loss = 0.0414478
I0429 13:54:27.328451   966 solver.cpp:244]     Train net output #0: loss = 0.0414478 (* 1 = 0.0414478 loss)
I0429 13:54:27.328460   966 sgd_solver.cpp:106] Iteration 79200, lr = 1e-12
I0429 13:55:19.617641   966 solver.cpp:228] Iteration 79300, loss = 0.103987
I0429 13:55:19.617812   966 solver.cpp:244]     Train net output #0: loss = 0.103987 (* 1 = 0.103987 loss)
I0429 13:55:19.617821   966 sgd_solver.cpp:106] Iteration 79300, lr = 1e-12
I0429 13:56:09.144207   966 solver.cpp:228] Iteration 79400, loss = 0.0762631
I0429 13:56:09.144371   966 solver.cpp:244]     Train net output #0: loss = 0.0762631 (* 1 = 0.0762631 loss)
I0429 13:56:09.144378   966 sgd_solver.cpp:106] Iteration 79400, lr = 1e-12
I0429 13:56:58.685618   966 solver.cpp:228] Iteration 79500, loss = 0.185301
I0429 13:56:58.685775   966 solver.cpp:244]     Train net output #0: loss = 0.185301 (* 1 = 0.185301 loss)
I0429 13:56:58.685782   966 sgd_solver.cpp:106] Iteration 79500, lr = 1e-12
I0429 13:57:50.784484   966 solver.cpp:228] Iteration 79600, loss = 0.173911
I0429 13:57:50.784644   966 solver.cpp:244]     Train net output #0: loss = 0.173911 (* 1 = 0.173911 loss)
I0429 13:57:50.784652   966 sgd_solver.cpp:106] Iteration 79600, lr = 1e-12
I0429 13:58:40.303570   966 solver.cpp:228] Iteration 79700, loss = 0.281713
I0429 13:58:40.303735   966 solver.cpp:244]     Train net output #0: loss = 0.281713 (* 1 = 0.281713 loss)
I0429 13:58:40.303741   966 sgd_solver.cpp:106] Iteration 79700, lr = 1e-12
I0429 13:59:29.843209   966 solver.cpp:228] Iteration 79800, loss = 0.11412
I0429 13:59:29.843364   966 solver.cpp:244]     Train net output #0: loss = 0.11412 (* 1 = 0.11412 loss)
I0429 13:59:29.843370   966 sgd_solver.cpp:106] Iteration 79800, lr = 1e-12
I0429 14:00:19.363587   966 solver.cpp:228] Iteration 79900, loss = 0.122755
I0429 14:00:19.363757   966 solver.cpp:244]     Train net output #0: loss = 0.122755 (* 1 = 0.122755 loss)
I0429 14:00:19.363765   966 sgd_solver.cpp:106] Iteration 79900, lr = 1e-12
I0429 14:01:10.918391   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_80000.caffemodel
I0429 14:01:19.261868   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_80000.solverstate
I0429 14:01:19.474748   966 solver.cpp:337] Iteration 80000, Testing net (#0)
I0429 14:01:19.474874   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 14:01:19.474880   966 net.cpp:693] Ignoring source layer visualize
I0429 14:01:19.474882   966 net.cpp:693] Ignoring source layer fake
I0429 14:06:12.088143   966 solver.cpp:404]     Test net output #0: loss = 0.182005 (* 1 = 0.182005 loss)
I0429 14:06:12.398387   966 solver.cpp:228] Iteration 80000, loss = 0.113955
I0429 14:06:12.398424   966 solver.cpp:244]     Train net output #0: loss = 0.113955 (* 1 = 0.113955 loss)
I0429 14:06:12.398432   966 sgd_solver.cpp:106] Iteration 80000, lr = 1e-12
I0429 14:07:01.931905   966 solver.cpp:228] Iteration 80100, loss = 0.256502
I0429 14:07:01.932081   966 solver.cpp:244]     Train net output #0: loss = 0.256502 (* 1 = 0.256502 loss)
I0429 14:07:01.932090   966 sgd_solver.cpp:106] Iteration 80100, lr = 1e-12
I0429 14:07:51.475680   966 solver.cpp:228] Iteration 80200, loss = 0.129936
I0429 14:07:51.475880   966 solver.cpp:244]     Train net output #0: loss = 0.129936 (* 1 = 0.129936 loss)
I0429 14:07:51.475889   966 sgd_solver.cpp:106] Iteration 80200, lr = 1e-12
I0429 14:08:43.790844   966 solver.cpp:228] Iteration 80300, loss = 0.100153
I0429 14:08:43.791028   966 solver.cpp:244]     Train net output #0: loss = 0.100153 (* 1 = 0.100153 loss)
I0429 14:08:43.791036   966 sgd_solver.cpp:106] Iteration 80300, lr = 1e-12
I0429 14:09:33.299914   966 solver.cpp:228] Iteration 80400, loss = 0.0704805
I0429 14:09:33.300474   966 solver.cpp:244]     Train net output #0: loss = 0.0704805 (* 1 = 0.0704805 loss)
I0429 14:09:33.300483   966 sgd_solver.cpp:106] Iteration 80400, lr = 1e-12
I0429 14:10:22.815809   966 solver.cpp:228] Iteration 80500, loss = 0.0422421
I0429 14:10:22.815979   966 solver.cpp:244]     Train net output #0: loss = 0.0422421 (* 1 = 0.0422421 loss)
I0429 14:10:22.815986   966 sgd_solver.cpp:106] Iteration 80500, lr = 1e-12
I0429 14:11:14.555068   966 solver.cpp:228] Iteration 80600, loss = 0.227919
I0429 14:11:14.555265   966 solver.cpp:244]     Train net output #0: loss = 0.227919 (* 1 = 0.227919 loss)
I0429 14:11:14.555274   966 sgd_solver.cpp:106] Iteration 80600, lr = 1e-12
I0429 14:12:04.058253   966 solver.cpp:228] Iteration 80700, loss = 0.118616
I0429 14:12:04.058424   966 solver.cpp:244]     Train net output #0: loss = 0.118616 (* 1 = 0.118616 loss)
I0429 14:12:04.058431   966 sgd_solver.cpp:106] Iteration 80700, lr = 1e-12
I0429 14:12:53.581992   966 solver.cpp:228] Iteration 80800, loss = 0.113359
I0429 14:12:53.582176   966 solver.cpp:244]     Train net output #0: loss = 0.113359 (* 1 = 0.113359 loss)
I0429 14:12:53.582183   966 sgd_solver.cpp:106] Iteration 80800, lr = 1e-12
I0429 14:13:43.118309   966 solver.cpp:228] Iteration 80900, loss = 0.162952
I0429 14:13:43.118470   966 solver.cpp:244]     Train net output #0: loss = 0.162952 (* 1 = 0.162952 loss)
I0429 14:13:43.118477   966 sgd_solver.cpp:106] Iteration 80900, lr = 1e-12
I0429 14:14:34.433210   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_81000.caffemodel
I0429 14:14:46.188341   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_81000.solverstate
I0429 14:14:46.381402   966 solver.cpp:337] Iteration 81000, Testing net (#0)
I0429 14:14:46.381546   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 14:14:46.381567   966 net.cpp:693] Ignoring source layer visualize
I0429 14:14:46.381569   966 net.cpp:693] Ignoring source layer fake
I0429 14:19:40.020388   966 solver.cpp:404]     Test net output #0: loss = 0.181795 (* 1 = 0.181795 loss)
I0429 14:19:40.332303   966 solver.cpp:228] Iteration 81000, loss = 0.201866
I0429 14:19:40.332339   966 solver.cpp:244]     Train net output #0: loss = 0.201866 (* 1 = 0.201866 loss)
I0429 14:19:40.332345   966 sgd_solver.cpp:106] Iteration 81000, lr = 1e-12
I0429 14:20:29.846292   966 solver.cpp:228] Iteration 81100, loss = 0.114827
I0429 14:20:29.846490   966 solver.cpp:244]     Train net output #0: loss = 0.114827 (* 1 = 0.114827 loss)
I0429 14:20:29.846498   966 sgd_solver.cpp:106] Iteration 81100, lr = 1e-12
I0429 14:21:19.374501   966 solver.cpp:228] Iteration 81200, loss = 0.365929
I0429 14:21:19.374686   966 solver.cpp:244]     Train net output #0: loss = 0.365929 (* 1 = 0.365929 loss)
I0429 14:21:19.374692   966 sgd_solver.cpp:106] Iteration 81200, lr = 1e-12
I0429 14:22:10.832939   966 solver.cpp:228] Iteration 81300, loss = 0.0942745
I0429 14:22:10.833108   966 solver.cpp:244]     Train net output #0: loss = 0.0942745 (* 1 = 0.0942745 loss)
I0429 14:22:10.833117   966 sgd_solver.cpp:106] Iteration 81300, lr = 1e-12
I0429 14:23:00.350564   966 solver.cpp:228] Iteration 81400, loss = 0.103364
I0429 14:23:00.350721   966 solver.cpp:244]     Train net output #0: loss = 0.103364 (* 1 = 0.103364 loss)
I0429 14:23:00.350729   966 sgd_solver.cpp:106] Iteration 81400, lr = 1e-12
I0429 14:23:52.334167   966 solver.cpp:228] Iteration 81500, loss = 0.0987209
I0429 14:23:52.334328   966 solver.cpp:244]     Train net output #0: loss = 0.0987209 (* 1 = 0.0987209 loss)
I0429 14:23:52.334336   966 sgd_solver.cpp:106] Iteration 81500, lr = 1e-12
I0429 14:24:41.861627   966 solver.cpp:228] Iteration 81600, loss = 0.253369
I0429 14:24:41.861780   966 solver.cpp:244]     Train net output #0: loss = 0.253369 (* 1 = 0.253369 loss)
I0429 14:24:41.861788   966 sgd_solver.cpp:106] Iteration 81600, lr = 1e-12
I0429 14:25:31.408169   966 solver.cpp:228] Iteration 81700, loss = 0.126069
I0429 14:25:31.408349   966 solver.cpp:244]     Train net output #0: loss = 0.126069 (* 1 = 0.126069 loss)
I0429 14:25:31.408357   966 sgd_solver.cpp:106] Iteration 81700, lr = 1e-12
I0429 14:26:23.429967   966 solver.cpp:228] Iteration 81800, loss = 0.218637
I0429 14:26:23.430146   966 solver.cpp:244]     Train net output #0: loss = 0.218637 (* 1 = 0.218637 loss)
I0429 14:26:23.430155   966 sgd_solver.cpp:106] Iteration 81800, lr = 1e-12
I0429 14:27:12.962849   966 solver.cpp:228] Iteration 81900, loss = 0.0322784
I0429 14:27:12.963044   966 solver.cpp:244]     Train net output #0: loss = 0.0322784 (* 1 = 0.0322784 loss)
I0429 14:27:12.963063   966 sgd_solver.cpp:106] Iteration 81900, lr = 1e-12
I0429 14:28:02.181035   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_82000.caffemodel
I0429 14:28:17.228902   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_82000.solverstate
I0429 14:28:17.422098   966 solver.cpp:337] Iteration 82000, Testing net (#0)
I0429 14:28:17.422240   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 14:28:17.422245   966 net.cpp:693] Ignoring source layer visualize
I0429 14:28:17.422248   966 net.cpp:693] Ignoring source layer fake
I0429 14:33:09.869182   966 solver.cpp:404]     Test net output #0: loss = 0.179773 (* 1 = 0.179773 loss)
I0429 14:33:10.180764   966 solver.cpp:228] Iteration 82000, loss = 0.0729188
I0429 14:33:10.180783   966 solver.cpp:244]     Train net output #0: loss = 0.0729188 (* 1 = 0.0729188 loss)
I0429 14:33:10.180805   966 sgd_solver.cpp:106] Iteration 82000, lr = 1e-12
I0429 14:34:02.327569   966 solver.cpp:228] Iteration 82100, loss = 0.0934666
I0429 14:34:02.327764   966 solver.cpp:244]     Train net output #0: loss = 0.0934666 (* 1 = 0.0934666 loss)
I0429 14:34:02.327771   966 sgd_solver.cpp:106] Iteration 82100, lr = 1e-12
I0429 14:34:51.841894   966 solver.cpp:228] Iteration 82200, loss = 0.111527
I0429 14:34:51.842077   966 solver.cpp:244]     Train net output #0: loss = 0.111527 (* 1 = 0.111527 loss)
I0429 14:34:51.842084   966 sgd_solver.cpp:106] Iteration 82200, lr = 1e-12
I0429 14:35:41.373968   966 solver.cpp:228] Iteration 82300, loss = 0.159581
I0429 14:35:41.374161   966 solver.cpp:244]     Train net output #0: loss = 0.159581 (* 1 = 0.159581 loss)
I0429 14:35:41.374168   966 sgd_solver.cpp:106] Iteration 82300, lr = 1e-12
I0429 14:36:33.895084   966 solver.cpp:228] Iteration 82400, loss = 0.14591
I0429 14:36:33.895259   966 solver.cpp:244]     Train net output #0: loss = 0.14591 (* 1 = 0.14591 loss)
I0429 14:36:33.895267   966 sgd_solver.cpp:106] Iteration 82400, lr = 1e-12
I0429 14:37:23.406491   966 solver.cpp:228] Iteration 82500, loss = 0.16623
I0429 14:37:23.406661   966 solver.cpp:244]     Train net output #0: loss = 0.16623 (* 1 = 0.16623 loss)
I0429 14:37:23.406668   966 sgd_solver.cpp:106] Iteration 82500, lr = 1e-13
I0429 14:38:12.936695   966 solver.cpp:228] Iteration 82600, loss = 0.159885
I0429 14:38:12.936856   966 solver.cpp:244]     Train net output #0: loss = 0.159885 (* 1 = 0.159885 loss)
I0429 14:38:12.936861   966 sgd_solver.cpp:106] Iteration 82600, lr = 1e-13
I0429 14:39:02.465066   966 solver.cpp:228] Iteration 82700, loss = 0.0881311
I0429 14:39:02.465234   966 solver.cpp:244]     Train net output #0: loss = 0.0881311 (* 1 = 0.0881311 loss)
I0429 14:39:02.465241   966 sgd_solver.cpp:106] Iteration 82700, lr = 1e-13
I0429 14:39:54.771423   966 solver.cpp:228] Iteration 82800, loss = 0.115792
I0429 14:39:54.771585   966 solver.cpp:244]     Train net output #0: loss = 0.115792 (* 1 = 0.115792 loss)
I0429 14:39:54.771592   966 sgd_solver.cpp:106] Iteration 82800, lr = 1e-13
I0429 14:40:44.285172   966 solver.cpp:228] Iteration 82900, loss = 0.113267
I0429 14:40:44.285331   966 solver.cpp:244]     Train net output #0: loss = 0.113267 (* 1 = 0.113267 loss)
I0429 14:40:44.285338   966 sgd_solver.cpp:106] Iteration 82900, lr = 1e-13
I0429 14:41:33.502519   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_83000.caffemodel
I0429 14:41:50.048615   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_83000.solverstate
I0429 14:41:50.253617   966 solver.cpp:337] Iteration 83000, Testing net (#0)
I0429 14:41:50.253737   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 14:41:50.253743   966 net.cpp:693] Ignoring source layer visualize
I0429 14:41:50.253746   966 net.cpp:693] Ignoring source layer fake
I0429 14:46:42.392895   966 solver.cpp:404]     Test net output #0: loss = 0.189039 (* 1 = 0.189039 loss)
I0429 14:46:42.701764   966 solver.cpp:228] Iteration 83000, loss = 0.140733
I0429 14:46:42.701781   966 solver.cpp:244]     Train net output #0: loss = 0.140733 (* 1 = 0.140733 loss)
I0429 14:46:42.701804   966 sgd_solver.cpp:106] Iteration 83000, lr = 1e-13
I0429 14:47:34.883098   966 solver.cpp:228] Iteration 83100, loss = 0.139595
I0429 14:47:34.883270   966 solver.cpp:244]     Train net output #0: loss = 0.139595 (* 1 = 0.139595 loss)
I0429 14:47:34.883277   966 sgd_solver.cpp:106] Iteration 83100, lr = 1e-13
I0429 14:48:24.411936   966 solver.cpp:228] Iteration 83200, loss = 0.0850868
I0429 14:48:24.412101   966 solver.cpp:244]     Train net output #0: loss = 0.0850868 (* 1 = 0.0850868 loss)
I0429 14:48:24.412108   966 sgd_solver.cpp:106] Iteration 83200, lr = 1e-13
I0429 14:49:13.939653   966 solver.cpp:228] Iteration 83300, loss = 0.228619
I0429 14:49:13.939818   966 solver.cpp:244]     Train net output #0: loss = 0.228619 (* 1 = 0.228619 loss)
I0429 14:49:13.939824   966 sgd_solver.cpp:106] Iteration 83300, lr = 1e-13
I0429 14:50:05.410866   966 solver.cpp:228] Iteration 83400, loss = 0.150981
I0429 14:50:05.411053   966 solver.cpp:244]     Train net output #0: loss = 0.150981 (* 1 = 0.150981 loss)
I0429 14:50:05.411062   966 sgd_solver.cpp:106] Iteration 83400, lr = 1e-13
I0429 14:50:54.923964   966 solver.cpp:228] Iteration 83500, loss = 0.130475
I0429 14:50:54.924127   966 solver.cpp:244]     Train net output #0: loss = 0.130475 (* 1 = 0.130475 loss)
I0429 14:50:54.924135   966 sgd_solver.cpp:106] Iteration 83500, lr = 1e-13
I0429 14:51:44.437230   966 solver.cpp:228] Iteration 83600, loss = 0.10348
I0429 14:51:44.437398   966 solver.cpp:244]     Train net output #0: loss = 0.10348 (* 1 = 0.10348 loss)
I0429 14:51:44.437405   966 sgd_solver.cpp:106] Iteration 83600, lr = 1e-13
I0429 14:52:33.951007   966 solver.cpp:228] Iteration 83700, loss = 0.109023
I0429 14:52:33.951174   966 solver.cpp:244]     Train net output #0: loss = 0.109023 (* 1 = 0.109023 loss)
I0429 14:52:33.951181   966 sgd_solver.cpp:106] Iteration 83700, lr = 1e-13
I0429 14:53:25.555742   966 solver.cpp:228] Iteration 83800, loss = 0.154719
I0429 14:53:25.555907   966 solver.cpp:244]     Train net output #0: loss = 0.154719 (* 1 = 0.154719 loss)
I0429 14:53:25.555914   966 sgd_solver.cpp:106] Iteration 83800, lr = 1e-13
I0429 14:54:15.056987   966 solver.cpp:228] Iteration 83900, loss = 0.0818007
I0429 14:54:15.057164   966 solver.cpp:244]     Train net output #0: loss = 0.0818007 (* 1 = 0.0818007 loss)
I0429 14:54:15.057170   966 sgd_solver.cpp:106] Iteration 83900, lr = 1e-13
I0429 14:55:04.265177   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_84000.caffemodel
I0429 14:55:25.836447   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_84000.solverstate
I0429 14:55:26.030187   966 solver.cpp:337] Iteration 84000, Testing net (#0)
I0429 14:55:26.030306   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 14:55:26.030311   966 net.cpp:693] Ignoring source layer visualize
I0429 14:55:26.030313   966 net.cpp:693] Ignoring source layer fake
I0429 15:00:18.278686   966 solver.cpp:404]     Test net output #0: loss = 0.188769 (* 1 = 0.188769 loss)
I0429 15:00:18.588410   966 solver.cpp:228] Iteration 84000, loss = 0.0235112
I0429 15:00:18.588448   966 solver.cpp:244]     Train net output #0: loss = 0.0235112 (* 1 = 0.0235112 loss)
I0429 15:00:18.588454   966 sgd_solver.cpp:106] Iteration 84000, lr = 1e-13
I0429 15:01:10.609050   966 solver.cpp:228] Iteration 84100, loss = 0.0656651
I0429 15:01:10.609244   966 solver.cpp:244]     Train net output #0: loss = 0.0656651 (* 1 = 0.0656651 loss)
I0429 15:01:10.609253   966 sgd_solver.cpp:106] Iteration 84100, lr = 1e-13
I0429 15:02:00.122778   966 solver.cpp:228] Iteration 84200, loss = 0.110631
I0429 15:02:00.123672   966 solver.cpp:244]     Train net output #0: loss = 0.110631 (* 1 = 0.110631 loss)
I0429 15:02:00.123678   966 sgd_solver.cpp:106] Iteration 84200, lr = 1e-13
I0429 15:02:49.624723   966 solver.cpp:228] Iteration 84300, loss = 0.0972946
I0429 15:02:49.624886   966 solver.cpp:244]     Train net output #0: loss = 0.0972946 (* 1 = 0.0972946 loss)
I0429 15:02:49.624893   966 sgd_solver.cpp:106] Iteration 84300, lr = 1e-13
I0429 15:03:40.426416   966 solver.cpp:228] Iteration 84400, loss = 0.124306
I0429 15:03:40.426592   966 solver.cpp:244]     Train net output #0: loss = 0.124306 (* 1 = 0.124306 loss)
I0429 15:03:40.426599   966 sgd_solver.cpp:106] Iteration 84400, lr = 1e-13
I0429 15:04:29.958082   966 solver.cpp:228] Iteration 84500, loss = 0.106127
I0429 15:04:29.958264   966 solver.cpp:244]     Train net output #0: loss = 0.106127 (* 1 = 0.106127 loss)
I0429 15:04:29.958271   966 sgd_solver.cpp:106] Iteration 84500, lr = 1e-13
I0429 15:05:20.874462   966 solver.cpp:228] Iteration 84600, loss = 0.161416
I0429 15:05:20.874632   966 solver.cpp:244]     Train net output #0: loss = 0.161416 (* 1 = 0.161416 loss)
I0429 15:05:20.874639   966 sgd_solver.cpp:106] Iteration 84600, lr = 1e-13
I0429 15:06:10.402405   966 solver.cpp:228] Iteration 84700, loss = 0.224227
I0429 15:06:10.402583   966 solver.cpp:244]     Train net output #0: loss = 0.224227 (* 1 = 0.224227 loss)
I0429 15:06:10.402590   966 sgd_solver.cpp:106] Iteration 84700, lr = 1e-13
I0429 15:06:59.935354   966 solver.cpp:228] Iteration 84800, loss = 0.223932
I0429 15:06:59.935514   966 solver.cpp:244]     Train net output #0: loss = 0.223932 (* 1 = 0.223932 loss)
I0429 15:06:59.935520   966 sgd_solver.cpp:106] Iteration 84800, lr = 1e-13
I0429 15:07:50.975630   966 solver.cpp:228] Iteration 84900, loss = 0.156922
I0429 15:07:50.975802   966 solver.cpp:244]     Train net output #0: loss = 0.156922 (* 1 = 0.156922 loss)
I0429 15:07:50.975810   966 sgd_solver.cpp:106] Iteration 84900, lr = 1e-13
I0429 15:08:40.181630   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_85000.caffemodel
I0429 15:08:51.456866   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_85000.solverstate
I0429 15:08:51.646986   966 solver.cpp:337] Iteration 85000, Testing net (#0)
I0429 15:08:51.647107   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 15:08:51.647130   966 net.cpp:693] Ignoring source layer visualize
I0429 15:08:51.647131   966 net.cpp:693] Ignoring source layer fake
I0429 15:13:44.217438   966 solver.cpp:404]     Test net output #0: loss = 0.182118 (* 1 = 0.182118 loss)
I0429 15:13:44.526131   966 solver.cpp:228] Iteration 85000, loss = 0.131744
I0429 15:13:44.526149   966 solver.cpp:244]     Train net output #0: loss = 0.131744 (* 1 = 0.131744 loss)
I0429 15:13:44.526172   966 sgd_solver.cpp:106] Iteration 85000, lr = 1e-13
I0429 15:14:34.071260   966 solver.cpp:228] Iteration 85100, loss = 0.125923
I0429 15:14:34.071421   966 solver.cpp:244]     Train net output #0: loss = 0.125923 (* 1 = 0.125923 loss)
I0429 15:14:34.071427   966 sgd_solver.cpp:106] Iteration 85100, lr = 1e-13
I0429 15:15:25.378480   966 solver.cpp:228] Iteration 85200, loss = 0.240176
I0429 15:15:25.378656   966 solver.cpp:244]     Train net output #0: loss = 0.240176 (* 1 = 0.240176 loss)
I0429 15:15:25.378664   966 sgd_solver.cpp:106] Iteration 85200, lr = 1e-13
I0429 15:16:14.908879   966 solver.cpp:228] Iteration 85300, loss = 0.125988
I0429 15:16:14.909039   966 solver.cpp:244]     Train net output #0: loss = 0.125988 (* 1 = 0.125988 loss)
I0429 15:16:14.909045   966 sgd_solver.cpp:106] Iteration 85300, lr = 1e-13
I0429 15:17:04.433565   966 solver.cpp:228] Iteration 85400, loss = 0.201774
I0429 15:17:04.433730   966 solver.cpp:244]     Train net output #0: loss = 0.201774 (* 1 = 0.201774 loss)
I0429 15:17:04.433738   966 sgd_solver.cpp:106] Iteration 85400, lr = 1e-13
I0429 15:17:55.806417   966 solver.cpp:228] Iteration 85500, loss = 0.157737
I0429 15:17:55.806587   966 solver.cpp:244]     Train net output #0: loss = 0.157737 (* 1 = 0.157737 loss)
I0429 15:17:55.806594   966 sgd_solver.cpp:106] Iteration 85500, lr = 1e-13
I0429 15:18:45.327886   966 solver.cpp:228] Iteration 85600, loss = 0.141745
I0429 15:18:45.328030   966 solver.cpp:244]     Train net output #0: loss = 0.141745 (* 1 = 0.141745 loss)
I0429 15:18:45.328037   966 sgd_solver.cpp:106] Iteration 85600, lr = 1e-13
I0429 15:19:34.858048   966 solver.cpp:228] Iteration 85700, loss = 0.185743
I0429 15:19:34.858206   966 solver.cpp:244]     Train net output #0: loss = 0.185743 (* 1 = 0.185743 loss)
I0429 15:19:34.858213   966 sgd_solver.cpp:106] Iteration 85700, lr = 1e-13
I0429 15:20:24.391818   966 solver.cpp:228] Iteration 85800, loss = 0.202738
I0429 15:20:24.391973   966 solver.cpp:244]     Train net output #0: loss = 0.202738 (* 1 = 0.202738 loss)
I0429 15:20:24.391983   966 sgd_solver.cpp:106] Iteration 85800, lr = 1e-13
I0429 15:21:15.824940   966 solver.cpp:228] Iteration 85900, loss = 0.219566
I0429 15:21:15.825103   966 solver.cpp:244]     Train net output #0: loss = 0.219566 (* 1 = 0.219566 loss)
I0429 15:21:15.825109   966 sgd_solver.cpp:106] Iteration 85900, lr = 1e-13
I0429 15:22:05.027778   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_86000.caffemodel
I0429 15:22:16.727926   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_86000.solverstate
I0429 15:22:16.922883   966 solver.cpp:337] Iteration 86000, Testing net (#0)
I0429 15:22:16.923008   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 15:22:16.923015   966 net.cpp:693] Ignoring source layer visualize
I0429 15:22:16.923032   966 net.cpp:693] Ignoring source layer fake
I0429 15:27:10.125586   966 solver.cpp:404]     Test net output #0: loss = 0.181231 (* 1 = 0.181231 loss)
I0429 15:27:10.434088   966 solver.cpp:228] Iteration 86000, loss = 0.263355
I0429 15:27:10.434108   966 solver.cpp:244]     Train net output #0: loss = 0.263355 (* 1 = 0.263355 loss)
I0429 15:27:10.434130   966 sgd_solver.cpp:106] Iteration 86000, lr = 1e-13
I0429 15:27:59.971125   966 solver.cpp:228] Iteration 86100, loss = 0.0831294
I0429 15:27:59.971290   966 solver.cpp:244]     Train net output #0: loss = 0.0831294 (* 1 = 0.0831294 loss)
I0429 15:27:59.971298   966 sgd_solver.cpp:106] Iteration 86100, lr = 1e-13
I0429 15:28:54.288076   966 solver.cpp:228] Iteration 86200, loss = 0.140622
I0429 15:28:54.288252   966 solver.cpp:244]     Train net output #0: loss = 0.140622 (* 1 = 0.140622 loss)
I0429 15:28:54.288259   966 sgd_solver.cpp:106] Iteration 86200, lr = 1e-13
I0429 15:29:43.789923   966 solver.cpp:228] Iteration 86300, loss = 0.109521
I0429 15:29:43.790094   966 solver.cpp:244]     Train net output #0: loss = 0.109521 (* 1 = 0.109521 loss)
I0429 15:29:43.790102   966 sgd_solver.cpp:106] Iteration 86300, lr = 1e-13
I0429 15:30:33.298086   966 solver.cpp:228] Iteration 86400, loss = 0.180073
I0429 15:30:33.298243   966 solver.cpp:244]     Train net output #0: loss = 0.180073 (* 1 = 0.180073 loss)
I0429 15:30:33.298249   966 sgd_solver.cpp:106] Iteration 86400, lr = 1e-13
I0429 15:31:22.807005   966 solver.cpp:228] Iteration 86500, loss = 0.150512
I0429 15:31:22.807163   966 solver.cpp:244]     Train net output #0: loss = 0.150512 (* 1 = 0.150512 loss)
I0429 15:31:22.807171   966 sgd_solver.cpp:106] Iteration 86500, lr = 1e-13
I0429 15:32:14.097381   966 solver.cpp:228] Iteration 86600, loss = 0.126615
I0429 15:32:14.097566   966 solver.cpp:244]     Train net output #0: loss = 0.126615 (* 1 = 0.126615 loss)
I0429 15:32:14.097573   966 sgd_solver.cpp:106] Iteration 86600, lr = 1e-13
I0429 15:33:03.604763   966 solver.cpp:228] Iteration 86700, loss = 0.141491
I0429 15:33:03.604923   966 solver.cpp:244]     Train net output #0: loss = 0.141491 (* 1 = 0.141491 loss)
I0429 15:33:03.604929   966 sgd_solver.cpp:106] Iteration 86700, lr = 1e-13
I0429 15:33:53.114182   966 solver.cpp:228] Iteration 86800, loss = 0.104943
I0429 15:33:53.114362   966 solver.cpp:244]     Train net output #0: loss = 0.104943 (* 1 = 0.104943 loss)
I0429 15:33:53.114369   966 sgd_solver.cpp:106] Iteration 86800, lr = 1e-13
I0429 15:34:59.768694   966 solver.cpp:228] Iteration 86900, loss = 0.111776
I0429 15:34:59.768880   966 solver.cpp:244]     Train net output #0: loss = 0.111776 (* 1 = 0.111776 loss)
I0429 15:34:59.768893   966 sgd_solver.cpp:106] Iteration 86900, lr = 1e-13
I0429 15:35:48.878285   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_87000.caffemodel
I0429 15:36:01.957468   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_87000.solverstate
I0429 15:36:02.154539   966 solver.cpp:337] Iteration 87000, Testing net (#0)
I0429 15:36:02.154664   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 15:36:02.154669   966 net.cpp:693] Ignoring source layer visualize
I0429 15:36:02.154670   966 net.cpp:693] Ignoring source layer fake
I0429 15:40:54.496943   966 solver.cpp:404]     Test net output #0: loss = 0.180203 (* 1 = 0.180203 loss)
I0429 15:40:54.807332   966 solver.cpp:228] Iteration 87000, loss = 0.291398
I0429 15:40:54.807370   966 solver.cpp:244]     Train net output #0: loss = 0.291398 (* 1 = 0.291398 loss)
I0429 15:40:54.807376   966 sgd_solver.cpp:106] Iteration 87000, lr = 1e-13
I0429 15:41:44.317948   966 solver.cpp:228] Iteration 87100, loss = 0.141239
I0429 15:41:44.318119   966 solver.cpp:244]     Train net output #0: loss = 0.141239 (* 1 = 0.141239 loss)
I0429 15:41:44.318125   966 sgd_solver.cpp:106] Iteration 87100, lr = 1e-13
I0429 15:42:49.645071   966 solver.cpp:228] Iteration 87200, loss = 0.125045
I0429 15:42:49.645249   966 solver.cpp:244]     Train net output #0: loss = 0.125045 (* 1 = 0.125045 loss)
I0429 15:42:49.645258   966 sgd_solver.cpp:106] Iteration 87200, lr = 1e-13
I0429 15:43:38.993763   966 solver.cpp:228] Iteration 87300, loss = 0.0960192
I0429 15:43:38.993922   966 solver.cpp:244]     Train net output #0: loss = 0.0960192 (* 1 = 0.0960192 loss)
I0429 15:43:38.993929   966 sgd_solver.cpp:106] Iteration 87300, lr = 1e-13
I0429 15:44:28.483654   966 solver.cpp:228] Iteration 87400, loss = 0.0847211
I0429 15:44:28.483808   966 solver.cpp:244]     Train net output #0: loss = 0.0847211 (* 1 = 0.0847211 loss)
I0429 15:44:28.483814   966 sgd_solver.cpp:106] Iteration 87400, lr = 1e-13
I0429 15:45:33.722235   966 solver.cpp:228] Iteration 87500, loss = 0.139548
I0429 15:45:33.722404   966 solver.cpp:244]     Train net output #0: loss = 0.139548 (* 1 = 0.139548 loss)
I0429 15:45:33.722410   966 sgd_solver.cpp:106] Iteration 87500, lr = 1e-13
I0429 15:46:23.096750   966 solver.cpp:228] Iteration 87600, loss = 0.128335
I0429 15:46:23.096912   966 solver.cpp:244]     Train net output #0: loss = 0.128335 (* 1 = 0.128335 loss)
I0429 15:46:23.096920   966 sgd_solver.cpp:106] Iteration 87600, lr = 1e-13
I0429 15:47:12.604781   966 solver.cpp:228] Iteration 87700, loss = 0.171337
I0429 15:47:12.604965   966 solver.cpp:244]     Train net output #0: loss = 0.171337 (* 1 = 0.171337 loss)
I0429 15:47:12.604972   966 sgd_solver.cpp:106] Iteration 87700, lr = 1e-13
I0429 15:48:16.668942   966 solver.cpp:228] Iteration 87800, loss = 0.110159
I0429 15:48:16.669111   966 solver.cpp:244]     Train net output #0: loss = 0.110159 (* 1 = 0.110159 loss)
I0429 15:48:16.669118   966 sgd_solver.cpp:106] Iteration 87800, lr = 1e-13
I0429 15:49:06.159943   966 solver.cpp:228] Iteration 87900, loss = 0.174598
I0429 15:49:06.160115   966 solver.cpp:244]     Train net output #0: loss = 0.174598 (* 1 = 0.174598 loss)
I0429 15:49:06.160121   966 sgd_solver.cpp:106] Iteration 87900, lr = 1e-13
I0429 15:50:00.733711   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_88000.caffemodel
I0429 15:50:06.925626   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_88000.solverstate
I0429 15:50:07.116355   966 solver.cpp:337] Iteration 88000, Testing net (#0)
I0429 15:50:07.116479   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 15:50:07.116485   966 net.cpp:693] Ignoring source layer visualize
I0429 15:50:07.116487   966 net.cpp:693] Ignoring source layer fake
I0429 15:54:59.590294   966 solver.cpp:404]     Test net output #0: loss = 0.19052 (* 1 = 0.19052 loss)
I0429 15:54:59.898953   966 solver.cpp:228] Iteration 88000, loss = 0.091921
I0429 15:54:59.898972   966 solver.cpp:244]     Train net output #0: loss = 0.091921 (* 1 = 0.091921 loss)
I0429 15:54:59.898994   966 sgd_solver.cpp:106] Iteration 88000, lr = 1e-13
I0429 15:55:49.437664   966 solver.cpp:228] Iteration 88100, loss = 0.327615
I0429 15:55:49.437839   966 solver.cpp:244]     Train net output #0: loss = 0.327615 (* 1 = 0.327615 loss)
I0429 15:55:49.437846   966 sgd_solver.cpp:106] Iteration 88100, lr = 1e-13
I0429 15:56:38.988265   966 solver.cpp:228] Iteration 88200, loss = 0.0914362
I0429 15:56:38.988432   966 solver.cpp:244]     Train net output #0: loss = 0.0914362 (* 1 = 0.0914362 loss)
I0429 15:56:38.988440   966 sgd_solver.cpp:106] Iteration 88200, lr = 1e-13
I0429 15:57:30.377598   966 solver.cpp:228] Iteration 88300, loss = 0.163736
I0429 15:57:30.377786   966 solver.cpp:244]     Train net output #0: loss = 0.163736 (* 1 = 0.163736 loss)
I0429 15:57:30.377794   966 sgd_solver.cpp:106] Iteration 88300, lr = 1e-13
I0429 15:58:19.908627   966 solver.cpp:228] Iteration 88400, loss = 0.139762
I0429 15:58:19.908967   966 solver.cpp:244]     Train net output #0: loss = 0.139762 (* 1 = 0.139762 loss)
I0429 15:58:19.908975   966 sgd_solver.cpp:106] Iteration 88400, lr = 1e-13
I0429 15:59:09.439865   966 solver.cpp:228] Iteration 88500, loss = 0.0852863
I0429 15:59:09.440014   966 solver.cpp:244]     Train net output #0: loss = 0.0852863 (* 1 = 0.0852863 loss)
I0429 15:59:09.440021   966 sgd_solver.cpp:106] Iteration 88500, lr = 1e-13
I0429 15:59:58.983861   966 solver.cpp:228] Iteration 88600, loss = 0.222515
I0429 15:59:58.984047   966 solver.cpp:244]     Train net output #0: loss = 0.222515 (* 1 = 0.222515 loss)
I0429 15:59:58.984053   966 sgd_solver.cpp:106] Iteration 88600, lr = 1e-13
I0429 16:00:50.786684   966 solver.cpp:228] Iteration 88700, loss = 0.20678
I0429 16:00:50.786852   966 solver.cpp:244]     Train net output #0: loss = 0.20678 (* 1 = 0.20678 loss)
I0429 16:00:50.786860   966 sgd_solver.cpp:106] Iteration 88700, lr = 1e-13
I0429 16:01:40.312676   966 solver.cpp:228] Iteration 88800, loss = 0.111317
I0429 16:01:40.312822   966 solver.cpp:244]     Train net output #0: loss = 0.111317 (* 1 = 0.111317 loss)
I0429 16:01:40.312830   966 sgd_solver.cpp:106] Iteration 88800, lr = 1e-13
I0429 16:02:29.852304   966 solver.cpp:228] Iteration 88900, loss = 0.13588
I0429 16:02:29.852452   966 solver.cpp:244]     Train net output #0: loss = 0.13588 (* 1 = 0.13588 loss)
I0429 16:02:29.852458   966 sgd_solver.cpp:106] Iteration 88900, lr = 1e-13
I0429 16:03:21.020591   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_89000.caffemodel
I0429 16:03:42.616703   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_89000.solverstate
I0429 16:03:42.811313   966 solver.cpp:337] Iteration 89000, Testing net (#0)
I0429 16:03:42.811431   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 16:03:42.811437   966 net.cpp:693] Ignoring source layer visualize
I0429 16:03:42.811440   966 net.cpp:693] Ignoring source layer fake
I0429 16:08:35.291194   966 solver.cpp:404]     Test net output #0: loss = 0.187679 (* 1 = 0.187679 loss)
I0429 16:08:35.601289   966 solver.cpp:228] Iteration 89000, loss = 0.0936476
I0429 16:08:35.601336   966 solver.cpp:244]     Train net output #0: loss = 0.0936476 (* 1 = 0.0936476 loss)
I0429 16:08:35.601343   966 sgd_solver.cpp:106] Iteration 89000, lr = 1e-13
I0429 16:09:25.134903   966 solver.cpp:228] Iteration 89100, loss = 0.177096
I0429 16:09:25.135051   966 solver.cpp:244]     Train net output #0: loss = 0.177096 (* 1 = 0.177096 loss)
I0429 16:09:25.135057   966 sgd_solver.cpp:106] Iteration 89100, lr = 1e-13
I0429 16:10:14.666395   966 solver.cpp:228] Iteration 89200, loss = 0.131072
I0429 16:10:14.666551   966 solver.cpp:244]     Train net output #0: loss = 0.131072 (* 1 = 0.131072 loss)
I0429 16:10:14.666558   966 sgd_solver.cpp:106] Iteration 89200, lr = 1e-13
I0429 16:11:06.020031   966 solver.cpp:228] Iteration 89300, loss = 0.238318
I0429 16:11:06.020182   966 solver.cpp:244]     Train net output #0: loss = 0.238318 (* 1 = 0.238318 loss)
I0429 16:11:06.020190   966 sgd_solver.cpp:106] Iteration 89300, lr = 1e-13
I0429 16:11:55.527909   966 solver.cpp:228] Iteration 89400, loss = 0.147048
I0429 16:11:55.528069   966 solver.cpp:244]     Train net output #0: loss = 0.147048 (* 1 = 0.147048 loss)
I0429 16:11:55.528075   966 sgd_solver.cpp:106] Iteration 89400, lr = 1e-13
I0429 16:12:45.039383   966 solver.cpp:228] Iteration 89500, loss = 0.227741
I0429 16:12:45.039536   966 solver.cpp:244]     Train net output #0: loss = 0.227741 (* 1 = 0.227741 loss)
I0429 16:12:45.039542   966 sgd_solver.cpp:106] Iteration 89500, lr = 1e-13
I0429 16:13:34.571938   966 solver.cpp:228] Iteration 89600, loss = 0.0975221
I0429 16:13:34.572111   966 solver.cpp:244]     Train net output #0: loss = 0.0975221 (* 1 = 0.0975221 loss)
I0429 16:13:34.572118   966 sgd_solver.cpp:106] Iteration 89600, lr = 1e-13
I0429 16:14:25.895622   966 solver.cpp:228] Iteration 89700, loss = 0.11847
I0429 16:14:25.895787   966 solver.cpp:244]     Train net output #0: loss = 0.11847 (* 1 = 0.11847 loss)
I0429 16:14:25.895793   966 sgd_solver.cpp:106] Iteration 89700, lr = 1e-13
I0429 16:15:15.403379   966 solver.cpp:228] Iteration 89800, loss = 0.167707
I0429 16:15:15.403537   966 solver.cpp:244]     Train net output #0: loss = 0.167707 (* 1 = 0.167707 loss)
I0429 16:15:15.403556   966 sgd_solver.cpp:106] Iteration 89800, lr = 1e-13
I0429 16:16:04.918200   966 solver.cpp:228] Iteration 89900, loss = 0.0968649
I0429 16:16:04.918371   966 solver.cpp:244]     Train net output #0: loss = 0.0968649 (* 1 = 0.0968649 loss)
I0429 16:16:04.918378   966 sgd_solver.cpp:106] Iteration 89900, lr = 1e-13
I0429 16:16:55.958639   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_90000.caffemodel
I0429 16:17:18.638715   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_90000.solverstate
I0429 16:17:18.829741   966 solver.cpp:337] Iteration 90000, Testing net (#0)
I0429 16:17:18.829881   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 16:17:18.829887   966 net.cpp:693] Ignoring source layer visualize
I0429 16:17:18.829890   966 net.cpp:693] Ignoring source layer fake
I0429 16:22:11.261206   966 solver.cpp:404]     Test net output #0: loss = 0.181853 (* 1 = 0.181853 loss)
I0429 16:22:11.571293   966 solver.cpp:228] Iteration 90000, loss = 0.104128
I0429 16:22:11.571347   966 solver.cpp:244]     Train net output #0: loss = 0.104128 (* 1 = 0.104128 loss)
I0429 16:22:11.571357   966 sgd_solver.cpp:106] Iteration 90000, lr = 1e-14
I0429 16:23:01.122130   966 solver.cpp:228] Iteration 90100, loss = 0.117718
I0429 16:23:01.122290   966 solver.cpp:244]     Train net output #0: loss = 0.117718 (* 1 = 0.117718 loss)
I0429 16:23:01.122298   966 sgd_solver.cpp:106] Iteration 90100, lr = 1e-14
I0429 16:23:50.643654   966 solver.cpp:228] Iteration 90200, loss = 0.0942963
I0429 16:23:50.643810   966 solver.cpp:244]     Train net output #0: loss = 0.0942963 (* 1 = 0.0942963 loss)
I0429 16:23:50.643816   966 sgd_solver.cpp:106] Iteration 90200, lr = 1e-14
I0429 16:24:40.169217   966 solver.cpp:228] Iteration 90300, loss = 0.1347
I0429 16:24:40.169376   966 solver.cpp:244]     Train net output #0: loss = 0.1347 (* 1 = 0.1347 loss)
I0429 16:24:40.169384   966 sgd_solver.cpp:106] Iteration 90300, lr = 1e-14
I0429 16:25:32.377518   966 solver.cpp:228] Iteration 90400, loss = 0.120913
I0429 16:25:32.377692   966 solver.cpp:244]     Train net output #0: loss = 0.120913 (* 1 = 0.120913 loss)
I0429 16:25:32.377699   966 sgd_solver.cpp:106] Iteration 90400, lr = 1e-14
I0429 16:26:21.890625   966 solver.cpp:228] Iteration 90500, loss = 0.0607777
I0429 16:26:21.890795   966 solver.cpp:244]     Train net output #0: loss = 0.0607777 (* 1 = 0.0607777 loss)
I0429 16:26:21.890805   966 sgd_solver.cpp:106] Iteration 90500, lr = 1e-14
I0429 16:27:11.409193   966 solver.cpp:228] Iteration 90600, loss = 0.47268
I0429 16:27:11.409343   966 solver.cpp:244]     Train net output #0: loss = 0.47268 (* 1 = 0.47268 loss)
I0429 16:27:11.409351   966 sgd_solver.cpp:106] Iteration 90600, lr = 1e-14
I0429 16:28:02.229027   966 solver.cpp:228] Iteration 90700, loss = 0.155263
I0429 16:28:02.229192   966 solver.cpp:244]     Train net output #0: loss = 0.155263 (* 1 = 0.155263 loss)
I0429 16:28:02.229199   966 sgd_solver.cpp:106] Iteration 90700, lr = 1e-14
I0429 16:28:51.750398   966 solver.cpp:228] Iteration 90800, loss = 0.122304
I0429 16:28:51.750579   966 solver.cpp:244]     Train net output #0: loss = 0.122304 (* 1 = 0.122304 loss)
I0429 16:28:51.750586   966 sgd_solver.cpp:106] Iteration 90800, lr = 1e-14
I0429 16:29:42.629168   966 solver.cpp:228] Iteration 90900, loss = 0.111778
I0429 16:29:42.629360   966 solver.cpp:244]     Train net output #0: loss = 0.111778 (* 1 = 0.111778 loss)
I0429 16:29:42.629367   966 sgd_solver.cpp:106] Iteration 90900, lr = 1e-14
I0429 16:30:31.844460   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_91000.caffemodel
I0429 16:30:48.878352   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_91000.solverstate
I0429 16:30:49.072896   966 solver.cpp:337] Iteration 91000, Testing net (#0)
I0429 16:30:49.073017   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 16:30:49.073024   966 net.cpp:693] Ignoring source layer visualize
I0429 16:30:49.073025   966 net.cpp:693] Ignoring source layer fake
I0429 16:35:41.339586   966 solver.cpp:404]     Test net output #0: loss = 0.180658 (* 1 = 0.180658 loss)
I0429 16:35:41.651140   966 solver.cpp:228] Iteration 91000, loss = 0.129444
I0429 16:35:41.651159   966 solver.cpp:244]     Train net output #0: loss = 0.129444 (* 1 = 0.129444 loss)
I0429 16:35:41.651180   966 sgd_solver.cpp:106] Iteration 91000, lr = 1e-14
I0429 16:36:31.200562   966 solver.cpp:228] Iteration 91100, loss = 0.12886
I0429 16:36:31.200731   966 solver.cpp:244]     Train net output #0: loss = 0.12886 (* 1 = 0.12886 loss)
I0429 16:36:31.200738   966 sgd_solver.cpp:106] Iteration 91100, lr = 1e-14
I0429 16:37:36.174151   966 solver.cpp:228] Iteration 91200, loss = 0.0982311
I0429 16:37:36.174319   966 solver.cpp:244]     Train net output #0: loss = 0.0982311 (* 1 = 0.0982311 loss)
I0429 16:37:36.174327   966 sgd_solver.cpp:106] Iteration 91200, lr = 1e-14
I0429 16:38:25.669903   966 solver.cpp:228] Iteration 91300, loss = 0.14326
I0429 16:38:25.670089   966 solver.cpp:244]     Train net output #0: loss = 0.14326 (* 1 = 0.14326 loss)
I0429 16:38:25.670095   966 sgd_solver.cpp:106] Iteration 91300, lr = 1e-14
I0429 16:39:15.189221   966 solver.cpp:228] Iteration 91400, loss = 0.140921
I0429 16:39:15.189368   966 solver.cpp:244]     Train net output #0: loss = 0.140921 (* 1 = 0.140921 loss)
I0429 16:39:15.189375   966 sgd_solver.cpp:106] Iteration 91400, lr = 1e-14
I0429 16:40:13.338472   966 solver.cpp:228] Iteration 91500, loss = 0.170358
I0429 16:40:13.338644   966 solver.cpp:244]     Train net output #0: loss = 0.170358 (* 1 = 0.170358 loss)
I0429 16:40:13.338650   966 sgd_solver.cpp:106] Iteration 91500, lr = 1e-14
I0429 16:41:02.851604   966 solver.cpp:228] Iteration 91600, loss = 0.10183
I0429 16:41:02.851766   966 solver.cpp:244]     Train net output #0: loss = 0.10183 (* 1 = 0.10183 loss)
I0429 16:41:02.851773   966 sgd_solver.cpp:106] Iteration 91600, lr = 1e-14
I0429 16:41:52.389912   966 solver.cpp:228] Iteration 91700, loss = 0.155576
I0429 16:41:52.390079   966 solver.cpp:244]     Train net output #0: loss = 0.155576 (* 1 = 0.155576 loss)
I0429 16:41:52.390085   966 sgd_solver.cpp:106] Iteration 91700, lr = 1e-14
I0429 16:42:43.946244   966 solver.cpp:228] Iteration 91800, loss = 0.134489
I0429 16:42:43.946414   966 solver.cpp:244]     Train net output #0: loss = 0.134489 (* 1 = 0.134489 loss)
I0429 16:42:43.946422   966 sgd_solver.cpp:106] Iteration 91800, lr = 1e-14
I0429 16:43:33.471487   966 solver.cpp:228] Iteration 91900, loss = 0.1764
I0429 16:43:33.471655   966 solver.cpp:244]     Train net output #0: loss = 0.1764 (* 1 = 0.1764 loss)
I0429 16:43:33.471662   966 sgd_solver.cpp:106] Iteration 91900, lr = 1e-14
I0429 16:44:22.714875   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_92000.caffemodel
I0429 16:44:47.467355   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_92000.solverstate
I0429 16:44:47.737258   966 solver.cpp:337] Iteration 92000, Testing net (#0)
I0429 16:44:47.737382   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 16:44:47.737388   966 net.cpp:693] Ignoring source layer visualize
I0429 16:44:47.737390   966 net.cpp:693] Ignoring source layer fake
I0429 16:49:39.273547   966 solver.cpp:404]     Test net output #0: loss = 0.180655 (* 1 = 0.180655 loss)
I0429 16:49:39.583675   966 solver.cpp:228] Iteration 92000, loss = 0.249704
I0429 16:49:39.583715   966 solver.cpp:244]     Train net output #0: loss = 0.249704 (* 1 = 0.249704 loss)
I0429 16:49:39.583721   966 sgd_solver.cpp:106] Iteration 92000, lr = 1e-14
I0429 16:50:31.159876   966 solver.cpp:228] Iteration 92100, loss = 0.162553
I0429 16:50:31.160042   966 solver.cpp:244]     Train net output #0: loss = 0.162553 (* 1 = 0.162553 loss)
I0429 16:50:31.160048   966 sgd_solver.cpp:106] Iteration 92100, lr = 1e-14
I0429 16:51:20.684628   966 solver.cpp:228] Iteration 92200, loss = 0.076202
I0429 16:51:20.684798   966 solver.cpp:244]     Train net output #0: loss = 0.076202 (* 1 = 0.076202 loss)
I0429 16:51:20.684803   966 sgd_solver.cpp:106] Iteration 92200, lr = 1e-14
I0429 16:52:10.226353   966 solver.cpp:228] Iteration 92300, loss = 0.0899844
I0429 16:52:10.226508   966 solver.cpp:244]     Train net output #0: loss = 0.0899844 (* 1 = 0.0899844 loss)
I0429 16:52:10.226516   966 sgd_solver.cpp:106] Iteration 92300, lr = 1e-14
I0429 16:52:59.758751   966 solver.cpp:228] Iteration 92400, loss = 0.176974
I0429 16:52:59.758906   966 solver.cpp:244]     Train net output #0: loss = 0.176974 (* 1 = 0.176974 loss)
I0429 16:52:59.758913   966 sgd_solver.cpp:106] Iteration 92400, lr = 1e-14
I0429 16:53:51.221228   966 solver.cpp:228] Iteration 92500, loss = 0.278723
I0429 16:53:51.221377   966 solver.cpp:244]     Train net output #0: loss = 0.278723 (* 1 = 0.278723 loss)
I0429 16:53:51.221385   966 sgd_solver.cpp:106] Iteration 92500, lr = 1e-14
I0429 16:54:40.747908   966 solver.cpp:228] Iteration 92600, loss = 0.124684
I0429 16:54:40.748072   966 solver.cpp:244]     Train net output #0: loss = 0.124684 (* 1 = 0.124684 loss)
I0429 16:54:40.748080   966 sgd_solver.cpp:106] Iteration 92600, lr = 1e-14
I0429 16:55:30.274176   966 solver.cpp:228] Iteration 92700, loss = 0.108841
I0429 16:55:30.274320   966 solver.cpp:244]     Train net output #0: loss = 0.108841 (* 1 = 0.108841 loss)
I0429 16:55:30.274327   966 sgd_solver.cpp:106] Iteration 92700, lr = 1e-14
I0429 16:56:25.032840   966 solver.cpp:228] Iteration 92800, loss = 0.096893
I0429 16:56:25.032994   966 solver.cpp:244]     Train net output #0: loss = 0.096893 (* 1 = 0.096893 loss)
I0429 16:56:25.033001   966 sgd_solver.cpp:106] Iteration 92800, lr = 1e-14
I0429 16:57:14.525982   966 solver.cpp:228] Iteration 92900, loss = 0.0891641
I0429 16:57:14.526144   966 solver.cpp:244]     Train net output #0: loss = 0.0891641 (* 1 = 0.0891641 loss)
I0429 16:57:14.526150   966 sgd_solver.cpp:106] Iteration 92900, lr = 1e-14
I0429 16:58:03.742861   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_93000.caffemodel
I0429 16:58:18.715888   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_93000.solverstate
I0429 16:58:18.912503   966 solver.cpp:337] Iteration 93000, Testing net (#0)
I0429 16:58:18.912624   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 16:58:18.912631   966 net.cpp:693] Ignoring source layer visualize
I0429 16:58:18.912632   966 net.cpp:693] Ignoring source layer fake
I0429 17:03:11.564298   966 solver.cpp:404]     Test net output #0: loss = 0.192018 (* 1 = 0.192018 loss)
I0429 17:03:11.874361   966 solver.cpp:228] Iteration 93000, loss = 0.11315
I0429 17:03:11.874398   966 solver.cpp:244]     Train net output #0: loss = 0.11315 (* 1 = 0.11315 loss)
I0429 17:03:11.874404   966 sgd_solver.cpp:106] Iteration 93000, lr = 1e-14
I0429 17:04:03.694298   966 solver.cpp:228] Iteration 93100, loss = 0.0839729
I0429 17:04:03.694645   966 solver.cpp:244]     Train net output #0: loss = 0.0839729 (* 1 = 0.0839729 loss)
I0429 17:04:03.694654   966 sgd_solver.cpp:106] Iteration 93100, lr = 1e-14
I0429 17:04:53.194226   966 solver.cpp:228] Iteration 93200, loss = 0.0281296
I0429 17:04:53.194387   966 solver.cpp:244]     Train net output #0: loss = 0.0281296 (* 1 = 0.0281296 loss)
I0429 17:04:53.194394   966 sgd_solver.cpp:106] Iteration 93200, lr = 1e-14
I0429 17:05:42.710167   966 solver.cpp:228] Iteration 93300, loss = 0.114607
I0429 17:05:42.710364   966 solver.cpp:244]     Train net output #0: loss = 0.114607 (* 1 = 0.114607 loss)
I0429 17:05:42.710371   966 sgd_solver.cpp:106] Iteration 93300, lr = 1e-14
I0429 17:06:32.222214   966 solver.cpp:228] Iteration 93400, loss = 0.0890599
I0429 17:06:32.222373   966 solver.cpp:244]     Train net output #0: loss = 0.0890599 (* 1 = 0.0890599 loss)
I0429 17:06:32.222380   966 sgd_solver.cpp:106] Iteration 93400, lr = 1e-14
I0429 17:07:24.358446   966 solver.cpp:228] Iteration 93500, loss = 0.11513
I0429 17:07:24.358613   966 solver.cpp:244]     Train net output #0: loss = 0.11513 (* 1 = 0.11513 loss)
I0429 17:07:24.358619   966 sgd_solver.cpp:106] Iteration 93500, lr = 1e-14
I0429 17:08:13.866530   966 solver.cpp:228] Iteration 93600, loss = 0.203544
I0429 17:08:13.866698   966 solver.cpp:244]     Train net output #0: loss = 0.203544 (* 1 = 0.203544 loss)
I0429 17:08:13.866703   966 sgd_solver.cpp:106] Iteration 93600, lr = 1e-14
I0429 17:09:03.385442   966 solver.cpp:228] Iteration 93700, loss = 0.104651
I0429 17:09:03.385612   966 solver.cpp:244]     Train net output #0: loss = 0.104651 (* 1 = 0.104651 loss)
I0429 17:09:03.385618   966 sgd_solver.cpp:106] Iteration 93700, lr = 1e-14
I0429 17:09:54.542865   966 solver.cpp:228] Iteration 93800, loss = 0.161326
I0429 17:09:54.543035   966 solver.cpp:244]     Train net output #0: loss = 0.161326 (* 1 = 0.161326 loss)
I0429 17:09:54.543042   966 sgd_solver.cpp:106] Iteration 93800, lr = 1e-14
I0429 17:10:44.069290   966 solver.cpp:228] Iteration 93900, loss = 0.118238
I0429 17:10:44.069465   966 solver.cpp:244]     Train net output #0: loss = 0.118238 (* 1 = 0.118238 loss)
I0429 17:10:44.069473   966 sgd_solver.cpp:106] Iteration 93900, lr = 1e-14
I0429 17:11:34.903650   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_94000.caffemodel
I0429 17:11:44.493113   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_94000.solverstate
I0429 17:11:44.704830   966 solver.cpp:337] Iteration 94000, Testing net (#0)
I0429 17:11:44.704957   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 17:11:44.704978   966 net.cpp:693] Ignoring source layer visualize
I0429 17:11:44.704980   966 net.cpp:693] Ignoring source layer fake
I0429 17:16:36.920963   966 solver.cpp:404]     Test net output #0: loss = 0.186006 (* 1 = 0.186006 loss)
I0429 17:16:37.231899   966 solver.cpp:228] Iteration 94000, loss = 0.159505
I0429 17:16:37.231917   966 solver.cpp:244]     Train net output #0: loss = 0.159505 (* 1 = 0.159505 loss)
I0429 17:16:37.231938   966 sgd_solver.cpp:106] Iteration 94000, lr = 1e-14
I0429 17:17:26.802302   966 solver.cpp:228] Iteration 94100, loss = 0.148904
I0429 17:17:26.802533   966 solver.cpp:244]     Train net output #0: loss = 0.148904 (* 1 = 0.148904 loss)
I0429 17:17:26.802542   966 sgd_solver.cpp:106] Iteration 94100, lr = 1e-14
I0429 17:18:16.378127   966 solver.cpp:228] Iteration 94200, loss = 0.137914
I0429 17:18:16.379058   966 solver.cpp:244]     Train net output #0: loss = 0.137914 (* 1 = 0.137914 loss)
I0429 17:18:16.379065   966 sgd_solver.cpp:106] Iteration 94200, lr = 1e-14
I0429 17:19:08.169685   966 solver.cpp:228] Iteration 94300, loss = 0.160745
I0429 17:19:08.169876   966 solver.cpp:244]     Train net output #0: loss = 0.160745 (* 1 = 0.160745 loss)
I0429 17:19:08.169883   966 sgd_solver.cpp:106] Iteration 94300, lr = 1e-14
I0429 17:19:57.689802   966 solver.cpp:228] Iteration 94400, loss = 0.0687398
I0429 17:19:57.690013   966 solver.cpp:244]     Train net output #0: loss = 0.0687398 (* 1 = 0.0687398 loss)
I0429 17:19:57.690022   966 sgd_solver.cpp:106] Iteration 94400, lr = 1e-14
I0429 17:20:47.213405   966 solver.cpp:228] Iteration 94500, loss = 0.0981321
I0429 17:20:47.213583   966 solver.cpp:244]     Train net output #0: loss = 0.0981321 (* 1 = 0.0981321 loss)
I0429 17:20:47.213591   966 sgd_solver.cpp:106] Iteration 94500, lr = 1e-14
I0429 17:21:43.921126   966 solver.cpp:228] Iteration 94600, loss = 0.220201
I0429 17:21:43.922070   966 solver.cpp:244]     Train net output #0: loss = 0.220201 (* 1 = 0.220201 loss)
I0429 17:21:43.922094   966 sgd_solver.cpp:106] Iteration 94600, lr = 1e-14
I0429 17:22:33.422974   966 solver.cpp:228] Iteration 94700, loss = 0.176451
I0429 17:22:33.423173   966 solver.cpp:244]     Train net output #0: loss = 0.176451 (* 1 = 0.176451 loss)
I0429 17:22:33.423190   966 sgd_solver.cpp:106] Iteration 94700, lr = 1e-14
I0429 17:23:22.957732   966 solver.cpp:228] Iteration 94800, loss = 0.206031
I0429 17:23:22.957912   966 solver.cpp:244]     Train net output #0: loss = 0.206031 (* 1 = 0.206031 loss)
I0429 17:23:22.957919   966 sgd_solver.cpp:106] Iteration 94800, lr = 1e-14
I0429 17:24:19.531574   966 solver.cpp:228] Iteration 94900, loss = 0.203511
I0429 17:24:19.531756   966 solver.cpp:244]     Train net output #0: loss = 0.203511 (* 1 = 0.203511 loss)
I0429 17:24:19.531766   966 sgd_solver.cpp:106] Iteration 94900, lr = 1e-14
I0429 17:25:08.727437   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_95000.caffemodel
I0429 17:25:27.534260   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_95000.solverstate
I0429 17:25:27.792152   966 solver.cpp:337] Iteration 95000, Testing net (#0)
I0429 17:25:27.792276   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 17:25:27.792282   966 net.cpp:693] Ignoring source layer visualize
I0429 17:25:27.792284   966 net.cpp:693] Ignoring source layer fake
I0429 17:30:19.921162   966 solver.cpp:404]     Test net output #0: loss = 0.182048 (* 1 = 0.182048 loss)
I0429 17:30:20.230988   966 solver.cpp:228] Iteration 95000, loss = 0.100924
I0429 17:30:20.231005   966 solver.cpp:244]     Train net output #0: loss = 0.100924 (* 1 = 0.100924 loss)
I0429 17:30:20.231026   966 sgd_solver.cpp:106] Iteration 95000, lr = 1e-14
I0429 17:31:09.773216   966 solver.cpp:228] Iteration 95100, loss = 0.0658533
I0429 17:31:09.773388   966 solver.cpp:244]     Train net output #0: loss = 0.0658533 (* 1 = 0.0658533 loss)
I0429 17:31:09.773396   966 sgd_solver.cpp:106] Iteration 95100, lr = 1e-14
I0429 17:31:59.349756   966 solver.cpp:228] Iteration 95200, loss = 0.102899
I0429 17:31:59.349915   966 solver.cpp:244]     Train net output #0: loss = 0.102899 (* 1 = 0.102899 loss)
I0429 17:31:59.349921   966 sgd_solver.cpp:106] Iteration 95200, lr = 1e-14
I0429 17:33:00.701236   966 solver.cpp:228] Iteration 95300, loss = 0.196871
I0429 17:33:00.701403   966 solver.cpp:244]     Train net output #0: loss = 0.196871 (* 1 = 0.196871 loss)
I0429 17:33:00.701411   966 sgd_solver.cpp:106] Iteration 95300, lr = 1e-14
I0429 17:33:50.202153   966 solver.cpp:228] Iteration 95400, loss = 0.151786
I0429 17:33:50.202353   966 solver.cpp:244]     Train net output #0: loss = 0.151786 (* 1 = 0.151786 loss)
I0429 17:33:50.202361   966 sgd_solver.cpp:106] Iteration 95400, lr = 1e-14
I0429 17:34:39.718899   966 solver.cpp:228] Iteration 95500, loss = 0.237371
I0429 17:34:39.719074   966 solver.cpp:244]     Train net output #0: loss = 0.237371 (* 1 = 0.237371 loss)
I0429 17:34:39.719084   966 sgd_solver.cpp:106] Iteration 95500, lr = 1e-14
I0429 17:35:40.402442   966 solver.cpp:228] Iteration 95600, loss = 0.18688
I0429 17:35:40.402602   966 solver.cpp:244]     Train net output #0: loss = 0.18688 (* 1 = 0.18688 loss)
I0429 17:35:40.402611   966 sgd_solver.cpp:106] Iteration 95600, lr = 1e-14
I0429 17:36:29.896729   966 solver.cpp:228] Iteration 95700, loss = 0.152161
I0429 17:36:29.896917   966 solver.cpp:244]     Train net output #0: loss = 0.152161 (* 1 = 0.152161 loss)
I0429 17:36:29.896924   966 sgd_solver.cpp:106] Iteration 95700, lr = 1e-14
I0429 17:37:19.418097   966 solver.cpp:228] Iteration 95800, loss = 0.0944042
I0429 17:37:19.418254   966 solver.cpp:244]     Train net output #0: loss = 0.0944042 (* 1 = 0.0944042 loss)
I0429 17:37:19.418262   966 sgd_solver.cpp:106] Iteration 95800, lr = 1e-14
I0429 17:38:19.489572   966 solver.cpp:228] Iteration 95900, loss = 0.112748
I0429 17:38:19.489722   966 solver.cpp:244]     Train net output #0: loss = 0.112748 (* 1 = 0.112748 loss)
I0429 17:38:19.489729   966 sgd_solver.cpp:106] Iteration 95900, lr = 1e-14
I0429 17:39:08.653821   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_96000.caffemodel
I0429 17:39:22.155460   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_96000.solverstate
I0429 17:39:22.387501   966 solver.cpp:337] Iteration 96000, Testing net (#0)
I0429 17:39:22.387624   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 17:39:22.387630   966 net.cpp:693] Ignoring source layer visualize
I0429 17:39:22.387632   966 net.cpp:693] Ignoring source layer fake
I0429 17:44:14.285151   966 solver.cpp:404]     Test net output #0: loss = 0.180163 (* 1 = 0.180163 loss)
I0429 17:44:14.594980   966 solver.cpp:228] Iteration 96000, loss = 0.100389
I0429 17:44:14.595000   966 solver.cpp:244]     Train net output #0: loss = 0.100389 (* 1 = 0.100389 loss)
I0429 17:44:14.595021   966 sgd_solver.cpp:106] Iteration 96000, lr = 1e-14
I0429 17:45:04.132119   966 solver.cpp:228] Iteration 96100, loss = 0.148093
I0429 17:45:04.132275   966 solver.cpp:244]     Train net output #0: loss = 0.148093 (* 1 = 0.148093 loss)
I0429 17:45:04.132282   966 sgd_solver.cpp:106] Iteration 96100, lr = 1e-14
I0429 17:45:53.688181   966 solver.cpp:228] Iteration 96200, loss = 0.125728
I0429 17:45:53.688351   966 solver.cpp:244]     Train net output #0: loss = 0.125728 (* 1 = 0.125728 loss)
I0429 17:45:53.688359   966 sgd_solver.cpp:106] Iteration 96200, lr = 1e-14
I0429 17:46:54.285827   966 solver.cpp:228] Iteration 96300, loss = 0.0726485
I0429 17:46:54.286006   966 solver.cpp:244]     Train net output #0: loss = 0.0726485 (* 1 = 0.0726485 loss)
I0429 17:46:54.286015   966 sgd_solver.cpp:106] Iteration 96300, lr = 1e-14
I0429 17:47:43.768579   966 solver.cpp:228] Iteration 96400, loss = 0.0583977
I0429 17:47:43.768748   966 solver.cpp:244]     Train net output #0: loss = 0.0583977 (* 1 = 0.0583977 loss)
I0429 17:47:43.768755   966 sgd_solver.cpp:106] Iteration 96400, lr = 1e-14
I0429 17:48:33.272928   966 solver.cpp:228] Iteration 96500, loss = 0.128133
I0429 17:48:33.273099   966 solver.cpp:244]     Train net output #0: loss = 0.128133 (* 1 = 0.128133 loss)
I0429 17:48:33.273106   966 sgd_solver.cpp:106] Iteration 96500, lr = 1e-14
I0429 17:49:32.680341   966 solver.cpp:228] Iteration 96600, loss = 0.127109
I0429 17:49:32.680517   966 solver.cpp:244]     Train net output #0: loss = 0.127109 (* 1 = 0.127109 loss)
I0429 17:49:32.680524   966 sgd_solver.cpp:106] Iteration 96600, lr = 1e-14
I0429 17:50:22.166373   966 solver.cpp:228] Iteration 96700, loss = 0.605536
I0429 17:50:22.166550   966 solver.cpp:244]     Train net output #0: loss = 0.605536 (* 1 = 0.605536 loss)
I0429 17:50:22.166559   966 sgd_solver.cpp:106] Iteration 96700, lr = 1e-14
I0429 17:51:11.677912   966 solver.cpp:228] Iteration 96800, loss = 0.135493
I0429 17:51:11.679183   966 solver.cpp:244]     Train net output #0: loss = 0.135493 (* 1 = 0.135493 loss)
I0429 17:51:11.679190   966 sgd_solver.cpp:106] Iteration 96800, lr = 1e-14
I0429 17:52:08.840682   966 solver.cpp:228] Iteration 96900, loss = 0.181654
I0429 17:52:08.840854   966 solver.cpp:244]     Train net output #0: loss = 0.181654 (* 1 = 0.181654 loss)
I0429 17:52:08.840862   966 sgd_solver.cpp:106] Iteration 96900, lr = 1e-14
I0429 17:52:58.024796   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_97000.caffemodel
I0429 17:53:17.540853   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_97000.solverstate
I0429 17:53:17.727993   966 solver.cpp:337] Iteration 97000, Testing net (#0)
I0429 17:53:17.728153   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 17:53:17.728160   966 net.cpp:693] Ignoring source layer visualize
I0429 17:53:17.728163   966 net.cpp:693] Ignoring source layer fake
I0429 17:58:09.836535   966 solver.cpp:404]     Test net output #0: loss = 0.181513 (* 1 = 0.181513 loss)
I0429 17:58:10.146647   966 solver.cpp:228] Iteration 97000, loss = 0.1061
I0429 17:58:10.146694   966 solver.cpp:244]     Train net output #0: loss = 0.1061 (* 1 = 0.1061 loss)
I0429 17:58:10.146703   966 sgd_solver.cpp:106] Iteration 97000, lr = 1e-14
I0429 17:59:08.536669   966 solver.cpp:228] Iteration 97100, loss = 0.13679
I0429 17:59:08.536837   966 solver.cpp:244]     Train net output #0: loss = 0.13679 (* 1 = 0.13679 loss)
I0429 17:59:08.536844   966 sgd_solver.cpp:106] Iteration 97100, lr = 1e-14
I0429 17:59:58.037269   966 solver.cpp:228] Iteration 97200, loss = 0.102819
I0429 17:59:58.037443   966 solver.cpp:244]     Train net output #0: loss = 0.102819 (* 1 = 0.102819 loss)
I0429 17:59:58.037451   966 sgd_solver.cpp:106] Iteration 97200, lr = 1e-14
I0429 18:00:47.549370   966 solver.cpp:228] Iteration 97300, loss = 0.119582
I0429 18:00:47.549700   966 solver.cpp:244]     Train net output #0: loss = 0.119582 (* 1 = 0.119582 loss)
I0429 18:00:47.549706   966 sgd_solver.cpp:106] Iteration 97300, lr = 1e-14
I0429 18:01:46.805205   966 solver.cpp:228] Iteration 97400, loss = 0.0792159
I0429 18:01:46.805368   966 solver.cpp:244]     Train net output #0: loss = 0.0792159 (* 1 = 0.0792159 loss)
I0429 18:01:46.805377   966 sgd_solver.cpp:106] Iteration 97400, lr = 1e-14
I0429 18:02:36.287624   966 solver.cpp:228] Iteration 97500, loss = 0.080736
I0429 18:02:36.287849   966 solver.cpp:244]     Train net output #0: loss = 0.080736 (* 1 = 0.080736 loss)
I0429 18:02:36.287863   966 sgd_solver.cpp:106] Iteration 97500, lr = 1e-15
I0429 18:03:25.810933   966 solver.cpp:228] Iteration 97600, loss = 0.195899
I0429 18:03:25.811103   966 solver.cpp:244]     Train net output #0: loss = 0.195899 (* 1 = 0.195899 loss)
I0429 18:03:25.811110   966 sgd_solver.cpp:106] Iteration 97600, lr = 1e-15
I0429 18:04:25.697944   966 solver.cpp:228] Iteration 97700, loss = 0.126664
I0429 18:04:25.698125   966 solver.cpp:244]     Train net output #0: loss = 0.126664 (* 1 = 0.126664 loss)
I0429 18:04:25.698134   966 sgd_solver.cpp:106] Iteration 97700, lr = 1e-15
I0429 18:05:15.184456   966 solver.cpp:228] Iteration 97800, loss = 0.206307
I0429 18:05:15.184624   966 solver.cpp:244]     Train net output #0: loss = 0.206307 (* 1 = 0.206307 loss)
I0429 18:05:15.184634   966 sgd_solver.cpp:106] Iteration 97800, lr = 1e-15
I0429 18:06:04.715565   966 solver.cpp:228] Iteration 97900, loss = 0.318858
I0429 18:06:04.715723   966 solver.cpp:244]     Train net output #0: loss = 0.318858 (* 1 = 0.318858 loss)
I0429 18:06:04.715730   966 sgd_solver.cpp:106] Iteration 97900, lr = 1e-15
I0429 18:07:05.248867   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_98000.caffemodel
I0429 18:07:23.310900   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_98000.solverstate
I0429 18:07:23.549085   966 solver.cpp:337] Iteration 98000, Testing net (#0)
I0429 18:07:23.549211   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 18:07:23.549217   966 net.cpp:693] Ignoring source layer visualize
I0429 18:07:23.549234   966 net.cpp:693] Ignoring source layer fake
I0429 18:12:15.804913   966 solver.cpp:404]     Test net output #0: loss = 0.191447 (* 1 = 0.191447 loss)
I0429 18:12:16.114974   966 solver.cpp:228] Iteration 98000, loss = 0.134437
I0429 18:12:16.115010   966 solver.cpp:244]     Train net output #0: loss = 0.134437 (* 1 = 0.134437 loss)
I0429 18:12:16.115016   966 sgd_solver.cpp:106] Iteration 98000, lr = 1e-15
I0429 18:13:05.659541   966 solver.cpp:228] Iteration 98100, loss = 0.173874
I0429 18:13:05.659703   966 solver.cpp:244]     Train net output #0: loss = 0.173874 (* 1 = 0.173874 loss)
I0429 18:13:05.659710   966 sgd_solver.cpp:106] Iteration 98100, lr = 1e-15
I0429 18:13:55.216558   966 solver.cpp:228] Iteration 98200, loss = 0.139522
I0429 18:13:55.216768   966 solver.cpp:244]     Train net output #0: loss = 0.139522 (* 1 = 0.139522 loss)
I0429 18:13:55.216776   966 sgd_solver.cpp:106] Iteration 98200, lr = 1e-15
I0429 18:14:44.763490   966 solver.cpp:228] Iteration 98300, loss = 0.107157
I0429 18:14:44.763677   966 solver.cpp:244]     Train net output #0: loss = 0.107157 (* 1 = 0.107157 loss)
I0429 18:14:44.763686   966 sgd_solver.cpp:106] Iteration 98300, lr = 1e-15
I0429 18:15:45.056716   966 solver.cpp:228] Iteration 98400, loss = 0.0965203
I0429 18:15:45.056881   966 solver.cpp:244]     Train net output #0: loss = 0.0965203 (* 1 = 0.0965203 loss)
I0429 18:15:45.056890   966 sgd_solver.cpp:106] Iteration 98400, lr = 1e-15
I0429 18:16:34.557940   966 solver.cpp:228] Iteration 98500, loss = 0.174116
I0429 18:16:34.558100   966 solver.cpp:244]     Train net output #0: loss = 0.174116 (* 1 = 0.174116 loss)
I0429 18:16:34.558109   966 sgd_solver.cpp:106] Iteration 98500, lr = 1e-15
I0429 18:17:24.089354   966 solver.cpp:228] Iteration 98600, loss = 0.213957
I0429 18:17:24.089522   966 solver.cpp:244]     Train net output #0: loss = 0.213957 (* 1 = 0.213957 loss)
I0429 18:17:24.089529   966 sgd_solver.cpp:106] Iteration 98600, lr = 1e-15
I0429 18:18:25.057996   966 solver.cpp:228] Iteration 98700, loss = 0.323841
I0429 18:18:25.058181   966 solver.cpp:244]     Train net output #0: loss = 0.323841 (* 1 = 0.323841 loss)
I0429 18:18:25.058187   966 sgd_solver.cpp:106] Iteration 98700, lr = 1e-15
I0429 18:19:14.499138   966 solver.cpp:228] Iteration 98800, loss = 0.106196
I0429 18:19:14.499295   966 solver.cpp:244]     Train net output #0: loss = 0.106196 (* 1 = 0.106196 loss)
I0429 18:19:14.499302   966 sgd_solver.cpp:106] Iteration 98800, lr = 1e-15
I0429 18:20:04.004508   966 solver.cpp:228] Iteration 98900, loss = 0.145638
I0429 18:20:04.004670   966 solver.cpp:244]     Train net output #0: loss = 0.145638 (* 1 = 0.145638 loss)
I0429 18:20:04.004678   966 sgd_solver.cpp:106] Iteration 98900, lr = 1e-15
I0429 18:20:53.222506   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_99000.caffemodel
I0429 18:21:13.572680   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_99000.solverstate
I0429 18:21:13.778867   966 solver.cpp:337] Iteration 99000, Testing net (#0)
I0429 18:21:13.778991   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 18:21:13.778998   966 net.cpp:693] Ignoring source layer visualize
I0429 18:21:13.779000   966 net.cpp:693] Ignoring source layer fake
I0429 18:26:06.092370   966 solver.cpp:404]     Test net output #0: loss = 0.185859 (* 1 = 0.185859 loss)
I0429 18:26:06.402454   966 solver.cpp:228] Iteration 99000, loss = 0.197155
I0429 18:26:06.402472   966 solver.cpp:244]     Train net output #0: loss = 0.197155 (* 1 = 0.197155 loss)
I0429 18:26:06.402495   966 sgd_solver.cpp:106] Iteration 99000, lr = 1e-15
I0429 18:27:04.005682   966 solver.cpp:228] Iteration 99100, loss = 0.111761
I0429 18:27:04.005848   966 solver.cpp:244]     Train net output #0: loss = 0.111761 (* 1 = 0.111761 loss)
I0429 18:27:04.005856   966 sgd_solver.cpp:106] Iteration 99100, lr = 1e-15
I0429 18:27:53.556757   966 solver.cpp:228] Iteration 99200, loss = 0.143373
I0429 18:27:53.557019   966 solver.cpp:244]     Train net output #0: loss = 0.143373 (* 1 = 0.143373 loss)
I0429 18:27:53.557034   966 sgd_solver.cpp:106] Iteration 99200, lr = 1e-15
I0429 18:28:43.085526   966 solver.cpp:228] Iteration 99300, loss = 0.154472
I0429 18:28:43.085710   966 solver.cpp:244]     Train net output #0: loss = 0.154472 (* 1 = 0.154472 loss)
I0429 18:28:43.085717   966 sgd_solver.cpp:106] Iteration 99300, lr = 1e-15
I0429 18:29:39.850878   966 solver.cpp:228] Iteration 99400, loss = 0.101457
I0429 18:29:39.851056   966 solver.cpp:244]     Train net output #0: loss = 0.101457 (* 1 = 0.101457 loss)
I0429 18:29:39.851064   966 sgd_solver.cpp:106] Iteration 99400, lr = 1e-15
I0429 18:30:29.341485   966 solver.cpp:228] Iteration 99500, loss = 0.147288
I0429 18:30:29.341647   966 solver.cpp:244]     Train net output #0: loss = 0.147288 (* 1 = 0.147288 loss)
I0429 18:30:29.341655   966 sgd_solver.cpp:106] Iteration 99500, lr = 1e-15
I0429 18:31:18.855912   966 solver.cpp:228] Iteration 99600, loss = 0.105138
I0429 18:31:18.856077   966 solver.cpp:244]     Train net output #0: loss = 0.105138 (* 1 = 0.105138 loss)
I0429 18:31:18.856086   966 sgd_solver.cpp:106] Iteration 99600, lr = 1e-15
I0429 18:32:23.517611   966 solver.cpp:228] Iteration 99700, loss = 0.107894
I0429 18:32:23.517812   966 solver.cpp:244]     Train net output #0: loss = 0.107894 (* 1 = 0.107894 loss)
I0429 18:32:23.517822   966 sgd_solver.cpp:106] Iteration 99700, lr = 1e-15
I0429 18:33:12.894212   966 solver.cpp:228] Iteration 99800, loss = 0.0771145
I0429 18:33:12.894363   966 solver.cpp:244]     Train net output #0: loss = 0.0771145 (* 1 = 0.0771145 loss)
I0429 18:33:12.894371   966 sgd_solver.cpp:106] Iteration 99800, lr = 1e-15
I0429 18:34:02.398872   966 solver.cpp:228] Iteration 99900, loss = 0.428974
I0429 18:34:02.399034   966 solver.cpp:244]     Train net output #0: loss = 0.428974 (* 1 = 0.428974 loss)
I0429 18:34:02.399041   966 sgd_solver.cpp:106] Iteration 99900, lr = 1e-15
I0429 18:35:06.817214   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_100000.caffemodel
I0429 18:35:17.839437   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_100000.solverstate
I0429 18:35:18.043453   966 solver.cpp:337] Iteration 100000, Testing net (#0)
I0429 18:35:18.043560   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 18:35:18.043567   966 net.cpp:693] Ignoring source layer visualize
I0429 18:35:18.043570   966 net.cpp:693] Ignoring source layer fake
I0429 18:40:10.256151   966 solver.cpp:404]     Test net output #0: loss = 0.182376 (* 1 = 0.182376 loss)
I0429 18:40:10.569015   966 solver.cpp:228] Iteration 100000, loss = 0.12539
I0429 18:40:10.569051   966 solver.cpp:244]     Train net output #0: loss = 0.12539 (* 1 = 0.12539 loss)
I0429 18:40:10.569057   966 sgd_solver.cpp:106] Iteration 100000, lr = 1e-15
I0429 18:41:00.101658   966 solver.cpp:228] Iteration 100100, loss = 0.107952
I0429 18:41:00.101833   966 solver.cpp:244]     Train net output #0: loss = 0.107952 (* 1 = 0.107952 loss)
I0429 18:41:00.101840   966 sgd_solver.cpp:106] Iteration 100100, lr = 1e-15
I0429 18:41:49.632855   966 solver.cpp:228] Iteration 100200, loss = 0.149136
I0429 18:41:49.633056   966 solver.cpp:244]     Train net output #0: loss = 0.149136 (* 1 = 0.149136 loss)
I0429 18:41:49.633064   966 sgd_solver.cpp:106] Iteration 100200, lr = 1e-15
I0429 18:42:53.803397   966 solver.cpp:228] Iteration 100300, loss = 0.136254
I0429 18:42:53.803572   966 solver.cpp:244]     Train net output #0: loss = 0.136254 (* 1 = 0.136254 loss)
I0429 18:42:53.803581   966 sgd_solver.cpp:106] Iteration 100300, lr = 1e-15
I0429 18:43:43.298511   966 solver.cpp:228] Iteration 100400, loss = 0.128368
I0429 18:43:43.298676   966 solver.cpp:244]     Train net output #0: loss = 0.128368 (* 1 = 0.128368 loss)
I0429 18:43:43.298683   966 sgd_solver.cpp:106] Iteration 100400, lr = 1e-15
I0429 18:44:47.522933   966 solver.cpp:228] Iteration 100500, loss = 0.0673214
I0429 18:44:47.523156   966 solver.cpp:244]     Train net output #0: loss = 0.0673214 (* 1 = 0.0673214 loss)
I0429 18:44:47.523166   966 sgd_solver.cpp:106] Iteration 100500, lr = 1e-15
I0429 18:45:36.909060   966 solver.cpp:228] Iteration 100600, loss = 0.119577
I0429 18:45:36.909226   966 solver.cpp:244]     Train net output #0: loss = 0.119577 (* 1 = 0.119577 loss)
I0429 18:45:36.909234   966 sgd_solver.cpp:106] Iteration 100600, lr = 1e-15
I0429 18:46:26.431288   966 solver.cpp:228] Iteration 100700, loss = 0.130896
I0429 18:46:26.431478   966 solver.cpp:244]     Train net output #0: loss = 0.130896 (* 1 = 0.130896 loss)
I0429 18:46:26.431485   966 sgd_solver.cpp:106] Iteration 100700, lr = 1e-15
I0429 18:47:33.065470   966 solver.cpp:228] Iteration 100800, loss = 0.176752
I0429 18:47:33.065629   966 solver.cpp:244]     Train net output #0: loss = 0.176752 (* 1 = 0.176752 loss)
I0429 18:47:33.065637   966 sgd_solver.cpp:106] Iteration 100800, lr = 1e-15
I0429 18:48:22.460377   966 solver.cpp:228] Iteration 100900, loss = 0.0398679
I0429 18:48:22.460530   966 solver.cpp:244]     Train net output #0: loss = 0.0398679 (* 1 = 0.0398679 loss)
I0429 18:48:22.460537   966 sgd_solver.cpp:106] Iteration 100900, lr = 1e-15
I0429 18:49:11.662159   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_101000.caffemodel
I0429 18:49:17.440759   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_101000.solverstate
I0429 18:49:17.632274   966 solver.cpp:337] Iteration 101000, Testing net (#0)
I0429 18:49:17.632400   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 18:49:17.632406   966 net.cpp:693] Ignoring source layer visualize
I0429 18:49:17.632410   966 net.cpp:693] Ignoring source layer fake
I0429 18:54:10.064040   966 solver.cpp:404]     Test net output #0: loss = 0.179913 (* 1 = 0.179913 loss)
I0429 18:54:10.376097   966 solver.cpp:228] Iteration 101000, loss = 0.172677
I0429 18:54:10.376116   966 solver.cpp:244]     Train net output #0: loss = 0.172677 (* 1 = 0.172677 loss)
I0429 18:54:10.376138   966 sgd_solver.cpp:106] Iteration 101000, lr = 1e-15
I0429 18:54:59.924307   966 solver.cpp:228] Iteration 101100, loss = 0.112566
I0429 18:54:59.924470   966 solver.cpp:244]     Train net output #0: loss = 0.112566 (* 1 = 0.112566 loss)
I0429 18:54:59.924477   966 sgd_solver.cpp:106] Iteration 101100, lr = 1e-15
I0429 18:56:07.717102   966 solver.cpp:228] Iteration 101200, loss = 0.102294
I0429 18:56:07.717274   966 solver.cpp:244]     Train net output #0: loss = 0.102294 (* 1 = 0.102294 loss)
I0429 18:56:07.717283   966 sgd_solver.cpp:106] Iteration 101200, lr = 1e-15
I0429 18:56:57.228397   966 solver.cpp:228] Iteration 101300, loss = 0.0927249
I0429 18:56:57.228562   966 solver.cpp:244]     Train net output #0: loss = 0.0927249 (* 1 = 0.0927249 loss)
I0429 18:56:57.228571   966 sgd_solver.cpp:106] Iteration 101300, lr = 1e-15
I0429 18:57:46.762122   966 solver.cpp:228] Iteration 101400, loss = 0.111466
I0429 18:57:46.762295   966 solver.cpp:244]     Train net output #0: loss = 0.111466 (* 1 = 0.111466 loss)
I0429 18:57:46.762303   966 sgd_solver.cpp:106] Iteration 101400, lr = 1e-15
I0429 18:58:54.585662   966 solver.cpp:228] Iteration 101500, loss = 0.189562
I0429 18:58:54.586027   966 solver.cpp:244]     Train net output #0: loss = 0.189562 (* 1 = 0.189562 loss)
I0429 18:58:54.586035   966 sgd_solver.cpp:106] Iteration 101500, lr = 1e-15
I0429 18:59:44.010254   966 solver.cpp:228] Iteration 101600, loss = 0.239508
I0429 18:59:44.010426   966 solver.cpp:244]     Train net output #0: loss = 0.239508 (* 1 = 0.239508 loss)
I0429 18:59:44.010433   966 sgd_solver.cpp:106] Iteration 101600, lr = 1e-15
I0429 19:00:33.531755   966 solver.cpp:228] Iteration 101700, loss = 0.0839765
I0429 19:00:33.531966   966 solver.cpp:244]     Train net output #0: loss = 0.0839765 (* 1 = 0.0839765 loss)
I0429 19:00:33.531988   966 sgd_solver.cpp:106] Iteration 101700, lr = 1e-15
I0429 19:01:24.962527   966 solver.cpp:228] Iteration 101800, loss = 0.12052
I0429 19:01:24.962714   966 solver.cpp:244]     Train net output #0: loss = 0.12052 (* 1 = 0.12052 loss)
I0429 19:01:24.962721   966 sgd_solver.cpp:106] Iteration 101800, lr = 1e-15
I0429 19:02:14.496047   966 solver.cpp:228] Iteration 101900, loss = 0.0862058
I0429 19:02:14.496206   966 solver.cpp:244]     Train net output #0: loss = 0.0862058 (* 1 = 0.0862058 loss)
I0429 19:02:14.496213   966 sgd_solver.cpp:106] Iteration 101900, lr = 1e-15
I0429 19:03:03.713639   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_102000.caffemodel
I0429 19:03:15.196938   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_102000.solverstate
I0429 19:03:15.404422   966 solver.cpp:337] Iteration 102000, Testing net (#0)
I0429 19:03:15.404544   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 19:03:15.404551   966 net.cpp:693] Ignoring source layer visualize
I0429 19:03:15.404553   966 net.cpp:693] Ignoring source layer fake
I0429 19:08:08.746927   966 solver.cpp:404]     Test net output #0: loss = 0.182143 (* 1 = 0.182143 loss)
I0429 19:08:09.056476   966 solver.cpp:228] Iteration 102000, loss = 0.169041
I0429 19:08:09.056519   966 solver.cpp:244]     Train net output #0: loss = 0.169041 (* 1 = 0.169041 loss)
I0429 19:08:09.056524   966 sgd_solver.cpp:106] Iteration 102000, lr = 1e-15
I0429 19:08:58.598976   966 solver.cpp:228] Iteration 102100, loss = 0.14051
I0429 19:08:58.599135   966 solver.cpp:244]     Train net output #0: loss = 0.14051 (* 1 = 0.14051 loss)
I0429 19:08:58.599143   966 sgd_solver.cpp:106] Iteration 102100, lr = 1e-15
I0429 19:09:51.822940   966 solver.cpp:228] Iteration 102200, loss = 0.118067
I0429 19:09:51.825037   966 solver.cpp:244]     Train net output #0: loss = 0.118067 (* 1 = 0.118067 loss)
I0429 19:09:51.825062   966 sgd_solver.cpp:106] Iteration 102200, lr = 1e-15
I0429 19:10:41.344774   966 solver.cpp:228] Iteration 102300, loss = 0.10599
I0429 19:10:41.344928   966 solver.cpp:244]     Train net output #0: loss = 0.10599 (* 1 = 0.10599 loss)
I0429 19:10:41.344935   966 sgd_solver.cpp:106] Iteration 102300, lr = 1e-15
I0429 19:11:30.863087   966 solver.cpp:228] Iteration 102400, loss = 0.101451
I0429 19:11:30.863240   966 solver.cpp:244]     Train net output #0: loss = 0.101451 (* 1 = 0.101451 loss)
I0429 19:11:30.863246   966 sgd_solver.cpp:106] Iteration 102400, lr = 1e-15
I0429 19:12:22.210588   966 solver.cpp:228] Iteration 102500, loss = 0.0969327
I0429 19:12:22.210765   966 solver.cpp:244]     Train net output #0: loss = 0.0969327 (* 1 = 0.0969327 loss)
I0429 19:12:22.210773   966 sgd_solver.cpp:106] Iteration 102500, lr = 1e-15
I0429 19:13:11.715183   966 solver.cpp:228] Iteration 102600, loss = 0.123303
I0429 19:13:11.715411   966 solver.cpp:244]     Train net output #0: loss = 0.123303 (* 1 = 0.123303 loss)
I0429 19:13:11.715420   966 sgd_solver.cpp:106] Iteration 102600, lr = 1e-15
I0429 19:14:01.236387   966 solver.cpp:228] Iteration 102700, loss = 0.144021
I0429 19:14:01.236542   966 solver.cpp:244]     Train net output #0: loss = 0.144021 (* 1 = 0.144021 loss)
I0429 19:14:01.236548   966 sgd_solver.cpp:106] Iteration 102700, lr = 1e-15
I0429 19:14:52.445482   966 solver.cpp:228] Iteration 102800, loss = 0.0277245
I0429 19:14:52.445648   966 solver.cpp:244]     Train net output #0: loss = 0.0277245 (* 1 = 0.0277245 loss)
I0429 19:14:52.445655   966 sgd_solver.cpp:106] Iteration 102800, lr = 1e-15
I0429 19:15:41.957207   966 solver.cpp:228] Iteration 102900, loss = 0.140004
I0429 19:15:41.957372   966 solver.cpp:244]     Train net output #0: loss = 0.140004 (* 1 = 0.140004 loss)
I0429 19:15:41.957381   966 sgd_solver.cpp:106] Iteration 102900, lr = 1e-15
I0429 19:16:31.160295   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_103000.caffemodel
I0429 19:16:35.841611   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_103000.solverstate
I0429 19:16:36.044103   966 solver.cpp:337] Iteration 103000, Testing net (#0)
I0429 19:16:36.044224   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 19:16:36.044245   966 net.cpp:693] Ignoring source layer visualize
I0429 19:16:36.044247   966 net.cpp:693] Ignoring source layer fake
I0429 19:21:28.663326   966 solver.cpp:404]     Test net output #0: loss = 0.191812 (* 1 = 0.191812 loss)
I0429 19:21:28.972242   966 solver.cpp:228] Iteration 103000, loss = 0.125875
I0429 19:21:28.972260   966 solver.cpp:244]     Train net output #0: loss = 0.125875 (* 1 = 0.125875 loss)
I0429 19:21:28.972281   966 sgd_solver.cpp:106] Iteration 103000, lr = 1e-15
I0429 19:22:20.553673   966 solver.cpp:228] Iteration 103100, loss = 0.150446
I0429 19:22:20.553886   966 solver.cpp:244]     Train net output #0: loss = 0.150446 (* 1 = 0.150446 loss)
I0429 19:22:20.553894   966 sgd_solver.cpp:106] Iteration 103100, lr = 1e-15
I0429 19:23:10.076048   966 solver.cpp:228] Iteration 103200, loss = 0.149236
I0429 19:23:10.076218   966 solver.cpp:244]     Train net output #0: loss = 0.149236 (* 1 = 0.149236 loss)
I0429 19:23:10.076226   966 sgd_solver.cpp:106] Iteration 103200, lr = 1e-15
I0429 19:23:59.627028   966 solver.cpp:228] Iteration 103300, loss = 0.164839
I0429 19:23:59.627180   966 solver.cpp:244]     Train net output #0: loss = 0.164839 (* 1 = 0.164839 loss)
I0429 19:23:59.627187   966 sgd_solver.cpp:106] Iteration 103300, lr = 1e-15
I0429 19:24:50.483561   966 solver.cpp:228] Iteration 103400, loss = 0.111757
I0429 19:24:50.483717   966 solver.cpp:244]     Train net output #0: loss = 0.111757 (* 1 = 0.111757 loss)
I0429 19:24:50.483723   966 sgd_solver.cpp:106] Iteration 103400, lr = 1e-15
I0429 19:25:40.010236   966 solver.cpp:228] Iteration 103500, loss = 0.169519
I0429 19:25:40.010401   966 solver.cpp:244]     Train net output #0: loss = 0.169519 (* 1 = 0.169519 loss)
I0429 19:25:40.010407   966 sgd_solver.cpp:106] Iteration 103500, lr = 1e-15
I0429 19:26:29.535677   966 solver.cpp:228] Iteration 103600, loss = 0.215847
I0429 19:26:29.535825   966 solver.cpp:244]     Train net output #0: loss = 0.215847 (* 1 = 0.215847 loss)
I0429 19:26:29.535832   966 sgd_solver.cpp:106] Iteration 103600, lr = 1e-15
I0429 19:27:20.638396   966 solver.cpp:228] Iteration 103700, loss = 0.0800031
I0429 19:27:20.638556   966 solver.cpp:244]     Train net output #0: loss = 0.0800031 (* 1 = 0.0800031 loss)
I0429 19:27:20.638563   966 sgd_solver.cpp:106] Iteration 103700, lr = 1e-15
I0429 19:28:10.169395   966 solver.cpp:228] Iteration 103800, loss = 0.113658
I0429 19:28:10.169559   966 solver.cpp:244]     Train net output #0: loss = 0.113658 (* 1 = 0.113658 loss)
I0429 19:28:10.169566   966 sgd_solver.cpp:106] Iteration 103800, lr = 1e-15
I0429 19:29:01.506657   966 solver.cpp:228] Iteration 103900, loss = 0.0743007
I0429 19:29:01.506826   966 solver.cpp:244]     Train net output #0: loss = 0.0743007 (* 1 = 0.0743007 loss)
I0429 19:29:01.506834   966 sgd_solver.cpp:106] Iteration 103900, lr = 1e-15
I0429 19:29:50.722578   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_104000.caffemodel
I0429 19:30:11.494086   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_104000.solverstate
I0429 19:30:11.684588   966 solver.cpp:337] Iteration 104000, Testing net (#0)
I0429 19:30:11.684710   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 19:30:11.684716   966 net.cpp:693] Ignoring source layer visualize
I0429 19:30:11.684718   966 net.cpp:693] Ignoring source layer fake
I0429 19:35:03.809267   966 solver.cpp:404]     Test net output #0: loss = 0.185001 (* 1 = 0.185001 loss)
I0429 19:35:04.119346   966 solver.cpp:228] Iteration 104000, loss = 0.15148
I0429 19:35:04.119364   966 solver.cpp:244]     Train net output #0: loss = 0.15148 (* 1 = 0.15148 loss)
I0429 19:35:04.119385   966 sgd_solver.cpp:106] Iteration 104000, lr = 1e-15
I0429 19:35:53.649369   966 solver.cpp:228] Iteration 104100, loss = 0.154122
I0429 19:35:53.649741   966 solver.cpp:244]     Train net output #0: loss = 0.154122 (* 1 = 0.154122 loss)
I0429 19:35:53.649749   966 sgd_solver.cpp:106] Iteration 104100, lr = 1e-15
I0429 19:36:43.177920   966 solver.cpp:228] Iteration 104200, loss = 0.102757
I0429 19:36:43.178073   966 solver.cpp:244]     Train net output #0: loss = 0.102757 (* 1 = 0.102757 loss)
I0429 19:36:43.178081   966 sgd_solver.cpp:106] Iteration 104200, lr = 1e-15
I0429 19:37:37.793938   966 solver.cpp:228] Iteration 104300, loss = 0.192712
I0429 19:37:37.794098   966 solver.cpp:244]     Train net output #0: loss = 0.192712 (* 1 = 0.192712 loss)
I0429 19:37:37.794106   966 sgd_solver.cpp:106] Iteration 104300, lr = 1e-15
I0429 19:38:27.342061   966 solver.cpp:228] Iteration 104400, loss = 0.0928334
I0429 19:38:27.342229   966 solver.cpp:244]     Train net output #0: loss = 0.0928334 (* 1 = 0.0928334 loss)
I0429 19:38:27.342236   966 sgd_solver.cpp:106] Iteration 104400, lr = 1e-15
I0429 19:39:16.868140   966 solver.cpp:228] Iteration 104500, loss = 0.0507603
I0429 19:39:16.868324   966 solver.cpp:244]     Train net output #0: loss = 0.0507603 (* 1 = 0.0507603 loss)
I0429 19:39:16.868332   966 sgd_solver.cpp:106] Iteration 104500, lr = 1e-15
I0429 19:40:08.636744   966 solver.cpp:228] Iteration 104600, loss = 0.141844
I0429 19:40:08.637637   966 solver.cpp:244]     Train net output #0: loss = 0.141844 (* 1 = 0.141844 loss)
I0429 19:40:08.637646   966 sgd_solver.cpp:106] Iteration 104600, lr = 1e-15
I0429 19:40:58.158874   966 solver.cpp:228] Iteration 104700, loss = 0.155214
I0429 19:40:58.159040   966 solver.cpp:244]     Train net output #0: loss = 0.155214 (* 1 = 0.155214 loss)
I0429 19:40:58.159049   966 sgd_solver.cpp:106] Iteration 104700, lr = 1e-15
I0429 19:41:47.688757   966 solver.cpp:228] Iteration 104800, loss = 0.225776
I0429 19:41:47.688925   966 solver.cpp:244]     Train net output #0: loss = 0.225776 (* 1 = 0.225776 loss)
I0429 19:41:47.688931   966 sgd_solver.cpp:106] Iteration 104800, lr = 1e-15
I0429 19:42:37.226825   966 solver.cpp:228] Iteration 104900, loss = 0.182354
I0429 19:42:37.226989   966 solver.cpp:244]     Train net output #0: loss = 0.182354 (* 1 = 0.182354 loss)
I0429 19:42:37.226994   966 sgd_solver.cpp:106] Iteration 104900, lr = 1e-15
I0429 19:43:29.142246   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_105000.caffemodel
I0429 19:43:46.730295   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_105000.solverstate
I0429 19:43:46.934260   966 solver.cpp:337] Iteration 105000, Testing net (#0)
I0429 19:43:46.934365   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 19:43:46.934370   966 net.cpp:693] Ignoring source layer visualize
I0429 19:43:46.934373   966 net.cpp:693] Ignoring source layer fake
I0429 19:48:39.136287   966 solver.cpp:404]     Test net output #0: loss = 0.182398 (* 1 = 0.182398 loss)
I0429 19:48:39.447216   966 solver.cpp:228] Iteration 105000, loss = 0.130571
I0429 19:48:39.447233   966 solver.cpp:244]     Train net output #0: loss = 0.130571 (* 1 = 0.130571 loss)
I0429 19:48:39.447255   966 sgd_solver.cpp:106] Iteration 105000, lr = 1e-16
I0429 19:49:28.974210   966 solver.cpp:228] Iteration 105100, loss = 0.0935151
I0429 19:49:28.974351   966 solver.cpp:244]     Train net output #0: loss = 0.0935151 (* 1 = 0.0935151 loss)
I0429 19:49:28.974359   966 sgd_solver.cpp:106] Iteration 105100, lr = 1e-16
I0429 19:50:18.496119   966 solver.cpp:228] Iteration 105200, loss = 0.137597
I0429 19:50:18.496646   966 solver.cpp:244]     Train net output #0: loss = 0.137597 (* 1 = 0.137597 loss)
I0429 19:50:18.496670   966 sgd_solver.cpp:106] Iteration 105200, lr = 1e-16
I0429 19:51:10.354333   966 solver.cpp:228] Iteration 105300, loss = 0.136355
I0429 19:51:10.354516   966 solver.cpp:244]     Train net output #0: loss = 0.136355 (* 1 = 0.136355 loss)
I0429 19:51:10.354524   966 sgd_solver.cpp:106] Iteration 105300, lr = 1e-16
I0429 19:51:59.879874   966 solver.cpp:228] Iteration 105400, loss = 0.1007
I0429 19:51:59.880077   966 solver.cpp:244]     Train net output #0: loss = 0.1007 (* 1 = 0.1007 loss)
I0429 19:51:59.880089   966 sgd_solver.cpp:106] Iteration 105400, lr = 1e-16
I0429 19:52:49.419279   966 solver.cpp:228] Iteration 105500, loss = 0.146504
I0429 19:52:49.419438   966 solver.cpp:244]     Train net output #0: loss = 0.146504 (* 1 = 0.146504 loss)
I0429 19:52:49.419445   966 sgd_solver.cpp:106] Iteration 105500, lr = 1e-16
I0429 19:53:43.048300   966 solver.cpp:228] Iteration 105600, loss = 0.0624943
I0429 19:53:43.048465   966 solver.cpp:244]     Train net output #0: loss = 0.0624943 (* 1 = 0.0624943 loss)
I0429 19:53:43.048472   966 sgd_solver.cpp:106] Iteration 105600, lr = 1e-16
I0429 19:54:32.520428   966 solver.cpp:228] Iteration 105700, loss = 0.0620655
I0429 19:54:32.520615   966 solver.cpp:244]     Train net output #0: loss = 0.0620655 (* 1 = 0.0620655 loss)
I0429 19:54:32.520623   966 sgd_solver.cpp:106] Iteration 105700, lr = 1e-16
I0429 19:55:22.031076   966 solver.cpp:228] Iteration 105800, loss = 0.0731547
I0429 19:55:22.032398   966 solver.cpp:244]     Train net output #0: loss = 0.0731547 (* 1 = 0.0731547 loss)
I0429 19:55:22.032407   966 sgd_solver.cpp:106] Iteration 105800, lr = 1e-16
I0429 19:56:11.545208   966 solver.cpp:228] Iteration 105900, loss = 0.145154
I0429 19:56:11.545382   966 solver.cpp:244]     Train net output #0: loss = 0.145154 (* 1 = 0.145154 loss)
I0429 19:56:11.545388   966 sgd_solver.cpp:106] Iteration 105900, lr = 1e-16
I0429 19:57:04.565254   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_106000.caffemodel
I0429 19:57:11.452780   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_106000.solverstate
I0429 19:57:11.646057   966 solver.cpp:337] Iteration 106000, Testing net (#0)
I0429 19:57:11.646179   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 19:57:11.646185   966 net.cpp:693] Ignoring source layer visualize
I0429 19:57:11.646188   966 net.cpp:693] Ignoring source layer fake
I0429 20:02:03.469178   966 solver.cpp:404]     Test net output #0: loss = 0.179659 (* 1 = 0.179659 loss)
I0429 20:02:03.779006   966 solver.cpp:228] Iteration 106000, loss = 0.64817
I0429 20:02:03.779026   966 solver.cpp:244]     Train net output #0: loss = 0.64817 (* 1 = 0.64817 loss)
I0429 20:02:03.779047   966 sgd_solver.cpp:106] Iteration 106000, lr = 1e-16
I0429 20:02:53.301332   966 solver.cpp:228] Iteration 106100, loss = 0.0825179
I0429 20:02:53.301514   966 solver.cpp:244]     Train net output #0: loss = 0.0825179 (* 1 = 0.0825179 loss)
I0429 20:02:53.301523   966 sgd_solver.cpp:106] Iteration 106100, lr = 1e-16
I0429 20:03:42.828220   966 solver.cpp:228] Iteration 106200, loss = 0.103853
I0429 20:03:42.828383   966 solver.cpp:244]     Train net output #0: loss = 0.103853 (* 1 = 0.103853 loss)
I0429 20:03:42.828390   966 sgd_solver.cpp:106] Iteration 106200, lr = 1e-16
I0429 20:04:34.949555   966 solver.cpp:228] Iteration 106300, loss = 0.112289
I0429 20:04:34.949728   966 solver.cpp:244]     Train net output #0: loss = 0.112289 (* 1 = 0.112289 loss)
I0429 20:04:34.949735   966 sgd_solver.cpp:106] Iteration 106300, lr = 1e-16
I0429 20:05:24.480414   966 solver.cpp:228] Iteration 106400, loss = 0.11029
I0429 20:05:24.480571   966 solver.cpp:244]     Train net output #0: loss = 0.11029 (* 1 = 0.11029 loss)
I0429 20:05:24.480577   966 sgd_solver.cpp:106] Iteration 106400, lr = 1e-16
I0429 20:06:16.632419   966 solver.cpp:228] Iteration 106500, loss = 0.125392
I0429 20:06:16.632629   966 solver.cpp:244]     Train net output #0: loss = 0.125392 (* 1 = 0.125392 loss)
I0429 20:06:16.632647   966 sgd_solver.cpp:106] Iteration 106500, lr = 1e-16
I0429 20:07:06.159828   966 solver.cpp:228] Iteration 106600, loss = 0.0949696
I0429 20:07:06.160066   966 solver.cpp:244]     Train net output #0: loss = 0.0949696 (* 1 = 0.0949696 loss)
I0429 20:07:06.160082   966 sgd_solver.cpp:106] Iteration 106600, lr = 1e-16
I0429 20:07:55.685430   966 solver.cpp:228] Iteration 106700, loss = 0.161967
I0429 20:07:55.685603   966 solver.cpp:244]     Train net output #0: loss = 0.161967 (* 1 = 0.161967 loss)
I0429 20:07:55.685611   966 sgd_solver.cpp:106] Iteration 106700, lr = 1e-16
I0429 20:08:47.013370   966 solver.cpp:228] Iteration 106800, loss = 0.0778125
I0429 20:08:47.013551   966 solver.cpp:244]     Train net output #0: loss = 0.0778125 (* 1 = 0.0778125 loss)
I0429 20:08:47.013558   966 sgd_solver.cpp:106] Iteration 106800, lr = 1e-16
I0429 20:09:36.526845   966 solver.cpp:228] Iteration 106900, loss = 0.123408
I0429 20:09:36.527001   966 solver.cpp:244]     Train net output #0: loss = 0.123408 (* 1 = 0.123408 loss)
I0429 20:09:36.527009   966 sgd_solver.cpp:106] Iteration 106900, lr = 1e-16
I0429 20:10:25.736543   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_107000.caffemodel
I0429 20:10:42.570132   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_107000.solverstate
I0429 20:10:42.763057   966 solver.cpp:337] Iteration 107000, Testing net (#0)
I0429 20:10:42.763176   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 20:10:42.763182   966 net.cpp:693] Ignoring source layer visualize
I0429 20:10:42.763185   966 net.cpp:693] Ignoring source layer fake
I0429 20:15:34.674872   966 solver.cpp:404]     Test net output #0: loss = 0.183064 (* 1 = 0.183064 loss)
I0429 20:15:34.986125   966 solver.cpp:228] Iteration 107000, loss = 0.0923919
I0429 20:15:34.986161   966 solver.cpp:244]     Train net output #0: loss = 0.0923919 (* 1 = 0.0923919 loss)
I0429 20:15:34.986167   966 sgd_solver.cpp:106] Iteration 107000, lr = 1e-16
I0429 20:16:27.113107   966 solver.cpp:228] Iteration 107100, loss = 0.192653
I0429 20:16:27.113276   966 solver.cpp:244]     Train net output #0: loss = 0.192653 (* 1 = 0.192653 loss)
I0429 20:16:27.113283   966 sgd_solver.cpp:106] Iteration 107100, lr = 1e-16
I0429 20:17:16.634464   966 solver.cpp:228] Iteration 107200, loss = 0.146162
I0429 20:17:16.634629   966 solver.cpp:244]     Train net output #0: loss = 0.146162 (* 1 = 0.146162 loss)
I0429 20:17:16.634637   966 sgd_solver.cpp:106] Iteration 107200, lr = 1e-16
I0429 20:18:06.161937   966 solver.cpp:228] Iteration 107300, loss = 0.1828
I0429 20:18:06.162101   966 solver.cpp:244]     Train net output #0: loss = 0.1828 (* 1 = 0.1828 loss)
I0429 20:18:06.162107   966 sgd_solver.cpp:106] Iteration 107300, lr = 1e-16
I0429 20:19:02.793951   966 solver.cpp:228] Iteration 107400, loss = 0.0745063
I0429 20:19:02.794178   966 solver.cpp:244]     Train net output #0: loss = 0.0745063 (* 1 = 0.0745063 loss)
I0429 20:19:02.794198   966 sgd_solver.cpp:106] Iteration 107400, lr = 1e-16
I0429 20:19:52.294613   966 solver.cpp:228] Iteration 107500, loss = 0.143625
I0429 20:19:52.294781   966 solver.cpp:244]     Train net output #0: loss = 0.143625 (* 1 = 0.143625 loss)
I0429 20:19:52.294795   966 sgd_solver.cpp:106] Iteration 107500, lr = 1e-16
I0429 20:20:41.856212   966 solver.cpp:228] Iteration 107600, loss = 0.120895
I0429 20:20:41.856369   966 solver.cpp:244]     Train net output #0: loss = 0.120895 (* 1 = 0.120895 loss)
I0429 20:20:41.856376   966 sgd_solver.cpp:106] Iteration 107600, lr = 1e-16
I0429 20:21:42.567312   966 solver.cpp:228] Iteration 107700, loss = 0.223446
I0429 20:21:42.567482   966 solver.cpp:244]     Train net output #0: loss = 0.223446 (* 1 = 0.223446 loss)
I0429 20:21:42.567490   966 sgd_solver.cpp:106] Iteration 107700, lr = 1e-16
I0429 20:22:32.047405   966 solver.cpp:228] Iteration 107800, loss = 0.178178
I0429 20:22:32.048431   966 solver.cpp:244]     Train net output #0: loss = 0.178178 (* 1 = 0.178178 loss)
I0429 20:22:32.048439   966 sgd_solver.cpp:106] Iteration 107800, lr = 1e-16
I0429 20:23:21.544911   966 solver.cpp:228] Iteration 107900, loss = 0.151271
I0429 20:23:21.545109   966 solver.cpp:244]     Train net output #0: loss = 0.151271 (* 1 = 0.151271 loss)
I0429 20:23:21.545115   966 sgd_solver.cpp:106] Iteration 107900, lr = 1e-16
I0429 20:24:10.772764   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_108000.caffemodel
I0429 20:24:33.246160   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_108000.solverstate
I0429 20:24:33.450599   966 solver.cpp:337] Iteration 108000, Testing net (#0)
I0429 20:24:33.450706   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 20:24:33.450711   966 net.cpp:693] Ignoring source layer visualize
I0429 20:24:33.450713   966 net.cpp:693] Ignoring source layer fake
I0429 20:29:26.661432   966 solver.cpp:404]     Test net output #0: loss = 0.19235 (* 1 = 0.19235 loss)
I0429 20:29:26.970166   966 solver.cpp:228] Iteration 108000, loss = 0.0934953
I0429 20:29:26.970186   966 solver.cpp:244]     Train net output #0: loss = 0.0934953 (* 1 = 0.0934953 loss)
I0429 20:29:26.970209   966 sgd_solver.cpp:106] Iteration 108000, lr = 1e-16
I0429 20:30:27.571663   966 solver.cpp:228] Iteration 108100, loss = 0.0772105
I0429 20:30:27.571889   966 solver.cpp:244]     Train net output #0: loss = 0.0772105 (* 1 = 0.0772105 loss)
I0429 20:30:27.571897   966 sgd_solver.cpp:106] Iteration 108100, lr = 1e-16
I0429 20:31:17.058768   966 solver.cpp:228] Iteration 108200, loss = 0.129837
I0429 20:31:17.058938   966 solver.cpp:244]     Train net output #0: loss = 0.129837 (* 1 = 0.129837 loss)
I0429 20:31:17.058944   966 sgd_solver.cpp:106] Iteration 108200, lr = 1e-16
I0429 20:32:06.578300   966 solver.cpp:228] Iteration 108300, loss = 0.112928
I0429 20:32:06.578541   966 solver.cpp:244]     Train net output #0: loss = 0.112928 (* 1 = 0.112928 loss)
I0429 20:32:06.578552   966 sgd_solver.cpp:106] Iteration 108300, lr = 1e-16
I0429 20:33:06.412750   966 solver.cpp:228] Iteration 108400, loss = 0.0695714
I0429 20:33:06.412904   966 solver.cpp:244]     Train net output #0: loss = 0.0695714 (* 1 = 0.0695714 loss)
I0429 20:33:06.412911   966 sgd_solver.cpp:106] Iteration 108400, lr = 1e-16
I0429 20:33:55.879474   966 solver.cpp:228] Iteration 108500, loss = 0.120818
I0429 20:33:55.879662   966 solver.cpp:244]     Train net output #0: loss = 0.120818 (* 1 = 0.120818 loss)
I0429 20:33:55.879670   966 sgd_solver.cpp:106] Iteration 108500, lr = 1e-16
I0429 20:34:45.420066   966 solver.cpp:228] Iteration 108600, loss = 0.183484
I0429 20:34:45.420229   966 solver.cpp:244]     Train net output #0: loss = 0.183484 (* 1 = 0.183484 loss)
I0429 20:34:45.420236   966 sgd_solver.cpp:106] Iteration 108600, lr = 1e-16
I0429 20:35:34.926841   966 solver.cpp:228] Iteration 108700, loss = 0.140199
I0429 20:35:34.927009   966 solver.cpp:244]     Train net output #0: loss = 0.140199 (* 1 = 0.140199 loss)
I0429 20:35:34.927017   966 sgd_solver.cpp:106] Iteration 108700, lr = 1e-16
I0429 20:36:34.924049   966 solver.cpp:228] Iteration 108800, loss = 0.13498
I0429 20:36:34.924221   966 solver.cpp:244]     Train net output #0: loss = 0.13498 (* 1 = 0.13498 loss)
I0429 20:36:34.924228   966 sgd_solver.cpp:106] Iteration 108800, lr = 1e-16
I0429 20:37:24.413312   966 solver.cpp:228] Iteration 108900, loss = 0.119051
I0429 20:37:24.413511   966 solver.cpp:244]     Train net output #0: loss = 0.119051 (* 1 = 0.119051 loss)
I0429 20:37:24.413517   966 sgd_solver.cpp:106] Iteration 108900, lr = 1e-16
I0429 20:38:13.613914   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_109000.caffemodel
I0429 20:38:32.136584   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_109000.solverstate
I0429 20:38:32.344445   966 solver.cpp:337] Iteration 109000, Testing net (#0)
I0429 20:38:32.344573   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 20:38:32.344595   966 net.cpp:693] Ignoring source layer visualize
I0429 20:38:32.344599   966 net.cpp:693] Ignoring source layer fake
I0429 20:43:24.621064   966 solver.cpp:404]     Test net output #0: loss = 0.183715 (* 1 = 0.183715 loss)
I0429 20:43:24.930060   966 solver.cpp:228] Iteration 109000, loss = 0.112972
I0429 20:43:24.930099   966 solver.cpp:244]     Train net output #0: loss = 0.112972 (* 1 = 0.112972 loss)
I0429 20:43:24.930105   966 sgd_solver.cpp:106] Iteration 109000, lr = 1e-16
I0429 20:44:24.301127   966 solver.cpp:228] Iteration 109100, loss = 0.111991
I0429 20:44:24.301277   966 solver.cpp:244]     Train net output #0: loss = 0.111991 (* 1 = 0.111991 loss)
I0429 20:44:24.301285   966 sgd_solver.cpp:106] Iteration 109100, lr = 1e-16
I0429 20:45:13.767714   966 solver.cpp:228] Iteration 109200, loss = 0.10855
I0429 20:45:13.767879   966 solver.cpp:244]     Train net output #0: loss = 0.10855 (* 1 = 0.10855 loss)
I0429 20:45:13.767885   966 sgd_solver.cpp:106] Iteration 109200, lr = 1e-16
I0429 20:46:03.273319   966 solver.cpp:228] Iteration 109300, loss = 0.141101
I0429 20:46:03.273488   966 solver.cpp:244]     Train net output #0: loss = 0.141101 (* 1 = 0.141101 loss)
I0429 20:46:03.273495   966 sgd_solver.cpp:106] Iteration 109300, lr = 1e-16
I0429 20:47:03.093032   966 solver.cpp:228] Iteration 109400, loss = 0.10829
I0429 20:47:03.093209   966 solver.cpp:244]     Train net output #0: loss = 0.10829 (* 1 = 0.10829 loss)
I0429 20:47:03.093219   966 sgd_solver.cpp:106] Iteration 109400, lr = 1e-16
I0429 20:47:52.586921   966 solver.cpp:228] Iteration 109500, loss = 0.152081
I0429 20:47:52.587069   966 solver.cpp:244]     Train net output #0: loss = 0.152081 (* 1 = 0.152081 loss)
I0429 20:47:52.587075   966 sgd_solver.cpp:106] Iteration 109500, lr = 1e-16
I0429 20:48:50.989723   966 solver.cpp:228] Iteration 109600, loss = 0.267294
I0429 20:48:50.989919   966 solver.cpp:244]     Train net output #0: loss = 0.267294 (* 1 = 0.267294 loss)
I0429 20:48:50.989926   966 sgd_solver.cpp:106] Iteration 109600, lr = 1e-16
I0429 20:49:40.484288   966 solver.cpp:228] Iteration 109700, loss = 0.177216
I0429 20:49:40.484455   966 solver.cpp:244]     Train net output #0: loss = 0.177216 (* 1 = 0.177216 loss)
I0429 20:49:40.484462   966 sgd_solver.cpp:106] Iteration 109700, lr = 1e-16
I0429 20:50:30.019372   966 solver.cpp:228] Iteration 109800, loss = 0.357956
I0429 20:50:30.019529   966 solver.cpp:244]     Train net output #0: loss = 0.357956 (* 1 = 0.357956 loss)
I0429 20:50:30.019537   966 sgd_solver.cpp:106] Iteration 109800, lr = 1e-16
I0429 20:51:29.089500   966 solver.cpp:228] Iteration 109900, loss = 0.134689
I0429 20:51:29.089668   966 solver.cpp:244]     Train net output #0: loss = 0.134689 (* 1 = 0.134689 loss)
I0429 20:51:29.089674   966 sgd_solver.cpp:106] Iteration 109900, lr = 1e-16
I0429 20:52:18.282968   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_110000.caffemodel
I0429 20:52:24.971313   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_110000.solverstate
I0429 20:52:25.182404   966 solver.cpp:337] Iteration 110000, Testing net (#0)
I0429 20:52:25.182514   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 20:52:25.182521   966 net.cpp:693] Ignoring source layer visualize
I0429 20:52:25.182524   966 net.cpp:693] Ignoring source layer fake
I0429 20:57:17.649073   966 solver.cpp:404]     Test net output #0: loss = 0.182524 (* 1 = 0.182524 loss)
I0429 20:57:17.959442   966 solver.cpp:228] Iteration 110000, loss = 0.0299384
I0429 20:57:17.959463   966 solver.cpp:244]     Train net output #0: loss = 0.0299384 (* 1 = 0.0299384 loss)
I0429 20:57:17.959484   966 sgd_solver.cpp:106] Iteration 110000, lr = 1e-16
I0429 20:58:07.514995   966 solver.cpp:228] Iteration 110100, loss = 0.130745
I0429 20:58:07.515182   966 solver.cpp:244]     Train net output #0: loss = 0.130745 (* 1 = 0.130745 loss)
I0429 20:58:07.515189   966 sgd_solver.cpp:106] Iteration 110100, lr = 1e-16
I0429 20:59:07.513384   966 solver.cpp:228] Iteration 110200, loss = 0.192324
I0429 20:59:07.513576   966 solver.cpp:244]     Train net output #0: loss = 0.192324 (* 1 = 0.192324 loss)
I0429 20:59:07.513586   966 sgd_solver.cpp:106] Iteration 110200, lr = 1e-16
I0429 20:59:56.998704   966 solver.cpp:228] Iteration 110300, loss = 0.191782
I0429 20:59:56.998870   966 solver.cpp:244]     Train net output #0: loss = 0.191782 (* 1 = 0.191782 loss)
I0429 20:59:56.998878   966 sgd_solver.cpp:106] Iteration 110300, lr = 1e-16
I0429 21:00:46.535343   966 solver.cpp:228] Iteration 110400, loss = 0.184476
I0429 21:00:46.535509   966 solver.cpp:244]     Train net output #0: loss = 0.184476 (* 1 = 0.184476 loss)
I0429 21:00:46.535517   966 sgd_solver.cpp:106] Iteration 110400, lr = 1e-16
I0429 21:01:46.538530   966 solver.cpp:228] Iteration 110500, loss = 0.135508
I0429 21:01:46.538722   966 solver.cpp:244]     Train net output #0: loss = 0.135508 (* 1 = 0.135508 loss)
I0429 21:01:46.538730   966 sgd_solver.cpp:106] Iteration 110500, lr = 1e-16
I0429 21:02:36.039371   966 solver.cpp:228] Iteration 110600, loss = 0.107802
I0429 21:02:36.039541   966 solver.cpp:244]     Train net output #0: loss = 0.107802 (* 1 = 0.107802 loss)
I0429 21:02:36.039548   966 sgd_solver.cpp:106] Iteration 110600, lr = 1e-16
I0429 21:03:25.555745   966 solver.cpp:228] Iteration 110700, loss = 0.11833
I0429 21:03:25.555912   966 solver.cpp:244]     Train net output #0: loss = 0.11833 (* 1 = 0.11833 loss)
I0429 21:03:25.555918   966 sgd_solver.cpp:106] Iteration 110700, lr = 1e-16
I0429 21:04:15.085340   966 solver.cpp:228] Iteration 110800, loss = 0.102185
I0429 21:04:15.085522   966 solver.cpp:244]     Train net output #0: loss = 0.102185 (* 1 = 0.102185 loss)
I0429 21:04:15.085530   966 sgd_solver.cpp:106] Iteration 110800, lr = 1e-16
I0429 21:05:15.012747   966 solver.cpp:228] Iteration 110900, loss = 0.140363
I0429 21:05:15.012920   966 solver.cpp:244]     Train net output #0: loss = 0.140363 (* 1 = 0.140363 loss)
I0429 21:05:15.012928   966 sgd_solver.cpp:106] Iteration 110900, lr = 1e-16
I0429 21:06:04.208395   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_111000.caffemodel
I0429 21:06:13.316856   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_111000.solverstate
I0429 21:06:13.522439   966 solver.cpp:337] Iteration 111000, Testing net (#0)
I0429 21:06:13.522580   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 21:06:13.522586   966 net.cpp:693] Ignoring source layer visualize
I0429 21:06:13.522589   966 net.cpp:693] Ignoring source layer fake
I0429 21:11:05.825923   966 solver.cpp:404]     Test net output #0: loss = 0.17923 (* 1 = 0.17923 loss)
I0429 21:11:06.135139   966 solver.cpp:228] Iteration 111000, loss = 0.209503
I0429 21:11:06.135156   966 solver.cpp:244]     Train net output #0: loss = 0.209503 (* 1 = 0.209503 loss)
I0429 21:11:06.135177   966 sgd_solver.cpp:106] Iteration 111000, lr = 1e-16
I0429 21:11:55.675972   966 solver.cpp:228] Iteration 111100, loss = 0.0912623
I0429 21:11:55.676563   966 solver.cpp:244]     Train net output #0: loss = 0.0912623 (* 1 = 0.0912623 loss)
I0429 21:11:55.676570   966 sgd_solver.cpp:106] Iteration 111100, lr = 1e-16
I0429 21:12:57.618963   966 solver.cpp:228] Iteration 111200, loss = 0.112567
I0429 21:12:57.619123   966 solver.cpp:244]     Train net output #0: loss = 0.112567 (* 1 = 0.112567 loss)
I0429 21:12:57.619130   966 sgd_solver.cpp:106] Iteration 111200, lr = 1e-16
I0429 21:13:47.032850   966 solver.cpp:228] Iteration 111300, loss = 0.269873
I0429 21:13:47.033002   966 solver.cpp:244]     Train net output #0: loss = 0.269873 (* 1 = 0.269873 loss)
I0429 21:13:47.033008   966 sgd_solver.cpp:106] Iteration 111300, lr = 1e-16
I0429 21:14:36.534092   966 solver.cpp:228] Iteration 111400, loss = 0.122057
I0429 21:14:36.534274   966 solver.cpp:244]     Train net output #0: loss = 0.122057 (* 1 = 0.122057 loss)
I0429 21:14:36.534281   966 sgd_solver.cpp:106] Iteration 111400, lr = 1e-16
I0429 21:15:26.058068   966 solver.cpp:228] Iteration 111500, loss = 0.285585
I0429 21:15:26.058217   966 solver.cpp:244]     Train net output #0: loss = 0.285585 (* 1 = 0.285585 loss)
I0429 21:15:26.058223   966 sgd_solver.cpp:106] Iteration 111500, lr = 1e-16
I0429 21:16:27.193370   966 solver.cpp:228] Iteration 111600, loss = 0.0759248
I0429 21:16:27.193625   966 solver.cpp:244]     Train net output #0: loss = 0.0759248 (* 1 = 0.0759248 loss)
I0429 21:16:27.193645   966 sgd_solver.cpp:106] Iteration 111600, lr = 1e-16
I0429 21:17:16.690876   966 solver.cpp:228] Iteration 111700, loss = 0.0979185
I0429 21:17:16.691040   966 solver.cpp:244]     Train net output #0: loss = 0.0979185 (* 1 = 0.0979185 loss)
I0429 21:17:16.691047   966 sgd_solver.cpp:106] Iteration 111700, lr = 1e-16
I0429 21:18:06.192440   966 solver.cpp:228] Iteration 111800, loss = 0.108735
I0429 21:18:06.192605   966 solver.cpp:244]     Train net output #0: loss = 0.108735 (* 1 = 0.108735 loss)
I0429 21:18:06.192611   966 sgd_solver.cpp:106] Iteration 111800, lr = 1e-16
I0429 21:18:58.470129   966 solver.cpp:228] Iteration 111900, loss = 0.118699
I0429 21:18:58.470288   966 solver.cpp:244]     Train net output #0: loss = 0.118699 (* 1 = 0.118699 loss)
I0429 21:18:58.470294   966 sgd_solver.cpp:106] Iteration 111900, lr = 1e-16
I0429 21:19:47.671521   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_112000.caffemodel
I0429 21:19:54.591948   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_112000.solverstate
I0429 21:19:54.780298   966 solver.cpp:337] Iteration 112000, Testing net (#0)
I0429 21:19:54.780422   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 21:19:54.780444   966 net.cpp:693] Ignoring source layer visualize
I0429 21:19:54.780447   966 net.cpp:693] Ignoring source layer fake
I0429 21:24:47.373097   966 solver.cpp:404]     Test net output #0: loss = 0.183682 (* 1 = 0.183682 loss)
I0429 21:24:47.685022   966 solver.cpp:228] Iteration 112000, loss = 0.147342
I0429 21:24:47.685041   966 solver.cpp:244]     Train net output #0: loss = 0.147342 (* 1 = 0.147342 loss)
I0429 21:24:47.685062   966 sgd_solver.cpp:106] Iteration 112000, lr = 1e-16
I0429 21:25:37.206737   966 solver.cpp:228] Iteration 112100, loss = 0.0874973
I0429 21:25:37.206872   966 solver.cpp:244]     Train net output #0: loss = 0.0874973 (* 1 = 0.0874973 loss)
I0429 21:25:37.206878   966 sgd_solver.cpp:106] Iteration 112100, lr = 1e-16
I0429 21:26:34.378962   966 solver.cpp:228] Iteration 112200, loss = 0.109747
I0429 21:26:34.379175   966 solver.cpp:244]     Train net output #0: loss = 0.109747 (* 1 = 0.109747 loss)
I0429 21:26:34.379184   966 sgd_solver.cpp:106] Iteration 112200, lr = 1e-16
I0429 21:27:23.844529   966 solver.cpp:228] Iteration 112300, loss = 0.05993
I0429 21:27:23.844689   966 solver.cpp:244]     Train net output #0: loss = 0.05993 (* 1 = 0.05993 loss)
I0429 21:27:23.844697   966 sgd_solver.cpp:106] Iteration 112300, lr = 1e-16
I0429 21:28:13.341677   966 solver.cpp:228] Iteration 112400, loss = 0.118169
I0429 21:28:13.341835   966 solver.cpp:244]     Train net output #0: loss = 0.118169 (* 1 = 0.118169 loss)
I0429 21:28:13.341842   966 sgd_solver.cpp:106] Iteration 112400, lr = 1e-16
I0429 21:29:07.494521   966 solver.cpp:228] Iteration 112500, loss = 0.131476
I0429 21:29:07.494683   966 solver.cpp:244]     Train net output #0: loss = 0.131476 (* 1 = 0.131476 loss)
I0429 21:29:07.494691   966 sgd_solver.cpp:106] Iteration 112500, lr = 1e-17
I0429 21:29:56.998136   966 solver.cpp:228] Iteration 112600, loss = 0.103932
I0429 21:29:56.998325   966 solver.cpp:244]     Train net output #0: loss = 0.103932 (* 1 = 0.103932 loss)
I0429 21:29:56.998332   966 sgd_solver.cpp:106] Iteration 112600, lr = 1e-17
I0429 21:30:46.519912   966 solver.cpp:228] Iteration 112700, loss = 0.101921
I0429 21:30:46.520061   966 solver.cpp:244]     Train net output #0: loss = 0.101921 (* 1 = 0.101921 loss)
I0429 21:30:46.520068   966 sgd_solver.cpp:106] Iteration 112700, lr = 1e-17
I0429 21:31:50.714990   966 solver.cpp:228] Iteration 112800, loss = 0.151327
I0429 21:31:50.715168   966 solver.cpp:244]     Train net output #0: loss = 0.151327 (* 1 = 0.151327 loss)
I0429 21:31:50.715176   966 sgd_solver.cpp:106] Iteration 112800, lr = 1e-17
I0429 21:32:40.208540   966 solver.cpp:228] Iteration 112900, loss = 0.151084
I0429 21:32:40.208732   966 solver.cpp:244]     Train net output #0: loss = 0.151084 (* 1 = 0.151084 loss)
I0429 21:32:40.208739   966 sgd_solver.cpp:106] Iteration 112900, lr = 1e-17
I0429 21:33:44.144595   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_113000.caffemodel
I0429 21:33:50.363726   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_113000.solverstate
I0429 21:33:50.558197   966 solver.cpp:337] Iteration 113000, Testing net (#0)
I0429 21:33:50.558315   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 21:33:50.558337   966 net.cpp:693] Ignoring source layer visualize
I0429 21:33:50.558341   966 net.cpp:693] Ignoring source layer fake
I0429 21:38:42.593051   966 solver.cpp:404]     Test net output #0: loss = 0.191943 (* 1 = 0.191943 loss)
I0429 21:38:42.903391   966 solver.cpp:228] Iteration 113000, loss = 0.0961503
I0429 21:38:42.903429   966 solver.cpp:244]     Train net output #0: loss = 0.0961503 (* 1 = 0.0961503 loss)
I0429 21:38:42.903436   966 sgd_solver.cpp:106] Iteration 113000, lr = 1e-17
I0429 21:39:32.422960   966 solver.cpp:228] Iteration 113100, loss = 0.108403
I0429 21:39:32.423117   966 solver.cpp:244]     Train net output #0: loss = 0.108403 (* 1 = 0.108403 loss)
I0429 21:39:32.423125   966 sgd_solver.cpp:106] Iteration 113100, lr = 1e-17
I0429 21:40:21.969892   966 solver.cpp:228] Iteration 113200, loss = 0.434707
I0429 21:40:21.970036   966 solver.cpp:244]     Train net output #0: loss = 0.434707 (* 1 = 0.434707 loss)
I0429 21:40:21.970043   966 sgd_solver.cpp:106] Iteration 113200, lr = 1e-17
I0429 21:41:26.516031   966 solver.cpp:228] Iteration 113300, loss = 0.0746405
I0429 21:41:26.516191   966 solver.cpp:244]     Train net output #0: loss = 0.0746405 (* 1 = 0.0746405 loss)
I0429 21:41:26.516196   966 sgd_solver.cpp:106] Iteration 113300, lr = 1e-17
I0429 21:42:15.916502   966 solver.cpp:228] Iteration 113400, loss = 0.099957
I0429 21:42:15.916666   966 solver.cpp:244]     Train net output #0: loss = 0.099957 (* 1 = 0.099957 loss)
I0429 21:42:15.916684   966 sgd_solver.cpp:106] Iteration 113400, lr = 1e-17
I0429 21:43:05.413277   966 solver.cpp:228] Iteration 113500, loss = 0.257702
I0429 21:43:05.413450   966 solver.cpp:244]     Train net output #0: loss = 0.257702 (* 1 = 0.257702 loss)
I0429 21:43:05.413462   966 sgd_solver.cpp:106] Iteration 113500, lr = 1e-17
I0429 21:43:54.938241   966 solver.cpp:228] Iteration 113600, loss = 0.202821
I0429 21:43:54.938386   966 solver.cpp:244]     Train net output #0: loss = 0.202821 (* 1 = 0.202821 loss)
I0429 21:43:54.938392   966 sgd_solver.cpp:106] Iteration 113600, lr = 1e-17
I0429 21:44:46.390931   966 solver.cpp:228] Iteration 113700, loss = 0.208304
I0429 21:44:46.391093   966 solver.cpp:244]     Train net output #0: loss = 0.208304 (* 1 = 0.208304 loss)
I0429 21:44:46.391100   966 sgd_solver.cpp:106] Iteration 113700, lr = 1e-17
I0429 21:45:35.912277   966 solver.cpp:228] Iteration 113800, loss = 0.14138
I0429 21:45:35.912446   966 solver.cpp:244]     Train net output #0: loss = 0.14138 (* 1 = 0.14138 loss)
I0429 21:45:35.912452   966 sgd_solver.cpp:106] Iteration 113800, lr = 1e-17
I0429 21:46:25.454473   966 solver.cpp:228] Iteration 113900, loss = 0.161062
I0429 21:46:25.454651   966 solver.cpp:244]     Train net output #0: loss = 0.161062 (* 1 = 0.161062 loss)
I0429 21:46:25.454659   966 sgd_solver.cpp:106] Iteration 113900, lr = 1e-17
I0429 21:47:16.552253   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_114000.caffemodel
I0429 21:47:26.491689   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_114000.solverstate
I0429 21:47:26.746457   966 solver.cpp:337] Iteration 114000, Testing net (#0)
I0429 21:47:26.746603   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 21:47:26.746613   966 net.cpp:693] Ignoring source layer visualize
I0429 21:47:26.746615   966 net.cpp:693] Ignoring source layer fake
I0429 21:52:18.691164   966 solver.cpp:404]     Test net output #0: loss = 0.183669 (* 1 = 0.183669 loss)
I0429 21:52:19.004220   966 solver.cpp:228] Iteration 114000, loss = 0.102911
I0429 21:52:19.004256   966 solver.cpp:244]     Train net output #0: loss = 0.102911 (* 1 = 0.102911 loss)
I0429 21:52:19.004262   966 sgd_solver.cpp:106] Iteration 114000, lr = 1e-17
I0429 21:53:08.536674   966 solver.cpp:228] Iteration 114100, loss = 0.0888837
I0429 21:53:08.536835   966 solver.cpp:244]     Train net output #0: loss = 0.0888837 (* 1 = 0.0888837 loss)
I0429 21:53:08.536844   966 sgd_solver.cpp:106] Iteration 114100, lr = 1e-17
I0429 21:53:58.072173   966 solver.cpp:228] Iteration 114200, loss = 0.147777
I0429 21:53:58.072376   966 solver.cpp:244]     Train net output #0: loss = 0.147777 (* 1 = 0.147777 loss)
I0429 21:53:58.072394   966 sgd_solver.cpp:106] Iteration 114200, lr = 1e-17
I0429 21:55:04.809325   966 solver.cpp:228] Iteration 114300, loss = 0.107484
I0429 21:55:04.809504   966 solver.cpp:244]     Train net output #0: loss = 0.107484 (* 1 = 0.107484 loss)
I0429 21:55:04.809511   966 sgd_solver.cpp:106] Iteration 114300, lr = 1e-17
I0429 21:55:54.153234   966 solver.cpp:228] Iteration 114400, loss = 0.118582
I0429 21:55:54.153394   966 solver.cpp:244]     Train net output #0: loss = 0.118582 (* 1 = 0.118582 loss)
I0429 21:55:54.153400   966 sgd_solver.cpp:106] Iteration 114400, lr = 1e-17
I0429 21:56:43.649969   966 solver.cpp:228] Iteration 114500, loss = 0.19019
I0429 21:56:43.650126   966 solver.cpp:244]     Train net output #0: loss = 0.19019 (* 1 = 0.19019 loss)
I0429 21:56:43.650135   966 sgd_solver.cpp:106] Iteration 114500, lr = 1e-17
I0429 21:57:33.161170   966 solver.cpp:228] Iteration 114600, loss = 0.1483
I0429 21:57:33.161329   966 solver.cpp:244]     Train net output #0: loss = 0.1483 (* 1 = 0.1483 loss)
I0429 21:57:33.161336   966 sgd_solver.cpp:106] Iteration 114600, lr = 1e-17
I0429 21:58:24.631903   966 solver.cpp:228] Iteration 114700, loss = 0.100017
I0429 21:58:24.632061   966 solver.cpp:244]     Train net output #0: loss = 0.100017 (* 1 = 0.100017 loss)
I0429 21:58:24.632068   966 sgd_solver.cpp:106] Iteration 114700, lr = 1e-17
I0429 21:59:14.161284   966 solver.cpp:228] Iteration 114800, loss = 0.118037
I0429 21:59:14.161450   966 solver.cpp:244]     Train net output #0: loss = 0.118037 (* 1 = 0.118037 loss)
I0429 21:59:14.161458   966 sgd_solver.cpp:106] Iteration 114800, lr = 1e-17
I0429 22:00:03.682603   966 solver.cpp:228] Iteration 114900, loss = 0.106724
I0429 22:00:03.682773   966 solver.cpp:244]     Train net output #0: loss = 0.106724 (* 1 = 0.106724 loss)
I0429 22:00:03.682781   966 sgd_solver.cpp:106] Iteration 114900, lr = 1e-17
I0429 22:00:54.835297   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_115000.caffemodel
I0429 22:01:18.837594   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_115000.solverstate
I0429 22:01:19.039598   966 solver.cpp:337] Iteration 115000, Testing net (#0)
I0429 22:01:19.039723   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 22:01:19.039731   966 net.cpp:693] Ignoring source layer visualize
I0429 22:01:19.039732   966 net.cpp:693] Ignoring source layer fake
I0429 22:06:11.493911   966 solver.cpp:404]     Test net output #0: loss = 0.182968 (* 1 = 0.182968 loss)
I0429 22:06:11.802752   966 solver.cpp:228] Iteration 115000, loss = 0.100038
I0429 22:06:11.802770   966 solver.cpp:244]     Train net output #0: loss = 0.100038 (* 1 = 0.100038 loss)
I0429 22:06:11.802793   966 sgd_solver.cpp:106] Iteration 115000, lr = 1e-17
I0429 22:07:01.327316   966 solver.cpp:228] Iteration 115100, loss = 0.0973626
I0429 22:07:01.327477   966 solver.cpp:244]     Train net output #0: loss = 0.0973626 (* 1 = 0.0973626 loss)
I0429 22:07:01.327484   966 sgd_solver.cpp:106] Iteration 115100, lr = 1e-17
I0429 22:07:50.840427   966 solver.cpp:228] Iteration 115200, loss = 0.140532
I0429 22:07:50.840576   966 solver.cpp:244]     Train net output #0: loss = 0.140532 (* 1 = 0.140532 loss)
I0429 22:07:50.840584   966 sgd_solver.cpp:106] Iteration 115200, lr = 1e-17
I0429 22:08:42.153123   966 solver.cpp:228] Iteration 115300, loss = 0.0929385
I0429 22:08:42.153273   966 solver.cpp:244]     Train net output #0: loss = 0.0929385 (* 1 = 0.0929385 loss)
I0429 22:08:42.153280   966 sgd_solver.cpp:106] Iteration 115300, lr = 1e-17
I0429 22:09:31.649628   966 solver.cpp:228] Iteration 115400, loss = 0.0961409
I0429 22:09:31.649782   966 solver.cpp:244]     Train net output #0: loss = 0.0961409 (* 1 = 0.0961409 loss)
I0429 22:09:31.649790   966 sgd_solver.cpp:106] Iteration 115400, lr = 1e-17
I0429 22:10:21.160866   966 solver.cpp:228] Iteration 115500, loss = 0.0670839
I0429 22:10:21.161025   966 solver.cpp:244]     Train net output #0: loss = 0.0670839 (* 1 = 0.0670839 loss)
I0429 22:10:21.161031   966 sgd_solver.cpp:106] Iteration 115500, lr = 1e-17
I0429 22:11:12.037057   966 solver.cpp:228] Iteration 115600, loss = 0.0970124
I0429 22:11:12.037214   966 solver.cpp:244]     Train net output #0: loss = 0.0970124 (* 1 = 0.0970124 loss)
I0429 22:11:12.037221   966 sgd_solver.cpp:106] Iteration 115600, lr = 1e-17
I0429 22:12:01.545812   966 solver.cpp:228] Iteration 115700, loss = 0.124526
I0429 22:12:01.545974   966 solver.cpp:244]     Train net output #0: loss = 0.124526 (* 1 = 0.124526 loss)
I0429 22:12:01.545980   966 sgd_solver.cpp:106] Iteration 115700, lr = 1e-17
I0429 22:12:51.081948   966 solver.cpp:228] Iteration 115800, loss = 0.134336
I0429 22:12:51.082144   966 solver.cpp:244]     Train net output #0: loss = 0.134336 (* 1 = 0.134336 loss)
I0429 22:12:51.082156   966 sgd_solver.cpp:106] Iteration 115800, lr = 1e-17
I0429 22:13:42.196730   966 solver.cpp:228] Iteration 115900, loss = 0.107082
I0429 22:13:42.196988   966 solver.cpp:244]     Train net output #0: loss = 0.107082 (* 1 = 0.107082 loss)
I0429 22:13:42.197005   966 sgd_solver.cpp:106] Iteration 115900, lr = 1e-17
I0429 22:14:31.416668   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_116000.caffemodel
I0429 22:14:46.312430   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_116000.solverstate
I0429 22:14:46.508122   966 solver.cpp:337] Iteration 116000, Testing net (#0)
I0429 22:14:46.508229   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 22:14:46.508235   966 net.cpp:693] Ignoring source layer visualize
I0429 22:14:46.508237   966 net.cpp:693] Ignoring source layer fake
I0429 22:19:39.136600   966 solver.cpp:404]     Test net output #0: loss = 0.178835 (* 1 = 0.178835 loss)
I0429 22:19:39.445222   966 solver.cpp:228] Iteration 116000, loss = 0.130611
I0429 22:19:39.445240   966 solver.cpp:244]     Train net output #0: loss = 0.130611 (* 1 = 0.130611 loss)
I0429 22:19:39.445261   966 sgd_solver.cpp:106] Iteration 116000, lr = 1e-17
I0429 22:20:28.994650   966 solver.cpp:228] Iteration 116100, loss = 0.131081
I0429 22:20:28.994833   966 solver.cpp:244]     Train net output #0: loss = 0.131081 (* 1 = 0.131081 loss)
I0429 22:20:28.994840   966 sgd_solver.cpp:106] Iteration 116100, lr = 1e-17
I0429 22:21:20.365557   966 solver.cpp:228] Iteration 116200, loss = 0.20358
I0429 22:21:20.365731   966 solver.cpp:244]     Train net output #0: loss = 0.20358 (* 1 = 0.20358 loss)
I0429 22:21:20.365737   966 sgd_solver.cpp:106] Iteration 116200, lr = 1e-17
I0429 22:22:09.905530   966 solver.cpp:228] Iteration 116300, loss = 0.127463
I0429 22:22:09.905679   966 solver.cpp:244]     Train net output #0: loss = 0.127463 (* 1 = 0.127463 loss)
I0429 22:22:09.905688   966 sgd_solver.cpp:106] Iteration 116300, lr = 1e-17
I0429 22:23:01.381110   966 solver.cpp:228] Iteration 116400, loss = 0.0698229
I0429 22:23:01.381258   966 solver.cpp:244]     Train net output #0: loss = 0.0698229 (* 1 = 0.0698229 loss)
I0429 22:23:01.381265   966 sgd_solver.cpp:106] Iteration 116400, lr = 1e-17
I0429 22:23:50.899760   966 solver.cpp:228] Iteration 116500, loss = 0.223322
I0429 22:23:50.899951   966 solver.cpp:244]     Train net output #0: loss = 0.223322 (* 1 = 0.223322 loss)
I0429 22:23:50.899960   966 sgd_solver.cpp:106] Iteration 116500, lr = 1e-17
I0429 22:24:40.434316   966 solver.cpp:228] Iteration 116600, loss = 0.153687
I0429 22:24:40.434481   966 solver.cpp:244]     Train net output #0: loss = 0.153687 (* 1 = 0.153687 loss)
I0429 22:24:40.434489   966 sgd_solver.cpp:106] Iteration 116600, lr = 1e-17
I0429 22:25:29.966783   966 solver.cpp:228] Iteration 116700, loss = 0.214723
I0429 22:25:29.967300   966 solver.cpp:244]     Train net output #0: loss = 0.214723 (* 1 = 0.214723 loss)
I0429 22:25:29.967308   966 sgd_solver.cpp:106] Iteration 116700, lr = 1e-17
I0429 22:26:21.982085   966 solver.cpp:228] Iteration 116800, loss = 0.0778576
I0429 22:26:21.982266   966 solver.cpp:244]     Train net output #0: loss = 0.0778576 (* 1 = 0.0778576 loss)
I0429 22:26:21.982273   966 sgd_solver.cpp:106] Iteration 116800, lr = 1e-17
I0429 22:27:11.529572   966 solver.cpp:228] Iteration 116900, loss = 0.188358
I0429 22:27:11.529748   966 solver.cpp:244]     Train net output #0: loss = 0.188358 (* 1 = 0.188358 loss)
I0429 22:27:11.529755   966 sgd_solver.cpp:106] Iteration 116900, lr = 1e-17
I0429 22:28:00.771152   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_117000.caffemodel
I0429 22:28:08.777060   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_117000.solverstate
I0429 22:28:08.992080   966 solver.cpp:337] Iteration 117000, Testing net (#0)
I0429 22:28:08.992211   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 22:28:08.992218   966 net.cpp:693] Ignoring source layer visualize
I0429 22:28:08.992221   966 net.cpp:693] Ignoring source layer fake
I0429 22:33:01.992286   966 solver.cpp:404]     Test net output #0: loss = 0.184777 (* 1 = 0.184777 loss)
I0429 22:33:02.303077   966 solver.cpp:228] Iteration 117000, loss = 0.173912
I0429 22:33:02.303095   966 solver.cpp:244]     Train net output #0: loss = 0.173912 (* 1 = 0.173912 loss)
I0429 22:33:02.303117   966 sgd_solver.cpp:106] Iteration 117000, lr = 1e-17
I0429 22:33:53.944272   966 solver.cpp:228] Iteration 117100, loss = 0.233976
I0429 22:33:53.944442   966 solver.cpp:244]     Train net output #0: loss = 0.233976 (* 1 = 0.233976 loss)
I0429 22:33:53.944450   966 sgd_solver.cpp:106] Iteration 117100, lr = 1e-17
I0429 22:34:43.463589   966 solver.cpp:228] Iteration 117200, loss = 0.148296
I0429 22:34:43.463750   966 solver.cpp:244]     Train net output #0: loss = 0.148296 (* 1 = 0.148296 loss)
I0429 22:34:43.463757   966 sgd_solver.cpp:106] Iteration 117200, lr = 1e-17
I0429 22:35:32.995885   966 solver.cpp:228] Iteration 117300, loss = 0.0832477
I0429 22:35:32.996078   966 solver.cpp:244]     Train net output #0: loss = 0.0832477 (* 1 = 0.0832477 loss)
I0429 22:35:32.996088   966 sgd_solver.cpp:106] Iteration 117300, lr = 1e-17
I0429 22:36:22.536725   966 solver.cpp:228] Iteration 117400, loss = 0.134397
I0429 22:36:22.536906   966 solver.cpp:244]     Train net output #0: loss = 0.134397 (* 1 = 0.134397 loss)
I0429 22:36:22.536913   966 sgd_solver.cpp:106] Iteration 117400, lr = 1e-17
I0429 22:37:14.083628   966 solver.cpp:228] Iteration 117500, loss = 0.108659
I0429 22:37:14.083796   966 solver.cpp:244]     Train net output #0: loss = 0.108659 (* 1 = 0.108659 loss)
I0429 22:37:14.083803   966 sgd_solver.cpp:106] Iteration 117500, lr = 1e-17
I0429 22:38:03.606797   966 solver.cpp:228] Iteration 117600, loss = 0.178533
I0429 22:38:03.606966   966 solver.cpp:244]     Train net output #0: loss = 0.178533 (* 1 = 0.178533 loss)
I0429 22:38:03.606976   966 sgd_solver.cpp:106] Iteration 117600, lr = 1e-17
I0429 22:38:53.140481   966 solver.cpp:228] Iteration 117700, loss = 0.148083
I0429 22:38:53.140651   966 solver.cpp:244]     Train net output #0: loss = 0.148083 (* 1 = 0.148083 loss)
I0429 22:38:53.140658   966 sgd_solver.cpp:106] Iteration 117700, lr = 1e-17
I0429 22:39:44.594736   966 solver.cpp:228] Iteration 117800, loss = 0.0973763
I0429 22:39:44.595329   966 solver.cpp:244]     Train net output #0: loss = 0.0973763 (* 1 = 0.0973763 loss)
I0429 22:39:44.595337   966 sgd_solver.cpp:106] Iteration 117800, lr = 1e-17
I0429 22:40:34.118747   966 solver.cpp:228] Iteration 117900, loss = 0.11388
I0429 22:40:34.118903   966 solver.cpp:244]     Train net output #0: loss = 0.11388 (* 1 = 0.11388 loss)
I0429 22:40:34.118909   966 sgd_solver.cpp:106] Iteration 117900, lr = 1e-17
I0429 22:41:23.328449   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_118000.caffemodel
I0429 22:41:46.090159   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_118000.solverstate
I0429 22:41:46.280264   966 solver.cpp:337] Iteration 118000, Testing net (#0)
I0429 22:41:46.280390   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 22:41:46.280396   966 net.cpp:693] Ignoring source layer visualize
I0429 22:41:46.280398   966 net.cpp:693] Ignoring source layer fake
I0429 22:46:38.287875   966 solver.cpp:404]     Test net output #0: loss = 0.191728 (* 1 = 0.191728 loss)
I0429 22:46:38.598022   966 solver.cpp:228] Iteration 118000, loss = 0.216927
I0429 22:46:38.598040   966 solver.cpp:244]     Train net output #0: loss = 0.216927 (* 1 = 0.216927 loss)
I0429 22:46:38.598062   966 sgd_solver.cpp:106] Iteration 118000, lr = 1e-17
I0429 22:47:30.274891   966 solver.cpp:228] Iteration 118100, loss = 0.118084
I0429 22:47:30.275046   966 solver.cpp:244]     Train net output #0: loss = 0.118084 (* 1 = 0.118084 loss)
I0429 22:47:30.275055   966 sgd_solver.cpp:106] Iteration 118100, lr = 1e-17
I0429 22:48:19.787539   966 solver.cpp:228] Iteration 118200, loss = 0.126392
I0429 22:48:19.787719   966 solver.cpp:244]     Train net output #0: loss = 0.126392 (* 1 = 0.126392 loss)
I0429 22:48:19.787726   966 sgd_solver.cpp:106] Iteration 118200, lr = 1e-17
I0429 22:49:09.303042   966 solver.cpp:228] Iteration 118300, loss = 0.100466
I0429 22:49:09.303194   966 solver.cpp:244]     Train net output #0: loss = 0.100466 (* 1 = 0.100466 loss)
I0429 22:49:09.303200   966 sgd_solver.cpp:106] Iteration 118300, lr = 1e-17
I0429 22:49:58.819814   966 solver.cpp:228] Iteration 118400, loss = 0.109142
I0429 22:49:58.819993   966 solver.cpp:244]     Train net output #0: loss = 0.109142 (* 1 = 0.109142 loss)
I0429 22:49:58.820001   966 sgd_solver.cpp:106] Iteration 118400, lr = 1e-17
I0429 22:50:50.054550   966 solver.cpp:228] Iteration 118500, loss = 0.113978
I0429 22:50:50.054709   966 solver.cpp:244]     Train net output #0: loss = 0.113978 (* 1 = 0.113978 loss)
I0429 22:50:50.054718   966 sgd_solver.cpp:106] Iteration 118500, lr = 1e-17
I0429 22:51:39.573283   966 solver.cpp:228] Iteration 118600, loss = 0.166776
I0429 22:51:39.573462   966 solver.cpp:244]     Train net output #0: loss = 0.166776 (* 1 = 0.166776 loss)
I0429 22:51:39.573472   966 sgd_solver.cpp:106] Iteration 118600, lr = 1e-17
I0429 22:52:29.097275   966 solver.cpp:228] Iteration 118700, loss = 0.114335
I0429 22:52:29.097461   966 solver.cpp:244]     Train net output #0: loss = 0.114335 (* 1 = 0.114335 loss)
I0429 22:52:29.097468   966 sgd_solver.cpp:106] Iteration 118700, lr = 1e-17
I0429 22:53:20.182170   966 solver.cpp:228] Iteration 118800, loss = 0.133671
I0429 22:53:20.182334   966 solver.cpp:244]     Train net output #0: loss = 0.133671 (* 1 = 0.133671 loss)
I0429 22:53:20.182341   966 sgd_solver.cpp:106] Iteration 118800, lr = 1e-17
I0429 22:54:09.740195   966 solver.cpp:228] Iteration 118900, loss = 0.144212
I0429 22:54:09.740414   966 solver.cpp:244]     Train net output #0: loss = 0.144212 (* 1 = 0.144212 loss)
I0429 22:54:09.740424   966 sgd_solver.cpp:106] Iteration 118900, lr = 1e-17
I0429 22:55:00.538995   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_119000.caffemodel
I0429 22:55:04.223515   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_119000.solverstate
I0429 22:55:04.419407   966 solver.cpp:337] Iteration 119000, Testing net (#0)
I0429 22:55:04.419531   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 22:55:04.419536   966 net.cpp:693] Ignoring source layer visualize
I0429 22:55:04.419554   966 net.cpp:693] Ignoring source layer fake
I0429 22:59:56.870102   966 solver.cpp:404]     Test net output #0: loss = 0.183164 (* 1 = 0.183164 loss)
I0429 22:59:57.179750   966 solver.cpp:228] Iteration 119000, loss = 0.0986803
I0429 22:59:57.179772   966 solver.cpp:244]     Train net output #0: loss = 0.0986803 (* 1 = 0.0986803 loss)
I0429 22:59:57.179795   966 sgd_solver.cpp:106] Iteration 119000, lr = 1e-17
I0429 23:00:46.719135   966 solver.cpp:228] Iteration 119100, loss = 0.157744
I0429 23:00:46.719310   966 solver.cpp:244]     Train net output #0: loss = 0.157744 (* 1 = 0.157744 loss)
I0429 23:00:46.719318   966 sgd_solver.cpp:106] Iteration 119100, lr = 1e-17
I0429 23:01:36.250468   966 solver.cpp:228] Iteration 119200, loss = 0.228779
I0429 23:01:36.250625   966 solver.cpp:244]     Train net output #0: loss = 0.228779 (* 1 = 0.228779 loss)
I0429 23:01:36.250633   966 sgd_solver.cpp:106] Iteration 119200, lr = 1e-17
I0429 23:02:27.391166   966 solver.cpp:228] Iteration 119300, loss = 0.160421
I0429 23:02:27.391332   966 solver.cpp:244]     Train net output #0: loss = 0.160421 (* 1 = 0.160421 loss)
I0429 23:02:27.391342   966 sgd_solver.cpp:106] Iteration 119300, lr = 1e-17
I0429 23:03:16.921519   966 solver.cpp:228] Iteration 119400, loss = 0.0561853
I0429 23:03:16.921689   966 solver.cpp:244]     Train net output #0: loss = 0.0561853 (* 1 = 0.0561853 loss)
I0429 23:03:16.921696   966 sgd_solver.cpp:106] Iteration 119400, lr = 1e-17
I0429 23:04:06.444464   966 solver.cpp:228] Iteration 119500, loss = 0.126314
I0429 23:04:06.444612   966 solver.cpp:244]     Train net output #0: loss = 0.126314 (* 1 = 0.126314 loss)
I0429 23:04:06.444618   966 sgd_solver.cpp:106] Iteration 119500, lr = 1e-17
I0429 23:04:57.795786   966 solver.cpp:228] Iteration 119600, loss = 0.339365
I0429 23:04:57.795955   966 solver.cpp:244]     Train net output #0: loss = 0.339365 (* 1 = 0.339365 loss)
I0429 23:04:57.795964   966 sgd_solver.cpp:106] Iteration 119600, lr = 1e-17
I0429 23:05:47.329990   966 solver.cpp:228] Iteration 119700, loss = 0.14474
I0429 23:05:47.330174   966 solver.cpp:244]     Train net output #0: loss = 0.14474 (* 1 = 0.14474 loss)
I0429 23:05:47.330183   966 sgd_solver.cpp:106] Iteration 119700, lr = 1e-17
I0429 23:06:36.855744   966 solver.cpp:228] Iteration 119800, loss = 0.17661
I0429 23:06:36.855906   966 solver.cpp:244]     Train net output #0: loss = 0.17661 (* 1 = 0.17661 loss)
I0429 23:06:36.855912   966 sgd_solver.cpp:106] Iteration 119800, lr = 1e-17
I0429 23:07:28.330088   966 solver.cpp:228] Iteration 119900, loss = 0.166118
I0429 23:07:28.330294   966 solver.cpp:244]     Train net output #0: loss = 0.166118 (* 1 = 0.166118 loss)
I0429 23:07:28.330303   966 sgd_solver.cpp:106] Iteration 119900, lr = 1e-17
I0429 23:08:17.553632   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_120000.caffemodel
I0429 23:08:28.788100   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_120000.solverstate
I0429 23:08:28.980928   966 solver.cpp:337] Iteration 120000, Testing net (#0)
I0429 23:08:28.981053   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 23:08:28.981058   966 net.cpp:693] Ignoring source layer visualize
I0429 23:08:28.981060   966 net.cpp:693] Ignoring source layer fake
I0429 23:13:22.570957   966 solver.cpp:404]     Test net output #0: loss = 0.182656 (* 1 = 0.182656 loss)
I0429 23:13:22.882019   966 solver.cpp:228] Iteration 120000, loss = 0.170712
I0429 23:13:22.882046   966 solver.cpp:244]     Train net output #0: loss = 0.170712 (* 1 = 0.170712 loss)
I0429 23:13:22.882053   966 sgd_solver.cpp:106] Iteration 120000, lr = 1e-18
I0429 23:14:12.395901   966 solver.cpp:228] Iteration 120100, loss = 0.0853326
I0429 23:14:12.396088   966 solver.cpp:244]     Train net output #0: loss = 0.0853326 (* 1 = 0.0853326 loss)
I0429 23:14:12.396095   966 sgd_solver.cpp:106] Iteration 120100, lr = 1e-18
I0429 23:15:03.980948   966 solver.cpp:228] Iteration 120200, loss = 0.115191
I0429 23:15:03.981115   966 solver.cpp:244]     Train net output #0: loss = 0.115191 (* 1 = 0.115191 loss)
I0429 23:15:03.981122   966 sgd_solver.cpp:106] Iteration 120200, lr = 1e-18
I0429 23:15:53.500924   966 solver.cpp:228] Iteration 120300, loss = 0.0909171
I0429 23:15:53.501080   966 solver.cpp:244]     Train net output #0: loss = 0.0909171 (* 1 = 0.0909171 loss)
I0429 23:15:53.501088   966 sgd_solver.cpp:106] Iteration 120300, lr = 1e-18
I0429 23:16:43.022204   966 solver.cpp:228] Iteration 120400, loss = 0.240431
I0429 23:16:43.023316   966 solver.cpp:244]     Train net output #0: loss = 0.240431 (* 1 = 0.240431 loss)
I0429 23:16:43.023325   966 sgd_solver.cpp:106] Iteration 120400, lr = 1e-18
I0429 23:17:32.542630   966 solver.cpp:228] Iteration 120500, loss = 0.139378
I0429 23:17:32.542785   966 solver.cpp:244]     Train net output #0: loss = 0.139378 (* 1 = 0.139378 loss)
I0429 23:17:32.542793   966 sgd_solver.cpp:106] Iteration 120500, lr = 1e-18
I0429 23:18:23.955229   966 solver.cpp:228] Iteration 120600, loss = 0.122373
I0429 23:18:23.955392   966 solver.cpp:244]     Train net output #0: loss = 0.122373 (* 1 = 0.122373 loss)
I0429 23:18:23.955399   966 sgd_solver.cpp:106] Iteration 120600, lr = 1e-18
I0429 23:19:13.458447   966 solver.cpp:228] Iteration 120700, loss = 0.264866
I0429 23:19:13.458600   966 solver.cpp:244]     Train net output #0: loss = 0.264866 (* 1 = 0.264866 loss)
I0429 23:19:13.458607   966 sgd_solver.cpp:106] Iteration 120700, lr = 1e-18
I0429 23:20:02.984123   966 solver.cpp:228] Iteration 120800, loss = 0.121828
I0429 23:20:02.984295   966 solver.cpp:244]     Train net output #0: loss = 0.121828 (* 1 = 0.121828 loss)
I0429 23:20:02.984304   966 sgd_solver.cpp:106] Iteration 120800, lr = 1e-18
I0429 23:20:54.609441   966 solver.cpp:228] Iteration 120900, loss = 0.13957
I0429 23:20:54.609625   966 solver.cpp:244]     Train net output #0: loss = 0.13957 (* 1 = 0.13957 loss)
I0429 23:20:54.609643   966 sgd_solver.cpp:106] Iteration 120900, lr = 1e-18
I0429 23:21:43.856401   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_121000.caffemodel
I0429 23:21:57.392627   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_121000.solverstate
I0429 23:21:57.582602   966 solver.cpp:337] Iteration 121000, Testing net (#0)
I0429 23:21:57.582727   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 23:21:57.582733   966 net.cpp:693] Ignoring source layer visualize
I0429 23:21:57.582736   966 net.cpp:693] Ignoring source layer fake
I0429 23:26:49.294863   966 solver.cpp:404]     Test net output #0: loss = 0.178669 (* 1 = 0.178669 loss)
I0429 23:26:49.603642   966 solver.cpp:228] Iteration 121000, loss = 0.148938
I0429 23:26:49.603665   966 solver.cpp:244]     Train net output #0: loss = 0.148938 (* 1 = 0.148938 loss)
I0429 23:26:49.603688   966 sgd_solver.cpp:106] Iteration 121000, lr = 1e-18
I0429 23:27:39.128482   966 solver.cpp:228] Iteration 121100, loss = 0.17744
I0429 23:27:39.128674   966 solver.cpp:244]     Train net output #0: loss = 0.17744 (* 1 = 0.17744 loss)
I0429 23:27:39.128682   966 sgd_solver.cpp:106] Iteration 121100, lr = 1e-18
I0429 23:28:28.646793   966 solver.cpp:228] Iteration 121200, loss = 0.0736629
I0429 23:28:28.646939   966 solver.cpp:244]     Train net output #0: loss = 0.0736629 (* 1 = 0.0736629 loss)
I0429 23:28:28.646945   966 sgd_solver.cpp:106] Iteration 121200, lr = 1e-18
I0429 23:29:20.032032   966 solver.cpp:228] Iteration 121300, loss = 0.0925046
I0429 23:29:20.032181   966 solver.cpp:244]     Train net output #0: loss = 0.0925046 (* 1 = 0.0925046 loss)
I0429 23:29:20.032188   966 sgd_solver.cpp:106] Iteration 121300, lr = 1e-18
I0429 23:30:09.548671   966 solver.cpp:228] Iteration 121400, loss = 0.0965291
I0429 23:30:09.548831   966 solver.cpp:244]     Train net output #0: loss = 0.0965291 (* 1 = 0.0965291 loss)
I0429 23:30:09.548840   966 sgd_solver.cpp:106] Iteration 121400, lr = 1e-18
I0429 23:30:59.064749   966 solver.cpp:228] Iteration 121500, loss = 0.100674
I0429 23:30:59.064916   966 solver.cpp:244]     Train net output #0: loss = 0.100674 (* 1 = 0.100674 loss)
I0429 23:30:59.064925   966 sgd_solver.cpp:106] Iteration 121500, lr = 1e-18
I0429 23:31:50.289119   966 solver.cpp:228] Iteration 121600, loss = 0.0923208
I0429 23:31:50.289289   966 solver.cpp:244]     Train net output #0: loss = 0.0923208 (* 1 = 0.0923208 loss)
I0429 23:31:50.289296   966 sgd_solver.cpp:106] Iteration 121600, lr = 1e-18
I0429 23:32:39.795351   966 solver.cpp:228] Iteration 121700, loss = 0.170392
I0429 23:32:39.795502   966 solver.cpp:244]     Train net output #0: loss = 0.170392 (* 1 = 0.170392 loss)
I0429 23:32:39.795509   966 sgd_solver.cpp:106] Iteration 121700, lr = 1e-18
I0429 23:33:29.320539   966 solver.cpp:228] Iteration 121800, loss = 0.0887067
I0429 23:33:29.320696   966 solver.cpp:244]     Train net output #0: loss = 0.0887067 (* 1 = 0.0887067 loss)
I0429 23:33:29.320703   966 sgd_solver.cpp:106] Iteration 121800, lr = 1e-18
I0429 23:34:20.254703   966 solver.cpp:228] Iteration 121900, loss = 0.145787
I0429 23:34:20.254874   966 solver.cpp:244]     Train net output #0: loss = 0.145787 (* 1 = 0.145787 loss)
I0429 23:34:20.254881   966 sgd_solver.cpp:106] Iteration 121900, lr = 1e-18
I0429 23:35:09.461864   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_122000.caffemodel
I0429 23:35:21.124613   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_122000.solverstate
I0429 23:35:21.311683   966 solver.cpp:337] Iteration 122000, Testing net (#0)
I0429 23:35:21.311805   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 23:35:21.311812   966 net.cpp:693] Ignoring source layer visualize
I0429 23:35:21.311815   966 net.cpp:693] Ignoring source layer fake
I0429 23:40:13.783025   966 solver.cpp:404]     Test net output #0: loss = 0.18676 (* 1 = 0.18676 loss)
I0429 23:40:14.091979   966 solver.cpp:228] Iteration 122000, loss = 0.126039
I0429 23:40:14.092012   966 solver.cpp:244]     Train net output #0: loss = 0.126039 (* 1 = 0.126039 loss)
I0429 23:40:14.092025   966 sgd_solver.cpp:106] Iteration 122000, lr = 1e-18
I0429 23:41:04.963744   966 solver.cpp:228] Iteration 122100, loss = 0.215406
I0429 23:41:04.963907   966 solver.cpp:244]     Train net output #0: loss = 0.215406 (* 1 = 0.215406 loss)
I0429 23:41:04.963914   966 sgd_solver.cpp:106] Iteration 122100, lr = 1e-18
I0429 23:41:54.496489   966 solver.cpp:228] Iteration 122200, loss = 0.0884512
I0429 23:41:54.496675   966 solver.cpp:244]     Train net output #0: loss = 0.0884512 (* 1 = 0.0884512 loss)
I0429 23:41:54.496682   966 sgd_solver.cpp:106] Iteration 122200, lr = 1e-18
I0429 23:42:44.026854   966 solver.cpp:228] Iteration 122300, loss = 0.189332
I0429 23:42:44.027022   966 solver.cpp:244]     Train net output #0: loss = 0.189332 (* 1 = 0.189332 loss)
I0429 23:42:44.027029   966 sgd_solver.cpp:106] Iteration 122300, lr = 1e-18
I0429 23:43:35.113104   966 solver.cpp:228] Iteration 122400, loss = 0.091793
I0429 23:43:35.113271   966 solver.cpp:244]     Train net output #0: loss = 0.091793 (* 1 = 0.091793 loss)
I0429 23:43:35.113278   966 sgd_solver.cpp:106] Iteration 122400, lr = 1e-18
I0429 23:44:24.635772   966 solver.cpp:228] Iteration 122500, loss = 0.0460059
I0429 23:44:24.635969   966 solver.cpp:244]     Train net output #0: loss = 0.0460059 (* 1 = 0.0460059 loss)
I0429 23:44:24.635977   966 sgd_solver.cpp:106] Iteration 122500, lr = 1e-18
I0429 23:45:14.165259   966 solver.cpp:228] Iteration 122600, loss = 0.107714
I0429 23:45:14.165452   966 solver.cpp:244]     Train net output #0: loss = 0.107714 (* 1 = 0.107714 loss)
I0429 23:45:14.165459   966 sgd_solver.cpp:106] Iteration 122600, lr = 1e-18
I0429 23:46:05.460451   966 solver.cpp:228] Iteration 122700, loss = 0.133576
I0429 23:46:05.460609   966 solver.cpp:244]     Train net output #0: loss = 0.133576 (* 1 = 0.133576 loss)
I0429 23:46:05.460615   966 sgd_solver.cpp:106] Iteration 122700, lr = 1e-18
I0429 23:46:54.993999   966 solver.cpp:228] Iteration 122800, loss = 0.205648
I0429 23:46:54.994168   966 solver.cpp:244]     Train net output #0: loss = 0.205648 (* 1 = 0.205648 loss)
I0429 23:46:54.994175   966 sgd_solver.cpp:106] Iteration 122800, lr = 1e-18
I0429 23:47:44.524811   966 solver.cpp:228] Iteration 122900, loss = 0.098337
I0429 23:47:44.525630   966 solver.cpp:244]     Train net output #0: loss = 0.098337 (* 1 = 0.098337 loss)
I0429 23:47:44.525638   966 sgd_solver.cpp:106] Iteration 122900, lr = 1e-18
I0429 23:48:35.672806   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_123000.caffemodel
I0429 23:48:46.256278   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_123000.solverstate
I0429 23:48:46.452185   966 solver.cpp:337] Iteration 123000, Testing net (#0)
I0429 23:48:46.452307   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0429 23:48:46.452313   966 net.cpp:693] Ignoring source layer visualize
I0429 23:48:46.452316   966 net.cpp:693] Ignoring source layer fake
I0429 23:53:38.335574   966 solver.cpp:404]     Test net output #0: loss = 0.190824 (* 1 = 0.190824 loss)
I0429 23:53:38.648319   966 solver.cpp:228] Iteration 123000, loss = 0.12489
I0429 23:53:38.648344   966 solver.cpp:244]     Train net output #0: loss = 0.12489 (* 1 = 0.12489 loss)
I0429 23:53:38.648366   966 sgd_solver.cpp:106] Iteration 123000, lr = 1e-18
I0429 23:54:28.193409   966 solver.cpp:228] Iteration 123100, loss = 0.198878
I0429 23:54:28.193588   966 solver.cpp:244]     Train net output #0: loss = 0.198878 (* 1 = 0.198878 loss)
I0429 23:54:28.193595   966 sgd_solver.cpp:106] Iteration 123100, lr = 1e-18
I0429 23:55:17.728163   966 solver.cpp:228] Iteration 123200, loss = 0.177406
I0429 23:55:17.729642   966 solver.cpp:244]     Train net output #0: loss = 0.177406 (* 1 = 0.177406 loss)
I0429 23:55:17.729650   966 sgd_solver.cpp:106] Iteration 123200, lr = 1e-18
I0429 23:56:07.250005   966 solver.cpp:228] Iteration 123300, loss = 0.146334
I0429 23:56:07.250154   966 solver.cpp:244]     Train net output #0: loss = 0.146334 (* 1 = 0.146334 loss)
I0429 23:56:07.250161   966 sgd_solver.cpp:106] Iteration 123300, lr = 1e-18
I0429 23:56:58.641201   966 solver.cpp:228] Iteration 123400, loss = 0.0938049
I0429 23:56:58.641357   966 solver.cpp:244]     Train net output #0: loss = 0.0938049 (* 1 = 0.0938049 loss)
I0429 23:56:58.641365   966 sgd_solver.cpp:106] Iteration 123400, lr = 1e-18
I0429 23:57:48.167237   966 solver.cpp:228] Iteration 123500, loss = 0.0745786
I0429 23:57:48.168340   966 solver.cpp:244]     Train net output #0: loss = 0.0745786 (* 1 = 0.0745786 loss)
I0429 23:57:48.168364   966 sgd_solver.cpp:106] Iteration 123500, lr = 1e-18
I0429 23:58:37.699841   966 solver.cpp:228] Iteration 123600, loss = 0.161962
I0429 23:58:37.700000   966 solver.cpp:244]     Train net output #0: loss = 0.161962 (* 1 = 0.161962 loss)
I0429 23:58:37.700006   966 sgd_solver.cpp:106] Iteration 123600, lr = 1e-18
I0429 23:59:29.064661   966 solver.cpp:228] Iteration 123700, loss = 0.16661
I0429 23:59:29.064824   966 solver.cpp:244]     Train net output #0: loss = 0.16661 (* 1 = 0.16661 loss)
I0429 23:59:29.064832   966 sgd_solver.cpp:106] Iteration 123700, lr = 1e-18
I0430 00:00:18.582859   966 solver.cpp:228] Iteration 123800, loss = 0.0843231
I0430 00:00:18.583005   966 solver.cpp:244]     Train net output #0: loss = 0.0843231 (* 1 = 0.0843231 loss)
I0430 00:00:18.583012   966 sgd_solver.cpp:106] Iteration 123800, lr = 1e-18
I0430 00:01:08.119138   966 solver.cpp:228] Iteration 123900, loss = 0.109459
I0430 00:01:08.119295   966 solver.cpp:244]     Train net output #0: loss = 0.109459 (* 1 = 0.109459 loss)
I0430 00:01:08.119302   966 sgd_solver.cpp:106] Iteration 123900, lr = 1e-18
I0430 00:01:57.332015   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_124000.caffemodel
I0430 00:02:30.000547   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_124000.solverstate
I0430 00:02:30.282035   966 solver.cpp:337] Iteration 124000, Testing net (#0)
I0430 00:02:30.282219   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 00:02:30.282230   966 net.cpp:693] Ignoring source layer visualize
I0430 00:02:30.282234   966 net.cpp:693] Ignoring source layer fake
I0430 00:07:22.938832   966 solver.cpp:404]     Test net output #0: loss = 0.182456 (* 1 = 0.182456 loss)
I0430 00:07:23.249581   966 solver.cpp:228] Iteration 124000, loss = 0.103215
I0430 00:07:23.249627   966 solver.cpp:244]     Train net output #0: loss = 0.103215 (* 1 = 0.103215 loss)
I0430 00:07:23.249634   966 sgd_solver.cpp:106] Iteration 124000, lr = 1e-18
I0430 00:08:14.571745   966 solver.cpp:228] Iteration 124100, loss = 0.186653
I0430 00:08:14.571916   966 solver.cpp:244]     Train net output #0: loss = 0.186653 (* 1 = 0.186653 loss)
I0430 00:08:14.571923   966 sgd_solver.cpp:106] Iteration 124100, lr = 1e-18
I0430 00:09:04.098901   966 solver.cpp:228] Iteration 124200, loss = 0.0862353
I0430 00:09:04.099053   966 solver.cpp:244]     Train net output #0: loss = 0.0862353 (* 1 = 0.0862353 loss)
I0430 00:09:04.099059   966 sgd_solver.cpp:106] Iteration 124200, lr = 1e-18
I0430 00:09:53.618728   966 solver.cpp:228] Iteration 124300, loss = 0.129756
I0430 00:09:53.618885   966 solver.cpp:244]     Train net output #0: loss = 0.129756 (* 1 = 0.129756 loss)
I0430 00:09:53.618893   966 sgd_solver.cpp:106] Iteration 124300, lr = 1e-18
I0430 00:10:44.968111   966 solver.cpp:228] Iteration 124400, loss = 0.186126
I0430 00:10:44.968272   966 solver.cpp:244]     Train net output #0: loss = 0.186126 (* 1 = 0.186126 loss)
I0430 00:10:44.968281   966 sgd_solver.cpp:106] Iteration 124400, lr = 1e-18
I0430 00:11:34.481578   966 solver.cpp:228] Iteration 124500, loss = 0.10705
I0430 00:11:34.481719   966 solver.cpp:244]     Train net output #0: loss = 0.10705 (* 1 = 0.10705 loss)
I0430 00:11:34.481726   966 sgd_solver.cpp:106] Iteration 124500, lr = 1e-18
I0430 00:12:23.999207   966 solver.cpp:228] Iteration 124600, loss = 0.109054
I0430 00:12:23.999379   966 solver.cpp:244]     Train net output #0: loss = 0.109054 (* 1 = 0.109054 loss)
I0430 00:12:23.999385   966 sgd_solver.cpp:106] Iteration 124600, lr = 1e-18
I0430 00:13:15.205726   966 solver.cpp:228] Iteration 124700, loss = 0.148801
I0430 00:13:15.205904   966 solver.cpp:244]     Train net output #0: loss = 0.148801 (* 1 = 0.148801 loss)
I0430 00:13:15.205911   966 sgd_solver.cpp:106] Iteration 124700, lr = 1e-18
I0430 00:14:04.719413   966 solver.cpp:228] Iteration 124800, loss = 0.106811
I0430 00:14:04.719563   966 solver.cpp:244]     Train net output #0: loss = 0.106811 (* 1 = 0.106811 loss)
I0430 00:14:04.719568   966 sgd_solver.cpp:106] Iteration 124800, lr = 1e-18
I0430 00:14:54.245517   966 solver.cpp:228] Iteration 124900, loss = 0.134887
I0430 00:14:54.245653   966 solver.cpp:244]     Train net output #0: loss = 0.134887 (* 1 = 0.134887 loss)
I0430 00:14:54.245659   966 sgd_solver.cpp:106] Iteration 124900, lr = 1e-18
I0430 00:15:44.723423   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_125000.caffemodel
I0430 00:15:56.008383   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_125000.solverstate
I0430 00:15:56.198786   966 solver.cpp:337] Iteration 125000, Testing net (#0)
I0430 00:15:56.198909   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 00:15:56.198915   966 net.cpp:693] Ignoring source layer visualize
I0430 00:15:56.198916   966 net.cpp:693] Ignoring source layer fake
I0430 00:20:48.265645   966 solver.cpp:404]     Test net output #0: loss = 0.182189 (* 1 = 0.182189 loss)
I0430 00:20:48.575686   966 solver.cpp:228] Iteration 125000, loss = 0.1692
I0430 00:20:48.575706   966 solver.cpp:244]     Train net output #0: loss = 0.1692 (* 1 = 0.1692 loss)
I0430 00:20:48.575729   966 sgd_solver.cpp:106] Iteration 125000, lr = 1e-18
I0430 00:21:38.113746   966 solver.cpp:228] Iteration 125100, loss = 0.0961181
I0430 00:21:38.113898   966 solver.cpp:244]     Train net output #0: loss = 0.0961181 (* 1 = 0.0961181 loss)
I0430 00:21:38.113904   966 sgd_solver.cpp:106] Iteration 125100, lr = 1e-18
I0430 00:22:27.645920   966 solver.cpp:228] Iteration 125200, loss = 0.186825
I0430 00:22:27.646081   966 solver.cpp:244]     Train net output #0: loss = 0.186825 (* 1 = 0.186825 loss)
I0430 00:22:27.646088   966 sgd_solver.cpp:106] Iteration 125200, lr = 1e-18
I0430 00:23:18.484119   966 solver.cpp:228] Iteration 125300, loss = 0.122786
I0430 00:23:18.484283   966 solver.cpp:244]     Train net output #0: loss = 0.122786 (* 1 = 0.122786 loss)
I0430 00:23:18.484290   966 sgd_solver.cpp:106] Iteration 125300, lr = 1e-18
I0430 00:24:08.013423   966 solver.cpp:228] Iteration 125400, loss = 0.121428
I0430 00:24:08.013573   966 solver.cpp:244]     Train net output #0: loss = 0.121428 (* 1 = 0.121428 loss)
I0430 00:24:08.013581   966 sgd_solver.cpp:106] Iteration 125400, lr = 1e-18
I0430 00:24:59.180230   966 solver.cpp:228] Iteration 125500, loss = 0.0662096
I0430 00:24:59.180397   966 solver.cpp:244]     Train net output #0: loss = 0.0662096 (* 1 = 0.0662096 loss)
I0430 00:24:59.180404   966 sgd_solver.cpp:106] Iteration 125500, lr = 1e-18
I0430 00:25:48.702373   966 solver.cpp:228] Iteration 125600, loss = 0.0904803
I0430 00:25:48.702539   966 solver.cpp:244]     Train net output #0: loss = 0.0904803 (* 1 = 0.0904803 loss)
I0430 00:25:48.702546   966 sgd_solver.cpp:106] Iteration 125600, lr = 1e-18
I0430 00:26:38.252254   966 solver.cpp:228] Iteration 125700, loss = 0.0803197
I0430 00:26:38.252436   966 solver.cpp:244]     Train net output #0: loss = 0.0803197 (* 1 = 0.0803197 loss)
I0430 00:26:38.252444   966 sgd_solver.cpp:106] Iteration 125700, lr = 1e-18
I0430 00:27:29.532805   966 solver.cpp:228] Iteration 125800, loss = 0.155531
I0430 00:27:29.532961   966 solver.cpp:244]     Train net output #0: loss = 0.155531 (* 1 = 0.155531 loss)
I0430 00:27:29.532968   966 sgd_solver.cpp:106] Iteration 125800, lr = 1e-18
I0430 00:28:19.050777   966 solver.cpp:228] Iteration 125900, loss = 0.092681
I0430 00:28:19.050935   966 solver.cpp:244]     Train net output #0: loss = 0.092681 (* 1 = 0.092681 loss)
I0430 00:28:19.050941   966 sgd_solver.cpp:106] Iteration 125900, lr = 1e-18
I0430 00:29:08.261060   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_126000.caffemodel
I0430 00:29:33.569789   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_126000.solverstate
I0430 00:29:33.758523   966 solver.cpp:337] Iteration 126000, Testing net (#0)
I0430 00:29:33.758644   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 00:29:33.758651   966 net.cpp:693] Ignoring source layer visualize
I0430 00:29:33.758661   966 net.cpp:693] Ignoring source layer fake
I0430 00:34:25.502028   966 solver.cpp:404]     Test net output #0: loss = 0.179068 (* 1 = 0.179068 loss)
I0430 00:34:25.811003   966 solver.cpp:228] Iteration 126000, loss = 0.090608
I0430 00:34:25.811022   966 solver.cpp:244]     Train net output #0: loss = 0.090608 (* 1 = 0.090608 loss)
I0430 00:34:25.811043   966 sgd_solver.cpp:106] Iteration 126000, lr = 1e-18
I0430 00:35:17.216115   966 solver.cpp:228] Iteration 126100, loss = 0.176076
I0430 00:35:17.216272   966 solver.cpp:244]     Train net output #0: loss = 0.176076 (* 1 = 0.176076 loss)
I0430 00:35:17.216280   966 sgd_solver.cpp:106] Iteration 126100, lr = 1e-18
I0430 00:36:06.737213   966 solver.cpp:228] Iteration 126200, loss = 0.112851
I0430 00:36:06.737373   966 solver.cpp:244]     Train net output #0: loss = 0.112851 (* 1 = 0.112851 loss)
I0430 00:36:06.737380   966 sgd_solver.cpp:106] Iteration 126200, lr = 1e-18
I0430 00:36:56.269417   966 solver.cpp:228] Iteration 126300, loss = 0.0730224
I0430 00:36:56.269615   966 solver.cpp:244]     Train net output #0: loss = 0.0730224 (* 1 = 0.0730224 loss)
I0430 00:36:56.269621   966 sgd_solver.cpp:106] Iteration 126300, lr = 1e-18
I0430 00:37:45.809885   966 solver.cpp:228] Iteration 126400, loss = 0.0895703
I0430 00:37:45.810073   966 solver.cpp:244]     Train net output #0: loss = 0.0895703 (* 1 = 0.0895703 loss)
I0430 00:37:45.810080   966 sgd_solver.cpp:106] Iteration 126400, lr = 1e-18
I0430 00:38:37.207734   966 solver.cpp:228] Iteration 126500, loss = 0.158763
I0430 00:38:37.209404   966 solver.cpp:244]     Train net output #0: loss = 0.158763 (* 1 = 0.158763 loss)
I0430 00:38:37.209411   966 sgd_solver.cpp:106] Iteration 126500, lr = 1e-18
I0430 00:39:26.742347   966 solver.cpp:228] Iteration 126600, loss = 0.196559
I0430 00:39:26.742501   966 solver.cpp:244]     Train net output #0: loss = 0.196559 (* 1 = 0.196559 loss)
I0430 00:39:26.742507   966 sgd_solver.cpp:106] Iteration 126600, lr = 1e-18
I0430 00:40:16.277487   966 solver.cpp:228] Iteration 126700, loss = 0.117912
I0430 00:40:16.277631   966 solver.cpp:244]     Train net output #0: loss = 0.117912 (* 1 = 0.117912 loss)
I0430 00:40:16.277638   966 sgd_solver.cpp:106] Iteration 126700, lr = 1e-18
I0430 00:41:07.797857   966 solver.cpp:228] Iteration 126800, loss = 0.202036
I0430 00:41:07.798014   966 solver.cpp:244]     Train net output #0: loss = 0.202036 (* 1 = 0.202036 loss)
I0430 00:41:07.798022   966 sgd_solver.cpp:106] Iteration 126800, lr = 1e-18
I0430 00:41:57.312885   966 solver.cpp:228] Iteration 126900, loss = 0.147124
I0430 00:41:57.313042   966 solver.cpp:244]     Train net output #0: loss = 0.147124 (* 1 = 0.147124 loss)
I0430 00:41:57.313050   966 sgd_solver.cpp:106] Iteration 126900, lr = 1e-18
I0430 00:42:46.526404   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_127000.caffemodel
I0430 00:43:25.002504   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_127000.solverstate
I0430 00:43:25.334390   966 solver.cpp:337] Iteration 127000, Testing net (#0)
I0430 00:43:25.334532   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 00:43:25.334537   966 net.cpp:693] Ignoring source layer visualize
I0430 00:43:25.334539   966 net.cpp:693] Ignoring source layer fake
I0430 00:48:18.298532   966 solver.cpp:404]     Test net output #0: loss = 0.187928 (* 1 = 0.187928 loss)
I0430 00:48:18.608932   966 solver.cpp:228] Iteration 127000, loss = 0.095911
I0430 00:48:18.608952   966 solver.cpp:244]     Train net output #0: loss = 0.095911 (* 1 = 0.095911 loss)
I0430 00:48:18.608974   966 sgd_solver.cpp:106] Iteration 127000, lr = 1e-18
I0430 00:49:08.143240   966 solver.cpp:228] Iteration 127100, loss = 0.0815617
I0430 00:49:08.143405   966 solver.cpp:244]     Train net output #0: loss = 0.0815617 (* 1 = 0.0815617 loss)
I0430 00:49:08.143412   966 sgd_solver.cpp:106] Iteration 127100, lr = 1e-18
I0430 00:49:59.536104   966 solver.cpp:228] Iteration 127200, loss = 0.147482
I0430 00:49:59.536265   966 solver.cpp:244]     Train net output #0: loss = 0.147482 (* 1 = 0.147482 loss)
I0430 00:49:59.536273   966 sgd_solver.cpp:106] Iteration 127200, lr = 1e-18
I0430 00:50:49.049521   966 solver.cpp:228] Iteration 127300, loss = 0.164588
I0430 00:50:49.049664   966 solver.cpp:244]     Train net output #0: loss = 0.164588 (* 1 = 0.164588 loss)
I0430 00:50:49.049671   966 sgd_solver.cpp:106] Iteration 127300, lr = 1e-18
I0430 00:51:38.569852   966 solver.cpp:228] Iteration 127400, loss = 0.124321
I0430 00:51:38.570881   966 solver.cpp:244]     Train net output #0: loss = 0.124321 (* 1 = 0.124321 loss)
I0430 00:51:38.570888   966 sgd_solver.cpp:106] Iteration 127400, lr = 1e-18
I0430 00:52:29.942798   966 solver.cpp:228] Iteration 127500, loss = 0.140276
I0430 00:52:29.942955   966 solver.cpp:244]     Train net output #0: loss = 0.140276 (* 1 = 0.140276 loss)
I0430 00:52:29.942963   966 sgd_solver.cpp:106] Iteration 127500, lr = 1e-19
I0430 00:53:19.445870   966 solver.cpp:228] Iteration 127600, loss = 0.141182
I0430 00:53:19.446044   966 solver.cpp:244]     Train net output #0: loss = 0.141182 (* 1 = 0.141182 loss)
I0430 00:53:19.446053   966 sgd_solver.cpp:106] Iteration 127600, lr = 1e-19
I0430 00:54:08.952621   966 solver.cpp:228] Iteration 127700, loss = 0.205524
I0430 00:54:08.953500   966 solver.cpp:244]     Train net output #0: loss = 0.205524 (* 1 = 0.205524 loss)
I0430 00:54:08.953507   966 sgd_solver.cpp:106] Iteration 127700, lr = 1e-19
I0430 00:55:00.189810   966 solver.cpp:228] Iteration 127800, loss = 0.27493
I0430 00:55:00.189962   966 solver.cpp:244]     Train net output #0: loss = 0.27493 (* 1 = 0.27493 loss)
I0430 00:55:00.189970   966 sgd_solver.cpp:106] Iteration 127800, lr = 1e-19
I0430 00:55:49.700448   966 solver.cpp:228] Iteration 127900, loss = 0.156472
I0430 00:55:49.700604   966 solver.cpp:244]     Train net output #0: loss = 0.156472 (* 1 = 0.156472 loss)
I0430 00:55:49.700610   966 sgd_solver.cpp:106] Iteration 127900, lr = 1e-19
I0430 00:56:38.898748   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_128000.caffemodel
I0430 00:57:03.061890   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_128000.solverstate
I0430 00:57:03.253849   966 solver.cpp:337] Iteration 128000, Testing net (#0)
I0430 00:57:03.253988   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 00:57:03.253993   966 net.cpp:693] Ignoring source layer visualize
I0430 00:57:03.253995   966 net.cpp:693] Ignoring source layer fake
I0430 01:01:55.164575   966 solver.cpp:404]     Test net output #0: loss = 0.190137 (* 1 = 0.190137 loss)
I0430 01:01:55.474042   966 solver.cpp:228] Iteration 128000, loss = 0.108847
I0430 01:01:55.474064   966 solver.cpp:244]     Train net output #0: loss = 0.108847 (* 1 = 0.108847 loss)
I0430 01:01:55.474071   966 sgd_solver.cpp:106] Iteration 128000, lr = 1e-19
I0430 01:02:46.305894   966 solver.cpp:228] Iteration 128100, loss = 0.0919884
I0430 01:02:46.306071   966 solver.cpp:244]     Train net output #0: loss = 0.0919884 (* 1 = 0.0919884 loss)
I0430 01:02:46.306077   966 sgd_solver.cpp:106] Iteration 128100, lr = 1e-19
I0430 01:03:35.828984   966 solver.cpp:228] Iteration 128200, loss = 0.153127
I0430 01:03:35.829172   966 solver.cpp:244]     Train net output #0: loss = 0.153127 (* 1 = 0.153127 loss)
I0430 01:03:35.829180   966 sgd_solver.cpp:106] Iteration 128200, lr = 1e-19
I0430 01:04:25.355185   966 solver.cpp:228] Iteration 128300, loss = 0.141037
I0430 01:04:25.355346   966 solver.cpp:244]     Train net output #0: loss = 0.141037 (* 1 = 0.141037 loss)
I0430 01:04:25.355355   966 sgd_solver.cpp:106] Iteration 128300, lr = 1e-19
I0430 01:05:16.251600   966 solver.cpp:228] Iteration 128400, loss = 0.14029
I0430 01:05:16.251760   966 solver.cpp:244]     Train net output #0: loss = 0.14029 (* 1 = 0.14029 loss)
I0430 01:05:16.251766   966 sgd_solver.cpp:106] Iteration 128400, lr = 1e-19
I0430 01:06:05.772122   966 solver.cpp:228] Iteration 128500, loss = 0.0508481
I0430 01:06:05.772294   966 solver.cpp:244]     Train net output #0: loss = 0.0508481 (* 1 = 0.0508481 loss)
I0430 01:06:05.772301   966 sgd_solver.cpp:106] Iteration 128500, lr = 1e-19
I0430 01:06:55.306877   966 solver.cpp:228] Iteration 128600, loss = 0.150533
I0430 01:06:55.307025   966 solver.cpp:244]     Train net output #0: loss = 0.150533 (* 1 = 0.150533 loss)
I0430 01:06:55.307032   966 sgd_solver.cpp:106] Iteration 128600, lr = 1e-19
I0430 01:07:46.424609   966 solver.cpp:228] Iteration 128700, loss = 0.270875
I0430 01:07:46.424778   966 solver.cpp:244]     Train net output #0: loss = 0.270875 (* 1 = 0.270875 loss)
I0430 01:07:46.424787   966 sgd_solver.cpp:106] Iteration 128700, lr = 1e-19
I0430 01:08:35.961983   966 solver.cpp:228] Iteration 128800, loss = 0.101876
I0430 01:08:35.962146   966 solver.cpp:244]     Train net output #0: loss = 0.101876 (* 1 = 0.101876 loss)
I0430 01:08:35.962152   966 sgd_solver.cpp:106] Iteration 128800, lr = 1e-19
I0430 01:09:27.289492   966 solver.cpp:228] Iteration 128900, loss = 0.117108
I0430 01:09:27.289636   966 solver.cpp:244]     Train net output #0: loss = 0.117108 (* 1 = 0.117108 loss)
I0430 01:09:27.289644   966 sgd_solver.cpp:106] Iteration 128900, lr = 1e-19
I0430 01:10:16.498688   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_129000.caffemodel
I0430 01:10:28.260687   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_129000.solverstate
I0430 01:10:28.454520   966 solver.cpp:337] Iteration 129000, Testing net (#0)
I0430 01:10:28.454644   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 01:10:28.454650   966 net.cpp:693] Ignoring source layer visualize
I0430 01:10:28.454669   966 net.cpp:693] Ignoring source layer fake
I0430 01:15:20.909041   966 solver.cpp:404]     Test net output #0: loss = 0.182005 (* 1 = 0.182005 loss)
I0430 01:15:21.219672   966 solver.cpp:228] Iteration 129000, loss = 0.193533
I0430 01:15:21.219707   966 solver.cpp:244]     Train net output #0: loss = 0.193533 (* 1 = 0.193533 loss)
I0430 01:15:21.219713   966 sgd_solver.cpp:106] Iteration 129000, lr = 1e-19
I0430 01:16:10.752274   966 solver.cpp:228] Iteration 129100, loss = 0.181425
I0430 01:16:10.752424   966 solver.cpp:244]     Train net output #0: loss = 0.181425 (* 1 = 0.181425 loss)
I0430 01:16:10.752431   966 sgd_solver.cpp:106] Iteration 129100, lr = 1e-19
I0430 01:17:00.276871   966 solver.cpp:228] Iteration 129200, loss = 0.170584
I0430 01:17:00.277026   966 solver.cpp:244]     Train net output #0: loss = 0.170584 (* 1 = 0.170584 loss)
I0430 01:17:00.277034   966 sgd_solver.cpp:106] Iteration 129200, lr = 1e-19
I0430 01:17:51.721427   966 solver.cpp:228] Iteration 129300, loss = 0.141171
I0430 01:17:51.721590   966 solver.cpp:244]     Train net output #0: loss = 0.141171 (* 1 = 0.141171 loss)
I0430 01:17:51.721598   966 sgd_solver.cpp:106] Iteration 129300, lr = 1e-19
I0430 01:18:41.247856   966 solver.cpp:228] Iteration 129400, loss = 0.240549
I0430 01:18:41.248013   966 solver.cpp:244]     Train net output #0: loss = 0.240549 (* 1 = 0.240549 loss)
I0430 01:18:41.248020   966 sgd_solver.cpp:106] Iteration 129400, lr = 1e-19
I0430 01:19:30.784786   966 solver.cpp:228] Iteration 129500, loss = 0.147837
I0430 01:19:30.784956   966 solver.cpp:244]     Train net output #0: loss = 0.147837 (* 1 = 0.147837 loss)
I0430 01:19:30.784965   966 sgd_solver.cpp:106] Iteration 129500, lr = 1e-19
I0430 01:20:22.267453   966 solver.cpp:228] Iteration 129600, loss = 0.136034
I0430 01:20:22.267617   966 solver.cpp:244]     Train net output #0: loss = 0.136034 (* 1 = 0.136034 loss)
I0430 01:20:22.267624   966 sgd_solver.cpp:106] Iteration 129600, lr = 1e-19
I0430 01:21:11.774096   966 solver.cpp:228] Iteration 129700, loss = 0.0812235
I0430 01:21:11.774260   966 solver.cpp:244]     Train net output #0: loss = 0.0812235 (* 1 = 0.0812235 loss)
I0430 01:21:11.774267   966 sgd_solver.cpp:106] Iteration 129700, lr = 1e-19
I0430 01:22:01.294661   966 solver.cpp:228] Iteration 129800, loss = 0.173994
I0430 01:22:01.294806   966 solver.cpp:244]     Train net output #0: loss = 0.173994 (* 1 = 0.173994 loss)
I0430 01:22:01.294813   966 sgd_solver.cpp:106] Iteration 129800, lr = 1e-19
I0430 01:22:50.835614   966 solver.cpp:228] Iteration 129900, loss = 0.141527
I0430 01:22:50.835757   966 solver.cpp:244]     Train net output #0: loss = 0.141527 (* 1 = 0.141527 loss)
I0430 01:22:50.835764   966 sgd_solver.cpp:106] Iteration 129900, lr = 1e-19
I0430 01:23:41.928056   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_130000.caffemodel
I0430 01:23:56.153167   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_130000.solverstate
I0430 01:23:56.364495   966 solver.cpp:337] Iteration 130000, Testing net (#0)
I0430 01:23:56.364601   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 01:23:56.364608   966 net.cpp:693] Ignoring source layer visualize
I0430 01:23:56.364609   966 net.cpp:693] Ignoring source layer fake
I0430 01:28:48.711918   966 solver.cpp:404]     Test net output #0: loss = 0.181795 (* 1 = 0.181795 loss)
I0430 01:28:49.022370   966 solver.cpp:228] Iteration 130000, loss = 0.0816765
I0430 01:28:49.022408   966 solver.cpp:244]     Train net output #0: loss = 0.0816765 (* 1 = 0.0816765 loss)
I0430 01:28:49.022414   966 sgd_solver.cpp:106] Iteration 130000, lr = 1e-19
I0430 01:29:38.544039   966 solver.cpp:228] Iteration 130100, loss = 0.0901184
I0430 01:29:38.544210   966 solver.cpp:244]     Train net output #0: loss = 0.0901184 (* 1 = 0.0901184 loss)
I0430 01:29:38.544219   966 sgd_solver.cpp:106] Iteration 130100, lr = 1e-19
I0430 01:30:28.083066   966 solver.cpp:228] Iteration 130200, loss = 0.11298
I0430 01:30:28.083221   966 solver.cpp:244]     Train net output #0: loss = 0.11298 (* 1 = 0.11298 loss)
I0430 01:30:28.083228   966 sgd_solver.cpp:106] Iteration 130200, lr = 1e-19
I0430 01:31:19.444232   966 solver.cpp:228] Iteration 130300, loss = 0.132438
I0430 01:31:19.444387   966 solver.cpp:244]     Train net output #0: loss = 0.132438 (* 1 = 0.132438 loss)
I0430 01:31:19.444394   966 sgd_solver.cpp:106] Iteration 130300, lr = 1e-19
I0430 01:32:08.960989   966 solver.cpp:228] Iteration 130400, loss = 0.0520634
I0430 01:32:08.961149   966 solver.cpp:244]     Train net output #0: loss = 0.0520634 (* 1 = 0.0520634 loss)
I0430 01:32:08.961156   966 sgd_solver.cpp:106] Iteration 130400, lr = 1e-19
I0430 01:32:58.465916   966 solver.cpp:228] Iteration 130500, loss = 0.0881763
I0430 01:32:58.466089   966 solver.cpp:244]     Train net output #0: loss = 0.0881763 (* 1 = 0.0881763 loss)
I0430 01:32:58.466095   966 sgd_solver.cpp:106] Iteration 130500, lr = 1e-19
I0430 01:33:49.857123   966 solver.cpp:228] Iteration 130600, loss = 0.0872426
I0430 01:33:49.857300   966 solver.cpp:244]     Train net output #0: loss = 0.0872426 (* 1 = 0.0872426 loss)
I0430 01:33:49.857306   966 sgd_solver.cpp:106] Iteration 130600, lr = 1e-19
I0430 01:34:39.364028   966 solver.cpp:228] Iteration 130700, loss = 0.114335
I0430 01:34:39.364192   966 solver.cpp:244]     Train net output #0: loss = 0.114335 (* 1 = 0.114335 loss)
I0430 01:34:39.364198   966 sgd_solver.cpp:106] Iteration 130700, lr = 1e-19
I0430 01:35:28.878765   966 solver.cpp:228] Iteration 130800, loss = 0.142428
I0430 01:35:28.878943   966 solver.cpp:244]     Train net output #0: loss = 0.142428 (* 1 = 0.142428 loss)
I0430 01:35:28.878950   966 sgd_solver.cpp:106] Iteration 130800, lr = 1e-19
I0430 01:36:18.406683   966 solver.cpp:228] Iteration 130900, loss = 0.134307
I0430 01:36:18.406839   966 solver.cpp:244]     Train net output #0: loss = 0.134307 (* 1 = 0.134307 loss)
I0430 01:36:18.406847   966 sgd_solver.cpp:106] Iteration 130900, lr = 1e-19
I0430 01:37:09.317704   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_131000.caffemodel
I0430 01:37:46.285946   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_131000.solverstate
I0430 01:37:46.476336   966 solver.cpp:337] Iteration 131000, Testing net (#0)
I0430 01:37:46.476475   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 01:37:46.476481   966 net.cpp:693] Ignoring source layer visualize
I0430 01:37:46.476485   966 net.cpp:693] Ignoring source layer fake
I0430 01:42:38.711864   966 solver.cpp:404]     Test net output #0: loss = 0.179773 (* 1 = 0.179773 loss)
I0430 01:42:39.021127   966 solver.cpp:228] Iteration 131000, loss = 0.161706
I0430 01:42:39.021167   966 solver.cpp:244]     Train net output #0: loss = 0.161706 (* 1 = 0.161706 loss)
I0430 01:42:39.021173   966 sgd_solver.cpp:106] Iteration 131000, lr = 1e-19
I0430 01:43:28.548250   966 solver.cpp:228] Iteration 131100, loss = 0.192714
I0430 01:43:28.548413   966 solver.cpp:244]     Train net output #0: loss = 0.192714 (* 1 = 0.192714 loss)
I0430 01:43:28.548419   966 sgd_solver.cpp:106] Iteration 131100, lr = 1e-19
I0430 01:44:18.072619   966 solver.cpp:228] Iteration 131200, loss = 0.140662
I0430 01:44:18.072819   966 solver.cpp:244]     Train net output #0: loss = 0.140662 (* 1 = 0.140662 loss)
I0430 01:44:18.072842   966 sgd_solver.cpp:106] Iteration 131200, lr = 1e-19
I0430 01:45:08.881139   966 solver.cpp:228] Iteration 131300, loss = 0.144461
I0430 01:45:08.881295   966 solver.cpp:244]     Train net output #0: loss = 0.144461 (* 1 = 0.144461 loss)
I0430 01:45:08.881302   966 sgd_solver.cpp:106] Iteration 131300, lr = 1e-19
I0430 01:45:58.399504   966 solver.cpp:228] Iteration 131400, loss = 0.139877
I0430 01:45:58.399660   966 solver.cpp:244]     Train net output #0: loss = 0.139877 (* 1 = 0.139877 loss)
I0430 01:45:58.399667   966 sgd_solver.cpp:106] Iteration 131400, lr = 1e-19
I0430 01:46:49.335954   966 solver.cpp:228] Iteration 131500, loss = 0.180641
I0430 01:46:49.336122   966 solver.cpp:244]     Train net output #0: loss = 0.180641 (* 1 = 0.180641 loss)
I0430 01:46:49.336128   966 sgd_solver.cpp:106] Iteration 131500, lr = 1e-19
I0430 01:47:38.868618   966 solver.cpp:228] Iteration 131600, loss = 0.214441
I0430 01:47:38.868769   966 solver.cpp:244]     Train net output #0: loss = 0.214441 (* 1 = 0.214441 loss)
I0430 01:47:38.868777   966 sgd_solver.cpp:106] Iteration 131600, lr = 1e-19
I0430 01:48:28.399740   966 solver.cpp:228] Iteration 131700, loss = 0.114306
I0430 01:48:28.399890   966 solver.cpp:244]     Train net output #0: loss = 0.114306 (* 1 = 0.114306 loss)
I0430 01:48:28.399896   966 sgd_solver.cpp:106] Iteration 131700, lr = 1e-19
I0430 01:49:19.455899   966 solver.cpp:228] Iteration 131800, loss = 0.130849
I0430 01:49:19.456073   966 solver.cpp:244]     Train net output #0: loss = 0.130849 (* 1 = 0.130849 loss)
I0430 01:49:19.456079   966 sgd_solver.cpp:106] Iteration 131800, lr = 1e-19
I0430 01:50:08.986613   966 solver.cpp:228] Iteration 131900, loss = 0.219189
I0430 01:50:08.986763   966 solver.cpp:244]     Train net output #0: loss = 0.219189 (* 1 = 0.219189 loss)
I0430 01:50:08.986768   966 sgd_solver.cpp:106] Iteration 131900, lr = 1e-19
I0430 01:50:58.224172   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_132000.caffemodel
I0430 01:51:33.240514   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_132000.solverstate
I0430 01:51:33.444592   966 solver.cpp:337] Iteration 132000, Testing net (#0)
I0430 01:51:33.444730   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 01:51:33.444736   966 net.cpp:693] Ignoring source layer visualize
I0430 01:51:33.444738   966 net.cpp:693] Ignoring source layer fake
I0430 01:56:25.390065   966 solver.cpp:404]     Test net output #0: loss = 0.189039 (* 1 = 0.189039 loss)
I0430 01:56:25.701213   966 solver.cpp:228] Iteration 132000, loss = 0.226153
I0430 01:56:25.701231   966 solver.cpp:244]     Train net output #0: loss = 0.226153 (* 1 = 0.226153 loss)
I0430 01:56:25.701253   966 sgd_solver.cpp:106] Iteration 132000, lr = 1e-19
I0430 01:57:17.043540   966 solver.cpp:228] Iteration 132100, loss = 0.162902
I0430 01:57:17.043714   966 solver.cpp:244]     Train net output #0: loss = 0.162902 (* 1 = 0.162902 loss)
I0430 01:57:17.043720   966 sgd_solver.cpp:106] Iteration 132100, lr = 1e-19
I0430 01:58:06.566884   966 solver.cpp:228] Iteration 132200, loss = 0.173365
I0430 01:58:06.567047   966 solver.cpp:244]     Train net output #0: loss = 0.173365 (* 1 = 0.173365 loss)
I0430 01:58:06.567054   966 sgd_solver.cpp:106] Iteration 132200, lr = 1e-19
I0430 01:58:56.097200   966 solver.cpp:228] Iteration 132300, loss = 0.134429
I0430 01:58:56.097345   966 solver.cpp:244]     Train net output #0: loss = 0.134429 (* 1 = 0.134429 loss)
I0430 01:58:56.097353   966 sgd_solver.cpp:106] Iteration 132300, lr = 1e-19
I0430 01:59:47.453168   966 solver.cpp:228] Iteration 132400, loss = 0.108373
I0430 01:59:47.453323   966 solver.cpp:244]     Train net output #0: loss = 0.108373 (* 1 = 0.108373 loss)
I0430 01:59:47.453330   966 sgd_solver.cpp:106] Iteration 132400, lr = 1e-19
I0430 02:00:36.979859   966 solver.cpp:228] Iteration 132500, loss = 0.0795006
I0430 02:00:36.980023   966 solver.cpp:244]     Train net output #0: loss = 0.0795006 (* 1 = 0.0795006 loss)
I0430 02:00:36.980031   966 sgd_solver.cpp:106] Iteration 132500, lr = 1e-19
I0430 02:01:26.504031   966 solver.cpp:228] Iteration 132600, loss = 0.106208
I0430 02:01:26.504186   966 solver.cpp:244]     Train net output #0: loss = 0.106208 (* 1 = 0.106208 loss)
I0430 02:01:26.504192   966 sgd_solver.cpp:106] Iteration 132600, lr = 1e-19
I0430 02:02:17.859518   966 solver.cpp:228] Iteration 132700, loss = 0.134641
I0430 02:02:17.859676   966 solver.cpp:244]     Train net output #0: loss = 0.134641 (* 1 = 0.134641 loss)
I0430 02:02:17.859683   966 sgd_solver.cpp:106] Iteration 132700, lr = 1e-19
I0430 02:03:07.377279   966 solver.cpp:228] Iteration 132800, loss = 0.206722
I0430 02:03:07.377461   966 solver.cpp:244]     Train net output #0: loss = 0.206722 (* 1 = 0.206722 loss)
I0430 02:03:07.377467   966 sgd_solver.cpp:106] Iteration 132800, lr = 1e-19
I0430 02:03:56.899622   966 solver.cpp:228] Iteration 132900, loss = 0.109495
I0430 02:03:56.899787   966 solver.cpp:244]     Train net output #0: loss = 0.109495 (* 1 = 0.109495 loss)
I0430 02:03:56.899794   966 sgd_solver.cpp:106] Iteration 132900, lr = 1e-19
I0430 02:04:46.125314   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_133000.caffemodel
I0430 02:04:56.557550   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_133000.solverstate
I0430 02:04:56.747223   966 solver.cpp:337] Iteration 133000, Testing net (#0)
I0430 02:04:56.747344   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 02:04:56.747349   966 net.cpp:693] Ignoring source layer visualize
I0430 02:04:56.747351   966 net.cpp:693] Ignoring source layer fake
I0430 02:09:49.293010   966 solver.cpp:404]     Test net output #0: loss = 0.188769 (* 1 = 0.188769 loss)
I0430 02:09:49.601696   966 solver.cpp:228] Iteration 133000, loss = 0.142671
I0430 02:09:49.601732   966 solver.cpp:244]     Train net output #0: loss = 0.142671 (* 1 = 0.142671 loss)
I0430 02:09:49.601738   966 sgd_solver.cpp:106] Iteration 133000, lr = 1e-19
I0430 02:10:40.947293   966 solver.cpp:228] Iteration 133100, loss = 0.0863445
I0430 02:10:40.947507   966 solver.cpp:244]     Train net output #0: loss = 0.0863445 (* 1 = 0.0863445 loss)
I0430 02:10:40.947516   966 sgd_solver.cpp:106] Iteration 133100, lr = 1e-19
I0430 02:11:30.449803   966 solver.cpp:228] Iteration 133200, loss = 0.169647
I0430 02:11:30.450028   966 solver.cpp:244]     Train net output #0: loss = 0.169647 (* 1 = 0.169647 loss)
I0430 02:11:30.450037   966 sgd_solver.cpp:106] Iteration 133200, lr = 1e-19
I0430 02:12:19.977830   966 solver.cpp:228] Iteration 133300, loss = 0.148928
I0430 02:12:19.978165   966 solver.cpp:244]     Train net output #0: loss = 0.148928 (* 1 = 0.148928 loss)
I0430 02:12:19.978174   966 sgd_solver.cpp:106] Iteration 133300, lr = 1e-19
I0430 02:13:11.267580   966 solver.cpp:228] Iteration 133400, loss = 0.130353
I0430 02:13:11.267913   966 solver.cpp:244]     Train net output #0: loss = 0.130353 (* 1 = 0.130353 loss)
I0430 02:13:11.267922   966 sgd_solver.cpp:106] Iteration 133400, lr = 1e-19
I0430 02:14:00.769645   966 solver.cpp:228] Iteration 133500, loss = 0.0906733
I0430 02:14:00.769798   966 solver.cpp:244]     Train net output #0: loss = 0.0906733 (* 1 = 0.0906733 loss)
I0430 02:14:00.769805   966 sgd_solver.cpp:106] Iteration 133500, lr = 1e-19
I0430 02:14:50.290685   966 solver.cpp:228] Iteration 133600, loss = 0.103957
I0430 02:14:50.290849   966 solver.cpp:244]     Train net output #0: loss = 0.103957 (* 1 = 0.103957 loss)
I0430 02:14:50.290856   966 sgd_solver.cpp:106] Iteration 133600, lr = 1e-19
I0430 02:15:39.799813   966 solver.cpp:228] Iteration 133700, loss = 0.107336
I0430 02:15:39.799975   966 solver.cpp:244]     Train net output #0: loss = 0.107336 (* 1 = 0.107336 loss)
I0430 02:15:39.799983   966 sgd_solver.cpp:106] Iteration 133700, lr = 1e-19
I0430 02:16:31.118388   966 solver.cpp:228] Iteration 133800, loss = 0.0836707
I0430 02:16:31.118546   966 solver.cpp:244]     Train net output #0: loss = 0.0836707 (* 1 = 0.0836707 loss)
I0430 02:16:31.118553   966 sgd_solver.cpp:106] Iteration 133800, lr = 1e-19
I0430 02:17:20.618710   966 solver.cpp:228] Iteration 133900, loss = 0.0264575
I0430 02:17:20.618880   966 solver.cpp:244]     Train net output #0: loss = 0.0264575 (* 1 = 0.0264575 loss)
I0430 02:17:20.618885   966 sgd_solver.cpp:106] Iteration 133900, lr = 1e-19
I0430 02:18:09.816138   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_134000.caffemodel
I0430 02:18:22.389360   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_134000.solverstate
I0430 02:18:22.731295   966 solver.cpp:337] Iteration 134000, Testing net (#0)
I0430 02:18:22.731420   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 02:18:22.731427   966 net.cpp:693] Ignoring source layer visualize
I0430 02:18:22.731431   966 net.cpp:693] Ignoring source layer fake
I0430 02:23:15.457481   966 solver.cpp:404]     Test net output #0: loss = 0.182118 (* 1 = 0.182118 loss)
I0430 02:23:15.768218   966 solver.cpp:228] Iteration 134000, loss = 0.143627
I0430 02:23:15.768237   966 solver.cpp:244]     Train net output #0: loss = 0.143627 (* 1 = 0.143627 loss)
I0430 02:23:15.768260   966 sgd_solver.cpp:106] Iteration 134000, lr = 1e-19
I0430 02:24:07.013777   966 solver.cpp:228] Iteration 134100, loss = 0.0969929
I0430 02:24:07.013963   966 solver.cpp:244]     Train net output #0: loss = 0.0969929 (* 1 = 0.0969929 loss)
I0430 02:24:07.013972   966 sgd_solver.cpp:106] Iteration 134100, lr = 1e-19
I0430 02:24:56.539206   966 solver.cpp:228] Iteration 134200, loss = 0.0984345
I0430 02:24:56.539412   966 solver.cpp:244]     Train net output #0: loss = 0.0984345 (* 1 = 0.0984345 loss)
I0430 02:24:56.539420   966 sgd_solver.cpp:106] Iteration 134200, lr = 1e-19
I0430 02:25:46.057015   966 solver.cpp:228] Iteration 134300, loss = 0.0844185
I0430 02:25:46.058607   966 solver.cpp:244]     Train net output #0: loss = 0.0844185 (* 1 = 0.0844185 loss)
I0430 02:25:46.058616   966 sgd_solver.cpp:106] Iteration 134300, lr = 1e-19
I0430 02:26:36.897002   966 solver.cpp:228] Iteration 134400, loss = 0.151826
I0430 02:26:36.897164   966 solver.cpp:244]     Train net output #0: loss = 0.151826 (* 1 = 0.151826 loss)
I0430 02:26:36.897171   966 sgd_solver.cpp:106] Iteration 134400, lr = 1e-19
I0430 02:27:26.431471   966 solver.cpp:228] Iteration 134500, loss = 0.0888787
I0430 02:27:26.431629   966 solver.cpp:244]     Train net output #0: loss = 0.0888787 (* 1 = 0.0888787 loss)
I0430 02:27:26.431638   966 sgd_solver.cpp:106] Iteration 134500, lr = 1e-19
I0430 02:28:17.349340   966 solver.cpp:228] Iteration 134600, loss = 0.212967
I0430 02:28:17.349514   966 solver.cpp:244]     Train net output #0: loss = 0.212967 (* 1 = 0.212967 loss)
I0430 02:28:17.349521   966 sgd_solver.cpp:106] Iteration 134600, lr = 1e-19
I0430 02:29:06.874852   966 solver.cpp:228] Iteration 134700, loss = 0.0989656
I0430 02:29:06.875026   966 solver.cpp:244]     Train net output #0: loss = 0.0989656 (* 1 = 0.0989656 loss)
I0430 02:29:06.875036   966 sgd_solver.cpp:106] Iteration 134700, lr = 1e-19
I0430 02:29:56.405433   966 solver.cpp:228] Iteration 134800, loss = 0.189092
I0430 02:29:56.405581   966 solver.cpp:244]     Train net output #0: loss = 0.189092 (* 1 = 0.189092 loss)
I0430 02:29:56.405587   966 sgd_solver.cpp:106] Iteration 134800, lr = 1e-19
I0430 02:30:47.693634   966 solver.cpp:228] Iteration 134900, loss = 0.141587
I0430 02:30:47.693788   966 solver.cpp:244]     Train net output #0: loss = 0.141587 (* 1 = 0.141587 loss)
I0430 02:30:47.693795   966 sgd_solver.cpp:106] Iteration 134900, lr = 1e-19
I0430 02:31:36.916359   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_135000.caffemodel
I0430 02:31:54.676748   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_135000.solverstate
I0430 02:31:54.890697   966 solver.cpp:337] Iteration 135000, Testing net (#0)
I0430 02:31:54.890844   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 02:31:54.890868   966 net.cpp:693] Ignoring source layer visualize
I0430 02:31:54.890872   966 net.cpp:693] Ignoring source layer fake
I0430 02:36:47.137663   966 solver.cpp:404]     Test net output #0: loss = 0.181231 (* 1 = 0.181231 loss)
I0430 02:36:47.446941   966 solver.cpp:228] Iteration 135000, loss = 0.0870045
I0430 02:36:47.446985   966 solver.cpp:244]     Train net output #0: loss = 0.0870045 (* 1 = 0.0870045 loss)
I0430 02:36:47.446995   966 sgd_solver.cpp:106] Iteration 135000, lr = 1e-20
I0430 02:37:36.979674   966 solver.cpp:228] Iteration 135100, loss = 0.120063
I0430 02:37:36.979833   966 solver.cpp:244]     Train net output #0: loss = 0.120063 (* 1 = 0.120063 loss)
I0430 02:37:36.979840   966 sgd_solver.cpp:106] Iteration 135100, lr = 1e-20
I0430 02:38:28.346976   966 solver.cpp:228] Iteration 135200, loss = 0.0849959
I0430 02:38:28.347141   966 solver.cpp:244]     Train net output #0: loss = 0.0849959 (* 1 = 0.0849959 loss)
I0430 02:38:28.347148   966 sgd_solver.cpp:106] Iteration 135200, lr = 1e-20
I0430 02:39:17.876407   966 solver.cpp:228] Iteration 135300, loss = 0.265887
I0430 02:39:17.876557   966 solver.cpp:244]     Train net output #0: loss = 0.265887 (* 1 = 0.265887 loss)
I0430 02:39:17.876565   966 sgd_solver.cpp:106] Iteration 135300, lr = 1e-20
I0430 02:40:07.405931   966 solver.cpp:228] Iteration 135400, loss = 0.14764
I0430 02:40:07.406100   966 solver.cpp:244]     Train net output #0: loss = 0.14764 (* 1 = 0.14764 loss)
I0430 02:40:07.406110   966 sgd_solver.cpp:106] Iteration 135400, lr = 1e-20
I0430 02:40:58.822443   966 solver.cpp:228] Iteration 135500, loss = 0.173926
I0430 02:40:58.822615   966 solver.cpp:244]     Train net output #0: loss = 0.173926 (* 1 = 0.173926 loss)
I0430 02:40:58.822624   966 sgd_solver.cpp:106] Iteration 135500, lr = 1e-20
I0430 02:41:48.340943   966 solver.cpp:228] Iteration 135600, loss = 0.127562
I0430 02:41:48.341114   966 solver.cpp:244]     Train net output #0: loss = 0.127562 (* 1 = 0.127562 loss)
I0430 02:41:48.341122   966 sgd_solver.cpp:106] Iteration 135600, lr = 1e-20
I0430 02:42:37.887389   966 solver.cpp:228] Iteration 135700, loss = 0.0865605
I0430 02:42:37.888881   966 solver.cpp:244]     Train net output #0: loss = 0.0865605 (* 1 = 0.0865605 loss)
I0430 02:42:37.888905   966 sgd_solver.cpp:106] Iteration 135700, lr = 1e-20
I0430 02:43:27.416116   966 solver.cpp:228] Iteration 135800, loss = 0.127847
I0430 02:43:27.416270   966 solver.cpp:244]     Train net output #0: loss = 0.127847 (* 1 = 0.127847 loss)
I0430 02:43:27.416277   966 sgd_solver.cpp:106] Iteration 135800, lr = 1e-20
I0430 02:44:18.881958   966 solver.cpp:228] Iteration 135900, loss = 0.156248
I0430 02:44:18.882114   966 solver.cpp:244]     Train net output #0: loss = 0.156248 (* 1 = 0.156248 loss)
I0430 02:44:18.882122   966 sgd_solver.cpp:106] Iteration 135900, lr = 1e-20
I0430 02:45:08.092120   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_136000.caffemodel
I0430 02:45:17.572831   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_136000.solverstate
I0430 02:45:17.779043   966 solver.cpp:337] Iteration 136000, Testing net (#0)
I0430 02:45:17.779166   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 02:45:17.779172   966 net.cpp:693] Ignoring source layer visualize
I0430 02:45:17.779175   966 net.cpp:693] Ignoring source layer fake
I0430 02:50:11.160581   966 solver.cpp:404]     Test net output #0: loss = 0.180203 (* 1 = 0.180203 loss)
I0430 02:50:11.470888   966 solver.cpp:228] Iteration 136000, loss = 0.192705
I0430 02:50:11.470930   966 solver.cpp:244]     Train net output #0: loss = 0.192705 (* 1 = 0.192705 loss)
I0430 02:50:11.470938   966 sgd_solver.cpp:106] Iteration 136000, lr = 1e-20
I0430 02:51:01.008935   966 solver.cpp:228] Iteration 136100, loss = 0.177762
I0430 02:51:01.009090   966 solver.cpp:244]     Train net output #0: loss = 0.177762 (* 1 = 0.177762 loss)
I0430 02:51:01.009097   966 sgd_solver.cpp:106] Iteration 136100, lr = 1e-20
I0430 02:51:52.443114   966 solver.cpp:228] Iteration 136200, loss = 0.130123
I0430 02:51:52.443300   966 solver.cpp:244]     Train net output #0: loss = 0.130123 (* 1 = 0.130123 loss)
I0430 02:51:52.443307   966 sgd_solver.cpp:106] Iteration 136200, lr = 1e-20
I0430 02:52:41.962431   966 solver.cpp:228] Iteration 136300, loss = 0.0928757
I0430 02:52:41.962595   966 solver.cpp:244]     Train net output #0: loss = 0.0928757 (* 1 = 0.0928757 loss)
I0430 02:52:41.962604   966 sgd_solver.cpp:106] Iteration 136300, lr = 1e-20
I0430 02:53:31.492955   966 solver.cpp:228] Iteration 136400, loss = 0.142022
I0430 02:53:31.493129   966 solver.cpp:244]     Train net output #0: loss = 0.142022 (* 1 = 0.142022 loss)
I0430 02:53:31.493139   966 sgd_solver.cpp:106] Iteration 136400, lr = 1e-20
I0430 02:54:21.013622   966 solver.cpp:228] Iteration 136500, loss = 0.107427
I0430 02:54:21.013779   966 solver.cpp:244]     Train net output #0: loss = 0.107427 (* 1 = 0.107427 loss)
I0430 02:54:21.013787   966 sgd_solver.cpp:106] Iteration 136500, lr = 1e-20
I0430 02:55:12.526798   966 solver.cpp:228] Iteration 136600, loss = 0.112815
I0430 02:55:12.527130   966 solver.cpp:244]     Train net output #0: loss = 0.112815 (* 1 = 0.112815 loss)
I0430 02:55:12.527139   966 sgd_solver.cpp:106] Iteration 136600, lr = 1e-20
I0430 02:56:02.040874   966 solver.cpp:228] Iteration 136700, loss = 0.121949
I0430 02:56:02.041031   966 solver.cpp:244]     Train net output #0: loss = 0.121949 (* 1 = 0.121949 loss)
I0430 02:56:02.041039   966 sgd_solver.cpp:106] Iteration 136700, lr = 1e-20
I0430 02:56:51.568378   966 solver.cpp:228] Iteration 136800, loss = 0.121496
I0430 02:56:51.568580   966 solver.cpp:244]     Train net output #0: loss = 0.121496 (* 1 = 0.121496 loss)
I0430 02:56:51.568589   966 sgd_solver.cpp:106] Iteration 136800, lr = 1e-20
I0430 02:57:42.932133   966 solver.cpp:228] Iteration 136900, loss = 0.148692
I0430 02:57:42.932337   966 solver.cpp:244]     Train net output #0: loss = 0.148692 (* 1 = 0.148692 loss)
I0430 02:57:42.932346   966 sgd_solver.cpp:106] Iteration 136900, lr = 1e-20
I0430 02:58:32.126541   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_137000.caffemodel
I0430 02:58:52.864109   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_137000.solverstate
I0430 02:58:53.063169   966 solver.cpp:337] Iteration 137000, Testing net (#0)
I0430 02:58:53.063295   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 02:58:53.063302   966 net.cpp:693] Ignoring source layer visualize
I0430 02:58:53.063303   966 net.cpp:693] Ignoring source layer fake
I0430 03:03:45.286129   966 solver.cpp:404]     Test net output #0: loss = 0.19052 (* 1 = 0.19052 loss)
I0430 03:03:45.596299   966 solver.cpp:228] Iteration 137000, loss = 0.109302
I0430 03:03:45.596319   966 solver.cpp:244]     Train net output #0: loss = 0.109302 (* 1 = 0.109302 loss)
I0430 03:03:45.596343   966 sgd_solver.cpp:106] Iteration 137000, lr = 1e-20
I0430 03:04:35.120342   966 solver.cpp:228] Iteration 137100, loss = 0.112504
I0430 03:04:35.120498   966 solver.cpp:244]     Train net output #0: loss = 0.112504 (* 1 = 0.112504 loss)
I0430 03:04:35.120506   966 sgd_solver.cpp:106] Iteration 137100, lr = 1e-20
I0430 03:05:26.333438   966 solver.cpp:228] Iteration 137200, loss = 0.134188
I0430 03:05:26.333598   966 solver.cpp:244]     Train net output #0: loss = 0.134188 (* 1 = 0.134188 loss)
I0430 03:05:26.333606   966 sgd_solver.cpp:106] Iteration 137200, lr = 1e-20
I0430 03:06:15.829074   966 solver.cpp:228] Iteration 137300, loss = 0.187989
I0430 03:06:15.829237   966 solver.cpp:244]     Train net output #0: loss = 0.187989 (* 1 = 0.187989 loss)
I0430 03:06:15.829246   966 sgd_solver.cpp:106] Iteration 137300, lr = 1e-20
I0430 03:07:05.341656   966 solver.cpp:228] Iteration 137400, loss = 0.116244
I0430 03:07:05.341814   966 solver.cpp:244]     Train net output #0: loss = 0.116244 (* 1 = 0.116244 loss)
I0430 03:07:05.341821   966 sgd_solver.cpp:106] Iteration 137400, lr = 1e-20
I0430 03:07:56.167846   966 solver.cpp:228] Iteration 137500, loss = 0.116228
I0430 03:07:56.168000   966 solver.cpp:244]     Train net output #0: loss = 0.116228 (* 1 = 0.116228 loss)
I0430 03:07:56.168007   966 sgd_solver.cpp:106] Iteration 137500, lr = 1e-20
I0430 03:08:45.679746   966 solver.cpp:228] Iteration 137600, loss = 0.14705
I0430 03:08:45.679900   966 solver.cpp:244]     Train net output #0: loss = 0.14705 (* 1 = 0.14705 loss)
I0430 03:08:45.679906   966 sgd_solver.cpp:106] Iteration 137600, lr = 1e-20
I0430 03:09:35.202752   966 solver.cpp:228] Iteration 137700, loss = 0.132907
I0430 03:09:35.202904   966 solver.cpp:244]     Train net output #0: loss = 0.132907 (* 1 = 0.132907 loss)
I0430 03:09:35.202910   966 sgd_solver.cpp:106] Iteration 137700, lr = 1e-20
I0430 03:10:26.115887   966 solver.cpp:228] Iteration 137800, loss = 0.146465
I0430 03:10:26.116039   966 solver.cpp:244]     Train net output #0: loss = 0.146465 (* 1 = 0.146465 loss)
I0430 03:10:26.116045   966 sgd_solver.cpp:106] Iteration 137800, lr = 1e-20
I0430 03:11:15.637440   966 solver.cpp:228] Iteration 137900, loss = 0.226363
I0430 03:11:15.637636   966 solver.cpp:244]     Train net output #0: loss = 0.226363 (* 1 = 0.226363 loss)
I0430 03:11:15.637645   966 sgd_solver.cpp:106] Iteration 137900, lr = 1e-20
I0430 03:12:06.453564   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_138000.caffemodel
I0430 03:12:18.580926   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_138000.solverstate
I0430 03:12:18.786968   966 solver.cpp:337] Iteration 138000, Testing net (#0)
I0430 03:12:18.787091   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 03:12:18.787096   966 net.cpp:693] Ignoring source layer visualize
I0430 03:12:18.787098   966 net.cpp:693] Ignoring source layer fake
I0430 03:17:11.391559   966 solver.cpp:404]     Test net output #0: loss = 0.187679 (* 1 = 0.187679 loss)
I0430 03:17:11.700426   966 solver.cpp:228] Iteration 138000, loss = 0.18327
I0430 03:17:11.700465   966 solver.cpp:244]     Train net output #0: loss = 0.18327 (* 1 = 0.18327 loss)
I0430 03:17:11.700474   966 sgd_solver.cpp:106] Iteration 138000, lr = 1e-20
I0430 03:18:01.228569   966 solver.cpp:228] Iteration 138100, loss = 0.104304
I0430 03:18:01.228754   966 solver.cpp:244]     Train net output #0: loss = 0.104304 (* 1 = 0.104304 loss)
I0430 03:18:01.228763   966 sgd_solver.cpp:106] Iteration 138100, lr = 1e-20
I0430 03:18:50.757277   966 solver.cpp:228] Iteration 138200, loss = 0.0975412
I0430 03:18:50.757452   966 solver.cpp:244]     Train net output #0: loss = 0.0975412 (* 1 = 0.0975412 loss)
I0430 03:18:50.757459   966 sgd_solver.cpp:106] Iteration 138200, lr = 1e-20
I0430 03:19:42.324417   966 solver.cpp:228] Iteration 138300, loss = 0.115035
I0430 03:19:42.324575   966 solver.cpp:244]     Train net output #0: loss = 0.115035 (* 1 = 0.115035 loss)
I0430 03:19:42.324582   966 sgd_solver.cpp:106] Iteration 138300, lr = 1e-20
I0430 03:20:31.838373   966 solver.cpp:228] Iteration 138400, loss = 0.144435
I0430 03:20:31.838524   966 solver.cpp:244]     Train net output #0: loss = 0.144435 (* 1 = 0.144435 loss)
I0430 03:20:31.838531   966 sgd_solver.cpp:106] Iteration 138400, lr = 1e-20
I0430 03:21:21.367221   966 solver.cpp:228] Iteration 138500, loss = 0.217907
I0430 03:21:21.367380   966 solver.cpp:244]     Train net output #0: loss = 0.217907 (* 1 = 0.217907 loss)
I0430 03:21:21.367388   966 sgd_solver.cpp:106] Iteration 138500, lr = 1e-20
I0430 03:22:12.802795   966 solver.cpp:228] Iteration 138600, loss = 0.152883
I0430 03:22:12.802966   966 solver.cpp:244]     Train net output #0: loss = 0.152883 (* 1 = 0.152883 loss)
I0430 03:22:12.802973   966 sgd_solver.cpp:106] Iteration 138600, lr = 1e-20
I0430 03:23:02.321504   966 solver.cpp:228] Iteration 138700, loss = 0.0828454
I0430 03:23:02.321660   966 solver.cpp:244]     Train net output #0: loss = 0.0828454 (* 1 = 0.0828454 loss)
I0430 03:23:02.321666   966 sgd_solver.cpp:106] Iteration 138700, lr = 1e-20
I0430 03:23:51.842033   966 solver.cpp:228] Iteration 138800, loss = 0.154223
I0430 03:23:51.842185   966 solver.cpp:244]     Train net output #0: loss = 0.154223 (* 1 = 0.154223 loss)
I0430 03:23:51.842192   966 sgd_solver.cpp:106] Iteration 138800, lr = 1e-20
I0430 03:24:41.375504   966 solver.cpp:228] Iteration 138900, loss = 0.126316
I0430 03:24:41.375666   966 solver.cpp:244]     Train net output #0: loss = 0.126316 (* 1 = 0.126316 loss)
I0430 03:24:41.375674   966 sgd_solver.cpp:106] Iteration 138900, lr = 1e-20
I0430 03:25:32.516171   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_139000.caffemodel
I0430 03:25:47.905608   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_139000.solverstate
I0430 03:25:48.113358   966 solver.cpp:337] Iteration 139000, Testing net (#0)
I0430 03:25:48.113521   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 03:25:48.113528   966 net.cpp:693] Ignoring source layer visualize
I0430 03:25:48.113530   966 net.cpp:693] Ignoring source layer fake
I0430 03:30:40.061702   966 solver.cpp:404]     Test net output #0: loss = 0.181853 (* 1 = 0.181853 loss)
I0430 03:30:40.370558   966 solver.cpp:228] Iteration 139000, loss = 0.210112
I0430 03:30:40.370595   966 solver.cpp:244]     Train net output #0: loss = 0.210112 (* 1 = 0.210112 loss)
I0430 03:30:40.370602   966 sgd_solver.cpp:106] Iteration 139000, lr = 1e-20
I0430 03:31:29.881662   966 solver.cpp:228] Iteration 139100, loss = 0.109156
I0430 03:31:29.881822   966 solver.cpp:244]     Train net output #0: loss = 0.109156 (* 1 = 0.109156 loss)
I0430 03:31:29.881829   966 sgd_solver.cpp:106] Iteration 139100, lr = 1e-20
I0430 03:32:19.399936   966 solver.cpp:228] Iteration 139200, loss = 0.117675
I0430 03:32:19.400113   966 solver.cpp:244]     Train net output #0: loss = 0.117675 (* 1 = 0.117675 loss)
I0430 03:32:19.400123   966 sgd_solver.cpp:106] Iteration 139200, lr = 1e-20
I0430 03:33:10.806991   966 solver.cpp:228] Iteration 139300, loss = 0.11741
I0430 03:33:10.807183   966 solver.cpp:244]     Train net output #0: loss = 0.11741 (* 1 = 0.11741 loss)
I0430 03:33:10.807193   966 sgd_solver.cpp:106] Iteration 139300, lr = 1e-20
I0430 03:34:00.305065   966 solver.cpp:228] Iteration 139400, loss = 0.130875
I0430 03:34:00.305229   966 solver.cpp:244]     Train net output #0: loss = 0.130875 (* 1 = 0.130875 loss)
I0430 03:34:00.305236   966 sgd_solver.cpp:106] Iteration 139400, lr = 1e-20
I0430 03:34:49.805655   966 solver.cpp:228] Iteration 139500, loss = 0.238989
I0430 03:34:49.805822   966 solver.cpp:244]     Train net output #0: loss = 0.238989 (* 1 = 0.238989 loss)
I0430 03:34:49.805830   966 sgd_solver.cpp:106] Iteration 139500, lr = 1e-20
I0430 03:35:39.317996   966 solver.cpp:228] Iteration 139600, loss = 0.148629
I0430 03:35:39.318178   966 solver.cpp:244]     Train net output #0: loss = 0.148629 (* 1 = 0.148629 loss)
I0430 03:35:39.318188   966 sgd_solver.cpp:106] Iteration 139600, lr = 1e-20
I0430 03:36:30.771806   966 solver.cpp:228] Iteration 139700, loss = 0.232674
I0430 03:36:30.773044   966 solver.cpp:244]     Train net output #0: loss = 0.232674 (* 1 = 0.232674 loss)
I0430 03:36:30.773052   966 sgd_solver.cpp:106] Iteration 139700, lr = 1e-20
I0430 03:37:20.267910   966 solver.cpp:228] Iteration 139800, loss = 0.159971
I0430 03:37:20.268082   966 solver.cpp:244]     Train net output #0: loss = 0.159971 (* 1 = 0.159971 loss)
I0430 03:37:20.268088   966 sgd_solver.cpp:106] Iteration 139800, lr = 1e-20
I0430 03:38:09.760340   966 solver.cpp:228] Iteration 139900, loss = 0.126516
I0430 03:38:09.760507   966 solver.cpp:244]     Train net output #0: loss = 0.126516 (* 1 = 0.126516 loss)
I0430 03:38:09.760514   966 sgd_solver.cpp:106] Iteration 139900, lr = 1e-20
I0430 03:39:01.365768   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_140000.caffemodel
I0430 03:39:09.676267   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_140000.solverstate
I0430 03:39:09.867482   966 solver.cpp:337] Iteration 140000, Testing net (#0)
I0430 03:39:09.867606   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 03:39:09.867619   966 net.cpp:693] Ignoring source layer visualize
I0430 03:39:09.867637   966 net.cpp:693] Ignoring source layer fake
I0430 03:44:02.770107   966 solver.cpp:404]     Test net output #0: loss = 0.180658 (* 1 = 0.180658 loss)
I0430 03:44:03.081061   966 solver.cpp:228] Iteration 140000, loss = 0.0962534
I0430 03:44:03.081082   966 solver.cpp:244]     Train net output #0: loss = 0.0962534 (* 1 = 0.0962534 loss)
I0430 03:44:03.081104   966 sgd_solver.cpp:106] Iteration 140000, lr = 1e-20
I0430 03:44:52.586601   966 solver.cpp:228] Iteration 140100, loss = 0.0931599
I0430 03:44:52.586752   966 solver.cpp:244]     Train net output #0: loss = 0.0931599 (* 1 = 0.0931599 loss)
I0430 03:44:52.586760   966 sgd_solver.cpp:106] Iteration 140100, lr = 1e-20
I0430 03:45:42.087435   966 solver.cpp:228] Iteration 140200, loss = 0.14833
I0430 03:45:42.087597   966 solver.cpp:244]     Train net output #0: loss = 0.14833 (* 1 = 0.14833 loss)
I0430 03:45:42.087604   966 sgd_solver.cpp:106] Iteration 140200, lr = 1e-20
I0430 03:46:33.326474   966 solver.cpp:228] Iteration 140300, loss = 0.0607855
I0430 03:46:33.326632   966 solver.cpp:244]     Train net output #0: loss = 0.0607855 (* 1 = 0.0607855 loss)
I0430 03:46:33.326640   966 sgd_solver.cpp:106] Iteration 140300, lr = 1e-20
I0430 03:47:22.822379   966 solver.cpp:228] Iteration 140400, loss = 0.487318
I0430 03:47:22.822572   966 solver.cpp:244]     Train net output #0: loss = 0.487318 (* 1 = 0.487318 loss)
I0430 03:47:22.822582   966 sgd_solver.cpp:106] Iteration 140400, lr = 1e-20
I0430 03:48:12.315719   966 solver.cpp:228] Iteration 140500, loss = 0.0984594
I0430 03:48:12.315876   966 solver.cpp:244]     Train net output #0: loss = 0.0984594 (* 1 = 0.0984594 loss)
I0430 03:48:12.315882   966 sgd_solver.cpp:106] Iteration 140500, lr = 1e-20
I0430 03:49:03.132120   966 solver.cpp:228] Iteration 140600, loss = 0.112665
I0430 03:49:03.132289   966 solver.cpp:244]     Train net output #0: loss = 0.112665 (* 1 = 0.112665 loss)
I0430 03:49:03.132297   966 sgd_solver.cpp:106] Iteration 140600, lr = 1e-20
I0430 03:49:52.625926   966 solver.cpp:228] Iteration 140700, loss = 0.13726
I0430 03:49:52.626091   966 solver.cpp:244]     Train net output #0: loss = 0.13726 (* 1 = 0.13726 loss)
I0430 03:49:52.626098   966 sgd_solver.cpp:106] Iteration 140700, lr = 1e-20
I0430 03:50:42.134493   966 solver.cpp:228] Iteration 140800, loss = 0.114301
I0430 03:50:42.134651   966 solver.cpp:244]     Train net output #0: loss = 0.114301 (* 1 = 0.114301 loss)
I0430 03:50:42.134658   966 sgd_solver.cpp:106] Iteration 140800, lr = 1e-20
I0430 03:51:33.020614   966 solver.cpp:228] Iteration 140900, loss = 0.148344
I0430 03:51:33.020766   966 solver.cpp:244]     Train net output #0: loss = 0.148344 (* 1 = 0.148344 loss)
I0430 03:51:33.020772   966 sgd_solver.cpp:106] Iteration 140900, lr = 1e-20
I0430 03:52:22.231791   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_141000.caffemodel
I0430 03:52:30.440810   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_141000.solverstate
I0430 03:52:30.632793   966 solver.cpp:337] Iteration 141000, Testing net (#0)
I0430 03:52:30.632920   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 03:52:30.632941   966 net.cpp:693] Ignoring source layer visualize
I0430 03:52:30.632944   966 net.cpp:693] Ignoring source layer fake
I0430 03:57:22.947509   966 solver.cpp:404]     Test net output #0: loss = 0.180655 (* 1 = 0.180655 loss)
I0430 03:57:23.256253   966 solver.cpp:228] Iteration 141000, loss = 0.0887134
I0430 03:57:23.256292   966 solver.cpp:244]     Train net output #0: loss = 0.0887134 (* 1 = 0.0887134 loss)
I0430 03:57:23.256299   966 sgd_solver.cpp:106] Iteration 141000, lr = 1e-20
I0430 03:58:12.767833   966 solver.cpp:228] Iteration 141100, loss = 0.173387
I0430 03:58:12.768024   966 solver.cpp:244]     Train net output #0: loss = 0.173387 (* 1 = 0.173387 loss)
I0430 03:58:12.768033   966 sgd_solver.cpp:106] Iteration 141100, lr = 1e-20
I0430 03:59:03.855587   966 solver.cpp:228] Iteration 141200, loss = 0.0591249
I0430 03:59:03.855752   966 solver.cpp:244]     Train net output #0: loss = 0.0591249 (* 1 = 0.0591249 loss)
I0430 03:59:03.855759   966 sgd_solver.cpp:106] Iteration 141200, lr = 1e-20
I0430 03:59:53.363665   966 solver.cpp:228] Iteration 141300, loss = 0.169851
I0430 03:59:53.363836   966 solver.cpp:244]     Train net output #0: loss = 0.169851 (* 1 = 0.169851 loss)
I0430 03:59:53.363842   966 sgd_solver.cpp:106] Iteration 141300, lr = 1e-20
I0430 04:00:44.959462   966 solver.cpp:228] Iteration 141400, loss = 0.112919
I0430 04:00:44.959620   966 solver.cpp:244]     Train net output #0: loss = 0.112919 (* 1 = 0.112919 loss)
I0430 04:00:44.959630   966 sgd_solver.cpp:106] Iteration 141400, lr = 1e-20
I0430 04:01:34.455543   966 solver.cpp:228] Iteration 141500, loss = 0.142887
I0430 04:01:34.455701   966 solver.cpp:244]     Train net output #0: loss = 0.142887 (* 1 = 0.142887 loss)
I0430 04:01:34.455708   966 sgd_solver.cpp:106] Iteration 141500, lr = 1e-20
I0430 04:02:23.964095   966 solver.cpp:228] Iteration 141600, loss = 0.118857
I0430 04:02:23.964272   966 solver.cpp:244]     Train net output #0: loss = 0.118857 (* 1 = 0.118857 loss)
I0430 04:02:23.964279   966 sgd_solver.cpp:106] Iteration 141600, lr = 1e-20
I0430 04:03:13.470360   966 solver.cpp:228] Iteration 141700, loss = 0.166512
I0430 04:03:13.470546   966 solver.cpp:244]     Train net output #0: loss = 0.166512 (* 1 = 0.166512 loss)
I0430 04:03:13.470552   966 sgd_solver.cpp:106] Iteration 141700, lr = 1e-20
I0430 04:04:04.913173   966 solver.cpp:228] Iteration 141800, loss = 0.112618
I0430 04:04:04.913331   966 solver.cpp:244]     Train net output #0: loss = 0.112618 (* 1 = 0.112618 loss)
I0430 04:04:04.913338   966 sgd_solver.cpp:106] Iteration 141800, lr = 1e-20
I0430 04:04:54.412631   966 solver.cpp:228] Iteration 141900, loss = 0.0869991
I0430 04:04:54.412798   966 solver.cpp:244]     Train net output #0: loss = 0.0869991 (* 1 = 0.0869991 loss)
I0430 04:04:54.412804   966 sgd_solver.cpp:106] Iteration 141900, lr = 1e-20
I0430 04:05:43.622615   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_142000.caffemodel
I0430 04:05:52.184540   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_142000.solverstate
I0430 04:05:52.379017   966 solver.cpp:337] Iteration 142000, Testing net (#0)
I0430 04:05:52.379137   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 04:05:52.379145   966 net.cpp:693] Ignoring source layer visualize
I0430 04:05:52.379148   966 net.cpp:693] Ignoring source layer fake
I0430 04:10:45.472286   966 solver.cpp:404]     Test net output #0: loss = 0.192018 (* 1 = 0.192018 loss)
I0430 04:10:45.782407   966 solver.cpp:228] Iteration 142000, loss = 0.136505
I0430 04:10:45.782445   966 solver.cpp:244]     Train net output #0: loss = 0.136505 (* 1 = 0.136505 loss)
I0430 04:10:45.782451   966 sgd_solver.cpp:106] Iteration 142000, lr = 1e-20
I0430 04:11:37.471807   966 solver.cpp:228] Iteration 142100, loss = 0.191873
I0430 04:11:37.471976   966 solver.cpp:244]     Train net output #0: loss = 0.191873 (* 1 = 0.191873 loss)
I0430 04:11:37.471983   966 sgd_solver.cpp:106] Iteration 142100, lr = 1e-20
I0430 04:12:26.984917   966 solver.cpp:228] Iteration 142200, loss = 0.142539
I0430 04:12:26.985127   966 solver.cpp:244]     Train net output #0: loss = 0.142539 (* 1 = 0.142539 loss)
I0430 04:12:26.985136   966 sgd_solver.cpp:106] Iteration 142200, lr = 1e-20
I0430 04:13:16.504398   966 solver.cpp:228] Iteration 142300, loss = 0.168381
I0430 04:13:16.504559   966 solver.cpp:244]     Train net output #0: loss = 0.168381 (* 1 = 0.168381 loss)
I0430 04:13:16.504565   966 sgd_solver.cpp:106] Iteration 142300, lr = 1e-20
I0430 04:14:06.014617   966 solver.cpp:228] Iteration 142400, loss = 0.115766
I0430 04:14:06.014780   966 solver.cpp:244]     Train net output #0: loss = 0.115766 (* 1 = 0.115766 loss)
I0430 04:14:06.014788   966 sgd_solver.cpp:106] Iteration 142400, lr = 1e-20
I0430 04:14:57.423753   966 solver.cpp:228] Iteration 142500, loss = 0.272044
I0430 04:14:57.423910   966 solver.cpp:244]     Train net output #0: loss = 0.272044 (* 1 = 0.272044 loss)
I0430 04:14:57.423918   966 sgd_solver.cpp:106] Iteration 142500, lr = 1e-21
I0430 04:15:46.930642   966 solver.cpp:228] Iteration 142600, loss = 0.121123
I0430 04:15:46.930822   966 solver.cpp:244]     Train net output #0: loss = 0.121123 (* 1 = 0.121123 loss)
I0430 04:15:46.930831   966 sgd_solver.cpp:106] Iteration 142600, lr = 1e-21
I0430 04:16:36.465574   966 solver.cpp:228] Iteration 142700, loss = 0.287057
I0430 04:16:36.465733   966 solver.cpp:244]     Train net output #0: loss = 0.287057 (* 1 = 0.287057 loss)
I0430 04:16:36.465739   966 sgd_solver.cpp:106] Iteration 142700, lr = 1e-21
I0430 04:17:27.831410   966 solver.cpp:228] Iteration 142800, loss = 0.153599
I0430 04:17:27.831565   966 solver.cpp:244]     Train net output #0: loss = 0.153599 (* 1 = 0.153599 loss)
I0430 04:17:27.831573   966 sgd_solver.cpp:106] Iteration 142800, lr = 1e-21
I0430 04:18:17.335367   966 solver.cpp:228] Iteration 142900, loss = 0.102034
I0430 04:18:17.335546   966 solver.cpp:244]     Train net output #0: loss = 0.102034 (* 1 = 0.102034 loss)
I0430 04:18:17.335554   966 sgd_solver.cpp:106] Iteration 142900, lr = 1e-21
I0430 04:19:06.541582   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_143000.caffemodel
I0430 04:19:23.947819   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_143000.solverstate
I0430 04:19:24.150468   966 solver.cpp:337] Iteration 143000, Testing net (#0)
I0430 04:19:24.150594   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 04:19:24.150600   966 net.cpp:693] Ignoring source layer visualize
I0430 04:19:24.150602   966 net.cpp:693] Ignoring source layer fake
I0430 04:24:17.245069   966 solver.cpp:404]     Test net output #0: loss = 0.186006 (* 1 = 0.186006 loss)
I0430 04:24:17.554957   966 solver.cpp:228] Iteration 143000, loss = 0.107514
I0430 04:24:17.554994   966 solver.cpp:244]     Train net output #0: loss = 0.107514 (* 1 = 0.107514 loss)
I0430 04:24:17.555001   966 sgd_solver.cpp:106] Iteration 143000, lr = 1e-21
I0430 04:25:08.821954   966 solver.cpp:228] Iteration 143100, loss = 0.0611247
I0430 04:25:08.822113   966 solver.cpp:244]     Train net output #0: loss = 0.0611247 (* 1 = 0.0611247 loss)
I0430 04:25:08.822119   966 sgd_solver.cpp:106] Iteration 143100, lr = 1e-21
I0430 04:25:58.311555   966 solver.cpp:228] Iteration 143200, loss = 0.122496
I0430 04:25:58.311713   966 solver.cpp:244]     Train net output #0: loss = 0.122496 (* 1 = 0.122496 loss)
I0430 04:25:58.311722   966 sgd_solver.cpp:106] Iteration 143200, lr = 1e-21
I0430 04:26:47.809938   966 solver.cpp:228] Iteration 143300, loss = 0.157055
I0430 04:26:47.810098   966 solver.cpp:244]     Train net output #0: loss = 0.157055 (* 1 = 0.157055 loss)
I0430 04:26:47.810106   966 sgd_solver.cpp:106] Iteration 143300, lr = 1e-21
I0430 04:27:37.320708   966 solver.cpp:228] Iteration 143400, loss = 0.214654
I0430 04:27:37.320873   966 solver.cpp:244]     Train net output #0: loss = 0.214654 (* 1 = 0.214654 loss)
I0430 04:27:37.320879   966 sgd_solver.cpp:106] Iteration 143400, lr = 1e-21
I0430 04:28:28.444074   966 solver.cpp:228] Iteration 143500, loss = 0.125009
I0430 04:28:28.444232   966 solver.cpp:244]     Train net output #0: loss = 0.125009 (* 1 = 0.125009 loss)
I0430 04:28:28.444239   966 sgd_solver.cpp:106] Iteration 143500, lr = 1e-21
I0430 04:29:17.933379   966 solver.cpp:228] Iteration 143600, loss = 0.493348
I0430 04:29:17.934265   966 solver.cpp:244]     Train net output #0: loss = 0.493348 (* 1 = 0.493348 loss)
I0430 04:29:17.934288   966 sgd_solver.cpp:106] Iteration 143600, lr = 1e-21
I0430 04:30:07.428071   966 solver.cpp:228] Iteration 143700, loss = 0.143858
I0430 04:30:07.428244   966 solver.cpp:244]     Train net output #0: loss = 0.143858 (* 1 = 0.143858 loss)
I0430 04:30:07.428251   966 sgd_solver.cpp:106] Iteration 143700, lr = 1e-21
I0430 04:30:58.217502   966 solver.cpp:228] Iteration 143800, loss = 0.107353
I0430 04:30:58.217658   966 solver.cpp:244]     Train net output #0: loss = 0.107353 (* 1 = 0.107353 loss)
I0430 04:30:58.217664   966 sgd_solver.cpp:106] Iteration 143800, lr = 1e-21
I0430 04:31:47.720794   966 solver.cpp:228] Iteration 143900, loss = 0.101445
I0430 04:31:47.720947   966 solver.cpp:244]     Train net output #0: loss = 0.101445 (* 1 = 0.101445 loss)
I0430 04:31:47.720954   966 sgd_solver.cpp:106] Iteration 143900, lr = 1e-21
I0430 04:32:38.249264   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_144000.caffemodel
I0430 04:32:57.900455   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_144000.solverstate
I0430 04:32:58.162319   966 solver.cpp:337] Iteration 144000, Testing net (#0)
I0430 04:32:58.162422   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 04:32:58.162428   966 net.cpp:693] Ignoring source layer visualize
I0430 04:32:58.162431   966 net.cpp:693] Ignoring source layer fake
I0430 04:37:50.349988   966 solver.cpp:404]     Test net output #0: loss = 0.182048 (* 1 = 0.182048 loss)
I0430 04:37:50.660359   966 solver.cpp:228] Iteration 144000, loss = 0.187613
I0430 04:37:50.660377   966 solver.cpp:244]     Train net output #0: loss = 0.187613 (* 1 = 0.187613 loss)
I0430 04:37:50.660398   966 sgd_solver.cpp:106] Iteration 144000, lr = 1e-21
I0430 04:38:40.170882   966 solver.cpp:228] Iteration 144100, loss = 0.120951
I0430 04:38:40.171052   966 solver.cpp:244]     Train net output #0: loss = 0.120951 (* 1 = 0.120951 loss)
I0430 04:38:40.171070   966 sgd_solver.cpp:106] Iteration 144100, lr = 1e-21
I0430 04:39:29.680982   966 solver.cpp:228] Iteration 144200, loss = 0.316251
I0430 04:39:29.681146   966 solver.cpp:244]     Train net output #0: loss = 0.316251 (* 1 = 0.316251 loss)
I0430 04:39:29.681154   966 sgd_solver.cpp:106] Iteration 144200, lr = 1e-21
I0430 04:40:20.927503   966 solver.cpp:228] Iteration 144300, loss = 0.290118
I0430 04:40:20.927682   966 solver.cpp:244]     Train net output #0: loss = 0.290118 (* 1 = 0.290118 loss)
I0430 04:40:20.927690   966 sgd_solver.cpp:106] Iteration 144300, lr = 1e-21
I0430 04:41:10.427248   966 solver.cpp:228] Iteration 144400, loss = 0.134094
I0430 04:41:10.427418   966 solver.cpp:244]     Train net output #0: loss = 0.134094 (* 1 = 0.134094 loss)
I0430 04:41:10.427424   966 sgd_solver.cpp:106] Iteration 144400, lr = 1e-21
I0430 04:41:59.938217   966 solver.cpp:228] Iteration 144500, loss = 0.107536
I0430 04:41:59.938364   966 solver.cpp:244]     Train net output #0: loss = 0.107536 (* 1 = 0.107536 loss)
I0430 04:41:59.938370   966 sgd_solver.cpp:106] Iteration 144500, lr = 1e-21
I0430 04:42:51.552541   966 solver.cpp:228] Iteration 144600, loss = 0.0907826
I0430 04:42:51.552693   966 solver.cpp:244]     Train net output #0: loss = 0.0907826 (* 1 = 0.0907826 loss)
I0430 04:42:51.552700   966 sgd_solver.cpp:106] Iteration 144600, lr = 1e-21
I0430 04:43:41.057440   966 solver.cpp:228] Iteration 144700, loss = 0.12493
I0430 04:43:41.057572   966 solver.cpp:244]     Train net output #0: loss = 0.12493 (* 1 = 0.12493 loss)
I0430 04:43:41.057579   966 sgd_solver.cpp:106] Iteration 144700, lr = 1e-21
I0430 04:44:30.575440   966 solver.cpp:228] Iteration 144800, loss = 0.131982
I0430 04:44:30.575608   966 solver.cpp:244]     Train net output #0: loss = 0.131982 (* 1 = 0.131982 loss)
I0430 04:44:30.575614   966 sgd_solver.cpp:106] Iteration 144800, lr = 1e-21
I0430 04:45:22.020366   966 solver.cpp:228] Iteration 144900, loss = 0.117453
I0430 04:45:22.020525   966 solver.cpp:244]     Train net output #0: loss = 0.117453 (* 1 = 0.117453 loss)
I0430 04:45:22.020532   966 sgd_solver.cpp:106] Iteration 144900, lr = 1e-21
I0430 04:46:11.227547   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_145000.caffemodel
I0430 04:46:21.391903   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_145000.solverstate
I0430 04:46:21.580658   966 solver.cpp:337] Iteration 145000, Testing net (#0)
I0430 04:46:21.580783   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 04:46:21.580806   966 net.cpp:693] Ignoring source layer visualize
I0430 04:46:21.580808   966 net.cpp:693] Ignoring source layer fake
I0430 04:51:14.250689   966 solver.cpp:404]     Test net output #0: loss = 0.180163 (* 1 = 0.180163 loss)
I0430 04:51:14.561902   966 solver.cpp:228] Iteration 145000, loss = 0.14055
I0430 04:51:14.561938   966 solver.cpp:244]     Train net output #0: loss = 0.14055 (* 1 = 0.14055 loss)
I0430 04:51:14.561945   966 sgd_solver.cpp:106] Iteration 145000, lr = 1e-21
I0430 04:52:04.092301   966 solver.cpp:228] Iteration 145100, loss = 0.136672
I0430 04:52:04.092530   966 solver.cpp:244]     Train net output #0: loss = 0.136672 (* 1 = 0.136672 loss)
I0430 04:52:04.092538   966 sgd_solver.cpp:106] Iteration 145100, lr = 1e-21
I0430 04:52:55.532817   966 solver.cpp:228] Iteration 145200, loss = 0.182731
I0430 04:52:55.533004   966 solver.cpp:244]     Train net output #0: loss = 0.182731 (* 1 = 0.182731 loss)
I0430 04:52:55.533015   966 sgd_solver.cpp:106] Iteration 145200, lr = 1e-21
I0430 04:53:45.034535   966 solver.cpp:228] Iteration 145300, loss = 0.196884
I0430 04:53:45.034704   966 solver.cpp:244]     Train net output #0: loss = 0.196884 (* 1 = 0.196884 loss)
I0430 04:53:45.034711   966 sgd_solver.cpp:106] Iteration 145300, lr = 1e-21
I0430 04:54:34.555308   966 solver.cpp:228] Iteration 145400, loss = 0.0858822
I0430 04:54:34.555459   966 solver.cpp:244]     Train net output #0: loss = 0.0858822 (* 1 = 0.0858822 loss)
I0430 04:54:34.555465   966 sgd_solver.cpp:106] Iteration 145400, lr = 1e-21
I0430 04:55:24.071259   966 solver.cpp:228] Iteration 145500, loss = 0.153433
I0430 04:55:24.071413   966 solver.cpp:244]     Train net output #0: loss = 0.153433 (* 1 = 0.153433 loss)
I0430 04:55:24.071419   966 sgd_solver.cpp:106] Iteration 145500, lr = 1e-21
I0430 04:56:15.647392   966 solver.cpp:228] Iteration 145600, loss = 0.120942
I0430 04:56:15.647562   966 solver.cpp:244]     Train net output #0: loss = 0.120942 (* 1 = 0.120942 loss)
I0430 04:56:15.647572   966 sgd_solver.cpp:106] Iteration 145600, lr = 1e-21
I0430 04:57:05.146378   966 solver.cpp:228] Iteration 145700, loss = 0.106719
I0430 04:57:05.146534   966 solver.cpp:244]     Train net output #0: loss = 0.106719 (* 1 = 0.106719 loss)
I0430 04:57:05.146543   966 sgd_solver.cpp:106] Iteration 145700, lr = 1e-21
I0430 04:57:54.679257   966 solver.cpp:228] Iteration 145800, loss = 0.191256
I0430 04:57:54.679407   966 solver.cpp:244]     Train net output #0: loss = 0.191256 (* 1 = 0.191256 loss)
I0430 04:57:54.679414   966 sgd_solver.cpp:106] Iteration 145800, lr = 1e-21
I0430 04:58:46.203966   966 solver.cpp:228] Iteration 145900, loss = 0.0879702
I0430 04:58:46.204129   966 solver.cpp:244]     Train net output #0: loss = 0.0879702 (* 1 = 0.0879702 loss)
I0430 04:58:46.204136   966 sgd_solver.cpp:106] Iteration 145900, lr = 1e-21
I0430 04:59:35.388815   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_146000.caffemodel
I0430 04:59:48.892510   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_146000.solverstate
I0430 04:59:49.086702   966 solver.cpp:337] Iteration 146000, Testing net (#0)
I0430 04:59:49.086825   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 04:59:49.086833   966 net.cpp:693] Ignoring source layer visualize
I0430 04:59:49.086834   966 net.cpp:693] Ignoring source layer fake
I0430 05:04:41.492691   966 solver.cpp:404]     Test net output #0: loss = 0.181513 (* 1 = 0.181513 loss)
I0430 05:04:41.801499   966 solver.cpp:228] Iteration 146000, loss = 0.131167
I0430 05:04:41.801518   966 solver.cpp:244]     Train net output #0: loss = 0.131167 (* 1 = 0.131167 loss)
I0430 05:04:41.801542   966 sgd_solver.cpp:106] Iteration 146000, lr = 1e-21
I0430 05:05:31.308715   966 solver.cpp:228] Iteration 146100, loss = 0.101673
I0430 05:05:31.308884   966 solver.cpp:244]     Train net output #0: loss = 0.101673 (* 1 = 0.101673 loss)
I0430 05:05:31.308892   966 sgd_solver.cpp:106] Iteration 146100, lr = 1e-21
I0430 05:06:20.806309   966 solver.cpp:228] Iteration 146200, loss = 0.0831518
I0430 05:06:20.806483   966 solver.cpp:244]     Train net output #0: loss = 0.0831518 (* 1 = 0.0831518 loss)
I0430 05:06:20.806489   966 sgd_solver.cpp:106] Iteration 146200, lr = 1e-21
I0430 05:07:12.094926   966 solver.cpp:228] Iteration 146300, loss = 0.0651279
I0430 05:07:12.095096   966 solver.cpp:244]     Train net output #0: loss = 0.0651279 (* 1 = 0.0651279 loss)
I0430 05:07:12.095104   966 sgd_solver.cpp:106] Iteration 146300, lr = 1e-21
I0430 05:08:01.591313   966 solver.cpp:228] Iteration 146400, loss = 0.0958764
I0430 05:08:01.591475   966 solver.cpp:244]     Train net output #0: loss = 0.0958764 (* 1 = 0.0958764 loss)
I0430 05:08:01.591481   966 sgd_solver.cpp:106] Iteration 146400, lr = 1e-21
I0430 05:08:51.090554   966 solver.cpp:228] Iteration 146500, loss = 0.0804245
I0430 05:08:51.090680   966 solver.cpp:244]     Train net output #0: loss = 0.0804245 (* 1 = 0.0804245 loss)
I0430 05:08:51.090687   966 sgd_solver.cpp:106] Iteration 146500, lr = 1e-21
I0430 05:09:42.237467   966 solver.cpp:228] Iteration 146600, loss = 0.139034
I0430 05:09:42.237617   966 solver.cpp:244]     Train net output #0: loss = 0.139034 (* 1 = 0.139034 loss)
I0430 05:09:42.237624   966 sgd_solver.cpp:106] Iteration 146600, lr = 1e-21
I0430 05:10:31.739138   966 solver.cpp:228] Iteration 146700, loss = 0.0944411
I0430 05:10:31.739312   966 solver.cpp:244]     Train net output #0: loss = 0.0944411 (* 1 = 0.0944411 loss)
I0430 05:10:31.739320   966 sgd_solver.cpp:106] Iteration 146700, lr = 1e-21
I0430 05:11:21.229969   966 solver.cpp:228] Iteration 146800, loss = 0.383799
I0430 05:11:21.230144   966 solver.cpp:244]     Train net output #0: loss = 0.383799 (* 1 = 0.383799 loss)
I0430 05:11:21.230150   966 sgd_solver.cpp:106] Iteration 146800, lr = 1e-21
I0430 05:12:12.029328   966 solver.cpp:228] Iteration 146900, loss = 0.129658
I0430 05:12:12.029500   966 solver.cpp:244]     Train net output #0: loss = 0.129658 (* 1 = 0.129658 loss)
I0430 05:12:12.029507   966 sgd_solver.cpp:106] Iteration 146900, lr = 1e-21
I0430 05:13:01.218822   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_147000.caffemodel
I0430 05:13:14.976613   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_147000.solverstate
I0430 05:13:15.286916   966 solver.cpp:337] Iteration 147000, Testing net (#0)
I0430 05:13:15.287060   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 05:13:15.287067   966 net.cpp:693] Ignoring source layer visualize
I0430 05:13:15.287070   966 net.cpp:693] Ignoring source layer fake
I0430 05:18:07.786134   966 solver.cpp:404]     Test net output #0: loss = 0.191447 (* 1 = 0.191447 loss)
I0430 05:18:08.097676   966 solver.cpp:228] Iteration 147000, loss = 0.099351
I0430 05:18:08.097718   966 solver.cpp:244]     Train net output #0: loss = 0.099351 (* 1 = 0.099351 loss)
I0430 05:18:08.097725   966 sgd_solver.cpp:106] Iteration 147000, lr = 1e-21
I0430 05:18:58.984220   966 solver.cpp:228] Iteration 147100, loss = 0.409288
I0430 05:18:58.984408   966 solver.cpp:244]     Train net output #0: loss = 0.409288 (* 1 = 0.409288 loss)
I0430 05:18:58.984417   966 sgd_solver.cpp:106] Iteration 147100, lr = 1e-21
I0430 05:19:48.498452   966 solver.cpp:228] Iteration 147200, loss = 0.312833
I0430 05:19:48.498615   966 solver.cpp:244]     Train net output #0: loss = 0.312833 (* 1 = 0.312833 loss)
I0430 05:19:48.498622   966 sgd_solver.cpp:106] Iteration 147200, lr = 1e-21
I0430 05:20:38.018642   966 solver.cpp:228] Iteration 147300, loss = 0.149557
I0430 05:20:38.018816   966 solver.cpp:244]     Train net output #0: loss = 0.149557 (* 1 = 0.149557 loss)
I0430 05:20:38.018823   966 sgd_solver.cpp:106] Iteration 147300, lr = 1e-21
I0430 05:21:29.278275   966 solver.cpp:228] Iteration 147400, loss = 0.0773551
I0430 05:21:29.278427   966 solver.cpp:244]     Train net output #0: loss = 0.0773551 (* 1 = 0.0773551 loss)
I0430 05:21:29.278434   966 sgd_solver.cpp:106] Iteration 147400, lr = 1e-21
I0430 05:22:18.789324   966 solver.cpp:228] Iteration 147500, loss = 0.125375
I0430 05:22:18.790587   966 solver.cpp:244]     Train net output #0: loss = 0.125375 (* 1 = 0.125375 loss)
I0430 05:22:18.790593   966 sgd_solver.cpp:106] Iteration 147500, lr = 1e-21
I0430 05:23:08.296645   966 solver.cpp:228] Iteration 147600, loss = 0.150869
I0430 05:23:08.296799   966 solver.cpp:244]     Train net output #0: loss = 0.150869 (* 1 = 0.150869 loss)
I0430 05:23:08.296808   966 sgd_solver.cpp:106] Iteration 147600, lr = 1e-21
I0430 05:23:59.740388   966 solver.cpp:228] Iteration 147700, loss = 0.21876
I0430 05:23:59.740586   966 solver.cpp:244]     Train net output #0: loss = 0.21876 (* 1 = 0.21876 loss)
I0430 05:23:59.740595   966 sgd_solver.cpp:106] Iteration 147700, lr = 1e-21
I0430 05:24:49.244895   966 solver.cpp:228] Iteration 147800, loss = 0.0513246
I0430 05:24:49.245098   966 solver.cpp:244]     Train net output #0: loss = 0.0513246 (* 1 = 0.0513246 loss)
I0430 05:24:49.245116   966 sgd_solver.cpp:106] Iteration 147800, lr = 1e-21
I0430 05:25:38.753917   966 solver.cpp:228] Iteration 147900, loss = 0.196629
I0430 05:25:38.754073   966 solver.cpp:244]     Train net output #0: loss = 0.196629 (* 1 = 0.196629 loss)
I0430 05:25:38.754081   966 sgd_solver.cpp:106] Iteration 147900, lr = 1e-21
I0430 05:26:29.818476   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_148000.caffemodel
I0430 05:26:56.582602   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_148000.solverstate
I0430 05:26:56.770844   966 solver.cpp:337] Iteration 148000, Testing net (#0)
I0430 05:26:56.770967   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 05:26:56.770972   966 net.cpp:693] Ignoring source layer visualize
I0430 05:26:56.770974   966 net.cpp:693] Ignoring source layer fake
I0430 05:31:49.101461   966 solver.cpp:404]     Test net output #0: loss = 0.185859 (* 1 = 0.185859 loss)
I0430 05:31:49.410277   966 solver.cpp:228] Iteration 148000, loss = 0.112568
I0430 05:31:49.410296   966 solver.cpp:244]     Train net output #0: loss = 0.112568 (* 1 = 0.112568 loss)
I0430 05:31:49.410318   966 sgd_solver.cpp:106] Iteration 148000, lr = 1e-21
I0430 05:32:38.927378   966 solver.cpp:228] Iteration 148100, loss = 0.0895795
I0430 05:32:38.927541   966 solver.cpp:244]     Train net output #0: loss = 0.0895795 (* 1 = 0.0895795 loss)
I0430 05:32:38.927547   966 sgd_solver.cpp:106] Iteration 148100, lr = 1e-21
I0430 05:33:28.437335   966 solver.cpp:228] Iteration 148200, loss = 0.219858
I0430 05:33:28.437520   966 solver.cpp:244]     Train net output #0: loss = 0.219858 (* 1 = 0.219858 loss)
I0430 05:33:28.437527   966 sgd_solver.cpp:106] Iteration 148200, lr = 1e-21
I0430 05:34:17.951259   966 solver.cpp:228] Iteration 148300, loss = 0.125344
I0430 05:34:17.951433   966 solver.cpp:244]     Train net output #0: loss = 0.125344 (* 1 = 0.125344 loss)
I0430 05:34:17.951442   966 sgd_solver.cpp:106] Iteration 148300, lr = 1e-21
I0430 05:35:09.313056   966 solver.cpp:228] Iteration 148400, loss = 0.228945
I0430 05:35:09.313252   966 solver.cpp:244]     Train net output #0: loss = 0.228945 (* 1 = 0.228945 loss)
I0430 05:35:09.313261   966 sgd_solver.cpp:106] Iteration 148400, lr = 1e-21
I0430 05:35:58.813503   966 solver.cpp:228] Iteration 148500, loss = 0.0880744
I0430 05:35:58.813678   966 solver.cpp:244]     Train net output #0: loss = 0.0880744 (* 1 = 0.0880744 loss)
I0430 05:35:58.813686   966 sgd_solver.cpp:106] Iteration 148500, lr = 1e-21
I0430 05:36:48.318590   966 solver.cpp:228] Iteration 148600, loss = 0.209866
I0430 05:36:48.318740   966 solver.cpp:244]     Train net output #0: loss = 0.209866 (* 1 = 0.209866 loss)
I0430 05:36:48.318747   966 sgd_solver.cpp:106] Iteration 148600, lr = 1e-21
I0430 05:37:39.632453   966 solver.cpp:228] Iteration 148700, loss = 0.120282
I0430 05:37:39.632601   966 solver.cpp:244]     Train net output #0: loss = 0.120282 (* 1 = 0.120282 loss)
I0430 05:37:39.632609   966 sgd_solver.cpp:106] Iteration 148700, lr = 1e-21
I0430 05:38:29.133851   966 solver.cpp:228] Iteration 148800, loss = 0.131456
I0430 05:38:29.134052   966 solver.cpp:244]     Train net output #0: loss = 0.131456 (* 1 = 0.131456 loss)
I0430 05:38:29.134058   966 sgd_solver.cpp:106] Iteration 148800, lr = 1e-21
I0430 05:39:18.640231   966 solver.cpp:228] Iteration 148900, loss = 0.31799
I0430 05:39:18.640398   966 solver.cpp:244]     Train net output #0: loss = 0.31799 (* 1 = 0.31799 loss)
I0430 05:39:18.640405   966 sgd_solver.cpp:106] Iteration 148900, lr = 1e-21
I0430 05:40:07.824522   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_149000.caffemodel
I0430 05:40:58.243211   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_149000.solverstate
I0430 05:40:58.448608   966 solver.cpp:337] Iteration 149000, Testing net (#0)
I0430 05:40:58.448743   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 05:40:58.448750   966 net.cpp:693] Ignoring source layer visualize
I0430 05:40:58.448753   966 net.cpp:693] Ignoring source layer fake
I0430 05:45:50.276623   966 solver.cpp:404]     Test net output #0: loss = 0.182376 (* 1 = 0.182376 loss)
I0430 05:45:50.585644   966 solver.cpp:228] Iteration 149000, loss = 0.110371
I0430 05:45:50.585676   966 solver.cpp:244]     Train net output #0: loss = 0.110371 (* 1 = 0.110371 loss)
I0430 05:45:50.585685   966 sgd_solver.cpp:106] Iteration 149000, lr = 1e-21
I0430 05:46:41.863198   966 solver.cpp:228] Iteration 149100, loss = 0.164866
I0430 05:46:41.863358   966 solver.cpp:244]     Train net output #0: loss = 0.164866 (* 1 = 0.164866 loss)
I0430 05:46:41.863364   966 sgd_solver.cpp:106] Iteration 149100, lr = 1e-21
I0430 05:47:31.350106   966 solver.cpp:228] Iteration 149200, loss = 0.101027
I0430 05:47:31.350256   966 solver.cpp:244]     Train net output #0: loss = 0.101027 (* 1 = 0.101027 loss)
I0430 05:47:31.350263   966 sgd_solver.cpp:106] Iteration 149200, lr = 1e-21
I0430 05:48:20.846349   966 solver.cpp:228] Iteration 149300, loss = 0.0700792
I0430 05:48:20.846526   966 solver.cpp:244]     Train net output #0: loss = 0.0700792 (* 1 = 0.0700792 loss)
I0430 05:48:20.846534   966 sgd_solver.cpp:106] Iteration 149300, lr = 1e-21
I0430 05:49:12.109235   966 solver.cpp:228] Iteration 149400, loss = 0.140347
I0430 05:49:12.109385   966 solver.cpp:244]     Train net output #0: loss = 0.140347 (* 1 = 0.140347 loss)
I0430 05:49:12.109392   966 sgd_solver.cpp:106] Iteration 149400, lr = 1e-21
I0430 05:50:01.602612   966 solver.cpp:228] Iteration 149500, loss = 0.283692
I0430 05:50:01.602762   966 solver.cpp:244]     Train net output #0: loss = 0.283692 (* 1 = 0.283692 loss)
I0430 05:50:01.602768   966 sgd_solver.cpp:106] Iteration 149500, lr = 1e-21
I0430 05:50:51.097138   966 solver.cpp:228] Iteration 149600, loss = 0.1157
I0430 05:50:51.097278   966 solver.cpp:244]     Train net output #0: loss = 0.1157 (* 1 = 0.1157 loss)
I0430 05:50:51.097285   966 sgd_solver.cpp:106] Iteration 149600, lr = 1e-21
I0430 05:51:42.232661   966 solver.cpp:228] Iteration 149700, loss = 0.0917775
I0430 05:51:42.232787   966 solver.cpp:244]     Train net output #0: loss = 0.0917775 (* 1 = 0.0917775 loss)
I0430 05:51:42.232795   966 sgd_solver.cpp:106] Iteration 149700, lr = 1e-21
I0430 05:52:31.722354   966 solver.cpp:228] Iteration 149800, loss = 0.140006
I0430 05:52:31.722496   966 solver.cpp:244]     Train net output #0: loss = 0.140006 (* 1 = 0.140006 loss)
I0430 05:52:31.722504   966 sgd_solver.cpp:106] Iteration 149800, lr = 1e-21
I0430 05:53:21.214072   966 solver.cpp:228] Iteration 149900, loss = 0.128761
I0430 05:53:21.214236   966 solver.cpp:244]     Train net output #0: loss = 0.128761 (* 1 = 0.128761 loss)
I0430 05:53:21.214242   966 sgd_solver.cpp:106] Iteration 149900, lr = 1e-21
I0430 05:54:11.646090   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_150000.caffemodel
I0430 05:55:01.887326   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_150000.solverstate
I0430 05:55:02.233685   966 solver.cpp:337] Iteration 150000, Testing net (#0)
I0430 05:55:02.233814   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 05:55:02.233832   966 net.cpp:693] Ignoring source layer visualize
I0430 05:55:02.233835   966 net.cpp:693] Ignoring source layer fake
I0430 05:59:54.363828   966 solver.cpp:404]     Test net output #0: loss = 0.179913 (* 1 = 0.179913 loss)
I0430 05:59:54.674316   966 solver.cpp:228] Iteration 150000, loss = 0.103438
I0430 05:59:54.674336   966 solver.cpp:244]     Train net output #0: loss = 0.103438 (* 1 = 0.103438 loss)
I0430 05:59:54.674358   966 sgd_solver.cpp:106] Iteration 150000, lr = 1e-22
I0430 06:00:44.311344   966 solver.cpp:228] Iteration 150100, loss = 0.0910826
I0430 06:00:44.311509   966 solver.cpp:244]     Train net output #0: loss = 0.0910826 (* 1 = 0.0910826 loss)
I0430 06:00:44.311517   966 sgd_solver.cpp:106] Iteration 150100, lr = 1e-22
I0430 06:01:33.930987   966 solver.cpp:228] Iteration 150200, loss = 0.139485
I0430 06:01:33.931160   966 solver.cpp:244]     Train net output #0: loss = 0.139485 (* 1 = 0.139485 loss)
I0430 06:01:33.931167   966 sgd_solver.cpp:106] Iteration 150200, lr = 1e-22
I0430 06:02:24.904335   966 solver.cpp:228] Iteration 150300, loss = 0.25671
I0430 06:02:24.904497   966 solver.cpp:244]     Train net output #0: loss = 0.25671 (* 1 = 0.25671 loss)
I0430 06:02:24.904505   966 sgd_solver.cpp:106] Iteration 150300, lr = 1e-22
I0430 06:03:14.530074   966 solver.cpp:228] Iteration 150400, loss = 0.128751
I0430 06:03:14.530222   966 solver.cpp:244]     Train net output #0: loss = 0.128751 (* 1 = 0.128751 loss)
I0430 06:03:14.530230   966 sgd_solver.cpp:106] Iteration 150400, lr = 1e-22
I0430 06:04:05.807006   966 solver.cpp:228] Iteration 150500, loss = 0.0549477
I0430 06:04:05.807165   966 solver.cpp:244]     Train net output #0: loss = 0.0549477 (* 1 = 0.0549477 loss)
I0430 06:04:05.807173   966 sgd_solver.cpp:106] Iteration 150500, lr = 1e-22
I0430 06:04:55.435721   966 solver.cpp:228] Iteration 150600, loss = 0.129035
I0430 06:04:55.435885   966 solver.cpp:244]     Train net output #0: loss = 0.129035 (* 1 = 0.129035 loss)
I0430 06:04:55.435894   966 sgd_solver.cpp:106] Iteration 150600, lr = 1e-22
I0430 06:05:45.067160   966 solver.cpp:228] Iteration 150700, loss = 0.0933854
I0430 06:05:45.067525   966 solver.cpp:244]     Train net output #0: loss = 0.0933854 (* 1 = 0.0933854 loss)
I0430 06:05:45.067533   966 sgd_solver.cpp:106] Iteration 150700, lr = 1e-22
I0430 06:06:36.523459   966 solver.cpp:228] Iteration 150800, loss = 0.196469
I0430 06:06:36.523627   966 solver.cpp:244]     Train net output #0: loss = 0.196469 (* 1 = 0.196469 loss)
I0430 06:06:36.523634   966 sgd_solver.cpp:106] Iteration 150800, lr = 1e-22
I0430 06:07:26.149386   966 solver.cpp:228] Iteration 150900, loss = 0.1498
I0430 06:07:26.149543   966 solver.cpp:244]     Train net output #0: loss = 0.1498 (* 1 = 0.1498 loss)
I0430 06:07:26.149561   966 sgd_solver.cpp:106] Iteration 150900, lr = 1e-22
I0430 06:08:15.462657   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_151000.caffemodel
I0430 06:08:30.230084   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_151000.solverstate
I0430 06:08:30.422073   966 solver.cpp:337] Iteration 151000, Testing net (#0)
I0430 06:08:30.422199   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 06:08:30.422205   966 net.cpp:693] Ignoring source layer visualize
I0430 06:08:30.422207   966 net.cpp:693] Ignoring source layer fake
I0430 06:13:23.523180   966 solver.cpp:404]     Test net output #0: loss = 0.182143 (* 1 = 0.182143 loss)
I0430 06:13:23.833629   966 solver.cpp:228] Iteration 151000, loss = 0.135121
I0430 06:13:23.833655   966 solver.cpp:244]     Train net output #0: loss = 0.135121 (* 1 = 0.135121 loss)
I0430 06:13:23.833664   966 sgd_solver.cpp:106] Iteration 151000, lr = 1e-22
I0430 06:14:15.338805   966 solver.cpp:228] Iteration 151100, loss = 0.150566
I0430 06:14:15.338994   966 solver.cpp:244]     Train net output #0: loss = 0.150566 (* 1 = 0.150566 loss)
I0430 06:14:15.339002   966 sgd_solver.cpp:106] Iteration 151100, lr = 1e-22
I0430 06:15:04.972846   966 solver.cpp:228] Iteration 151200, loss = 0.145664
I0430 06:15:04.973026   966 solver.cpp:244]     Train net output #0: loss = 0.145664 (* 1 = 0.145664 loss)
I0430 06:15:04.973032   966 sgd_solver.cpp:106] Iteration 151200, lr = 1e-22
I0430 06:15:54.611703   966 solver.cpp:228] Iteration 151300, loss = 0.159055
I0430 06:15:54.611873   966 solver.cpp:244]     Train net output #0: loss = 0.159055 (* 1 = 0.159055 loss)
I0430 06:15:54.611881   966 sgd_solver.cpp:106] Iteration 151300, lr = 1e-22
I0430 06:16:44.249729   966 solver.cpp:228] Iteration 151400, loss = 0.184152
I0430 06:16:44.249881   966 solver.cpp:244]     Train net output #0: loss = 0.184152 (* 1 = 0.184152 loss)
I0430 06:16:44.249886   966 sgd_solver.cpp:106] Iteration 151400, lr = 1e-22
I0430 06:17:35.732126   966 solver.cpp:228] Iteration 151500, loss = 0.137508
I0430 06:17:35.732285   966 solver.cpp:244]     Train net output #0: loss = 0.137508 (* 1 = 0.137508 loss)
I0430 06:17:35.732291   966 sgd_solver.cpp:106] Iteration 151500, lr = 1e-22
I0430 06:18:25.361176   966 solver.cpp:228] Iteration 151600, loss = 0.109135
I0430 06:18:25.362211   966 solver.cpp:244]     Train net output #0: loss = 0.109135 (* 1 = 0.109135 loss)
I0430 06:18:25.362220   966 sgd_solver.cpp:106] Iteration 151600, lr = 1e-22
I0430 06:19:14.992254   966 solver.cpp:228] Iteration 151700, loss = 0.204376
I0430 06:19:14.992420   966 solver.cpp:244]     Train net output #0: loss = 0.204376 (* 1 = 0.204376 loss)
I0430 06:19:14.992427   966 sgd_solver.cpp:106] Iteration 151700, lr = 1e-22
I0430 06:20:06.431499   966 solver.cpp:228] Iteration 151800, loss = 0.118735
I0430 06:20:06.431663   966 solver.cpp:244]     Train net output #0: loss = 0.118735 (* 1 = 0.118735 loss)
I0430 06:20:06.431670   966 sgd_solver.cpp:106] Iteration 151800, lr = 1e-22
I0430 06:20:56.056440   966 solver.cpp:228] Iteration 151900, loss = 0.269689
I0430 06:20:56.056604   966 solver.cpp:244]     Train net output #0: loss = 0.269689 (* 1 = 0.269689 loss)
I0430 06:20:56.056610   966 sgd_solver.cpp:106] Iteration 151900, lr = 1e-22
I0430 06:21:45.380951   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_152000.caffemodel
I0430 06:22:07.750547   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_152000.solverstate
I0430 06:22:08.005622   966 solver.cpp:337] Iteration 152000, Testing net (#0)
I0430 06:22:08.005744   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 06:22:08.005750   966 net.cpp:693] Ignoring source layer visualize
I0430 06:22:08.005753   966 net.cpp:693] Ignoring source layer fake
I0430 06:27:01.311094   966 solver.cpp:404]     Test net output #0: loss = 0.191812 (* 1 = 0.191812 loss)
I0430 06:27:01.623270   966 solver.cpp:228] Iteration 152000, loss = 0.120339
I0430 06:27:01.623307   966 solver.cpp:244]     Train net output #0: loss = 0.120339 (* 1 = 0.120339 loss)
I0430 06:27:01.623313   966 sgd_solver.cpp:106] Iteration 152000, lr = 1e-22
I0430 06:27:51.258671   966 solver.cpp:228] Iteration 152100, loss = 0.086725
I0430 06:27:51.258829   966 solver.cpp:244]     Train net output #0: loss = 0.086725 (* 1 = 0.086725 loss)
I0430 06:27:51.258836   966 sgd_solver.cpp:106] Iteration 152100, lr = 1e-22
I0430 06:28:42.631283   966 solver.cpp:228] Iteration 152200, loss = 0.125231
I0430 06:28:42.631458   966 solver.cpp:244]     Train net output #0: loss = 0.125231 (* 1 = 0.125231 loss)
I0430 06:28:42.631465   966 sgd_solver.cpp:106] Iteration 152200, lr = 1e-22
I0430 06:29:32.247257   966 solver.cpp:228] Iteration 152300, loss = 0.113809
I0430 06:29:32.247417   966 solver.cpp:244]     Train net output #0: loss = 0.113809 (* 1 = 0.113809 loss)
I0430 06:29:32.247424   966 sgd_solver.cpp:106] Iteration 152300, lr = 1e-22
I0430 06:30:21.870241   966 solver.cpp:228] Iteration 152400, loss = 0.0945029
I0430 06:30:21.870388   966 solver.cpp:244]     Train net output #0: loss = 0.0945029 (* 1 = 0.0945029 loss)
I0430 06:30:21.870394   966 sgd_solver.cpp:106] Iteration 152400, lr = 1e-22
I0430 06:31:13.280936   966 solver.cpp:228] Iteration 152500, loss = 0.105739
I0430 06:31:13.281093   966 solver.cpp:244]     Train net output #0: loss = 0.105739 (* 1 = 0.105739 loss)
I0430 06:31:13.281111   966 sgd_solver.cpp:106] Iteration 152500, lr = 1e-22
I0430 06:32:02.900615   966 solver.cpp:228] Iteration 152600, loss = 0.181001
I0430 06:32:02.900753   966 solver.cpp:244]     Train net output #0: loss = 0.181001 (* 1 = 0.181001 loss)
I0430 06:32:02.900760   966 sgd_solver.cpp:106] Iteration 152600, lr = 1e-22
I0430 06:32:52.519809   966 solver.cpp:228] Iteration 152700, loss = 0.153916
I0430 06:32:52.519974   966 solver.cpp:244]     Train net output #0: loss = 0.153916 (* 1 = 0.153916 loss)
I0430 06:32:52.519981   966 sgd_solver.cpp:106] Iteration 152700, lr = 1e-22
I0430 06:33:43.775513   966 solver.cpp:228] Iteration 152800, loss = 0.122474
I0430 06:33:43.775665   966 solver.cpp:244]     Train net output #0: loss = 0.122474 (* 1 = 0.122474 loss)
I0430 06:33:43.775671   966 sgd_solver.cpp:106] Iteration 152800, lr = 1e-22
I0430 06:34:33.403256   966 solver.cpp:228] Iteration 152900, loss = 0.144668
I0430 06:34:33.403410   966 solver.cpp:244]     Train net output #0: loss = 0.144668 (* 1 = 0.144668 loss)
I0430 06:34:33.403417   966 sgd_solver.cpp:106] Iteration 152900, lr = 1e-22
I0430 06:35:22.703498   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_153000.caffemodel
I0430 06:35:33.432559   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_153000.solverstate
I0430 06:35:33.628561   966 solver.cpp:337] Iteration 153000, Testing net (#0)
I0430 06:35:33.628685   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 06:35:33.628706   966 net.cpp:693] Ignoring source layer visualize
I0430 06:35:33.628710   966 net.cpp:693] Ignoring source layer fake
I0430 06:40:26.299610   966 solver.cpp:404]     Test net output #0: loss = 0.185001 (* 1 = 0.185001 loss)
I0430 06:40:26.609619   966 solver.cpp:228] Iteration 153000, loss = 0.0833259
I0430 06:40:26.609645   966 solver.cpp:244]     Train net output #0: loss = 0.0833259 (* 1 = 0.0833259 loss)
I0430 06:40:26.609653   966 sgd_solver.cpp:106] Iteration 153000, lr = 1e-22
I0430 06:41:17.483559   966 solver.cpp:228] Iteration 153100, loss = 0.117881
I0430 06:41:17.483719   966 solver.cpp:244]     Train net output #0: loss = 0.117881 (* 1 = 0.117881 loss)
I0430 06:41:17.483726   966 sgd_solver.cpp:106] Iteration 153100, lr = 1e-22
I0430 06:42:07.107234   966 solver.cpp:228] Iteration 153200, loss = 0.104432
I0430 06:42:07.107384   966 solver.cpp:244]     Train net output #0: loss = 0.104432 (* 1 = 0.104432 loss)
I0430 06:42:07.107391   966 sgd_solver.cpp:106] Iteration 153200, lr = 1e-22
I0430 06:42:56.737684   966 solver.cpp:228] Iteration 153300, loss = 0.145831
I0430 06:42:56.738203   966 solver.cpp:244]     Train net output #0: loss = 0.145831 (* 1 = 0.145831 loss)
I0430 06:42:56.738224   966 sgd_solver.cpp:106] Iteration 153300, lr = 1e-22
I0430 06:43:47.693199   966 solver.cpp:228] Iteration 153400, loss = 0.142205
I0430 06:43:47.694181   966 solver.cpp:244]     Train net output #0: loss = 0.142205 (* 1 = 0.142205 loss)
I0430 06:43:47.694191   966 sgd_solver.cpp:106] Iteration 153400, lr = 1e-22
I0430 06:44:37.314388   966 solver.cpp:228] Iteration 153500, loss = 0.217494
I0430 06:44:37.314550   966 solver.cpp:244]     Train net output #0: loss = 0.217494 (* 1 = 0.217494 loss)
I0430 06:44:37.314558   966 sgd_solver.cpp:106] Iteration 153500, lr = 1e-22
I0430 06:45:28.470348   966 solver.cpp:228] Iteration 153600, loss = 0.116281
I0430 06:45:28.470515   966 solver.cpp:244]     Train net output #0: loss = 0.116281 (* 1 = 0.116281 loss)
I0430 06:45:28.470522   966 sgd_solver.cpp:106] Iteration 153600, lr = 1e-22
I0430 06:46:18.095571   966 solver.cpp:228] Iteration 153700, loss = 0.0293597
I0430 06:46:18.097350   966 solver.cpp:244]     Train net output #0: loss = 0.0293597 (* 1 = 0.0293597 loss)
I0430 06:46:18.097373   966 sgd_solver.cpp:106] Iteration 153700, lr = 1e-22
I0430 06:47:07.720120   966 solver.cpp:228] Iteration 153800, loss = 0.0316838
I0430 06:47:07.720283   966 solver.cpp:244]     Train net output #0: loss = 0.0316838 (* 1 = 0.0316838 loss)
I0430 06:47:07.720290   966 sgd_solver.cpp:106] Iteration 153800, lr = 1e-22
I0430 06:48:00.071214   966 solver.cpp:228] Iteration 153900, loss = 0.138105
I0430 06:48:00.071383   966 solver.cpp:244]     Train net output #0: loss = 0.138105 (* 1 = 0.138105 loss)
I0430 06:48:00.071390   966 sgd_solver.cpp:106] Iteration 153900, lr = 1e-22
I0430 06:48:49.367424   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_154000.caffemodel
I0430 06:49:37.560351   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_154000.solverstate
I0430 06:49:37.751000   966 solver.cpp:337] Iteration 154000, Testing net (#0)
I0430 06:49:37.751121   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 06:49:37.751127   966 net.cpp:693] Ignoring source layer visualize
I0430 06:49:37.751129   966 net.cpp:693] Ignoring source layer fake
I0430 06:54:30.680248   966 solver.cpp:404]     Test net output #0: loss = 0.182398 (* 1 = 0.182398 loss)
I0430 06:54:30.990000   966 solver.cpp:228] Iteration 154000, loss = 0.17058
I0430 06:54:30.990036   966 solver.cpp:244]     Train net output #0: loss = 0.17058 (* 1 = 0.17058 loss)
I0430 06:54:30.990042   966 sgd_solver.cpp:106] Iteration 154000, lr = 1e-22
I0430 06:55:20.619771   966 solver.cpp:228] Iteration 154100, loss = 0.23848
I0430 06:55:20.619945   966 solver.cpp:244]     Train net output #0: loss = 0.23848 (* 1 = 0.23848 loss)
I0430 06:55:20.619952   966 sgd_solver.cpp:106] Iteration 154100, lr = 1e-22
I0430 06:56:10.240708   966 solver.cpp:228] Iteration 154200, loss = 0.127095
I0430 06:56:10.240873   966 solver.cpp:244]     Train net output #0: loss = 0.127095 (* 1 = 0.127095 loss)
I0430 06:56:10.240880   966 sgd_solver.cpp:106] Iteration 154200, lr = 1e-22
I0430 06:57:02.559881   966 solver.cpp:228] Iteration 154300, loss = 0.118705
I0430 06:57:02.560046   966 solver.cpp:244]     Train net output #0: loss = 0.118705 (* 1 = 0.118705 loss)
I0430 06:57:02.560053   966 sgd_solver.cpp:106] Iteration 154300, lr = 1e-22
I0430 06:57:52.179044   966 solver.cpp:228] Iteration 154400, loss = 0.213854
I0430 06:57:52.179220   966 solver.cpp:244]     Train net output #0: loss = 0.213854 (* 1 = 0.213854 loss)
I0430 06:57:52.179229   966 sgd_solver.cpp:106] Iteration 154400, lr = 1e-22
I0430 06:58:41.810333   966 solver.cpp:228] Iteration 154500, loss = 0.156365
I0430 06:58:41.810484   966 solver.cpp:244]     Train net output #0: loss = 0.156365 (* 1 = 0.156365 loss)
I0430 06:58:41.810492   966 sgd_solver.cpp:106] Iteration 154500, lr = 1e-22
I0430 06:59:33.289204   966 solver.cpp:228] Iteration 154600, loss = 0.288827
I0430 06:59:33.289369   966 solver.cpp:244]     Train net output #0: loss = 0.288827 (* 1 = 0.288827 loss)
I0430 06:59:33.289376   966 sgd_solver.cpp:106] Iteration 154600, lr = 1e-22
I0430 07:00:22.908890   966 solver.cpp:228] Iteration 154700, loss = 0.113825
I0430 07:00:22.909054   966 solver.cpp:244]     Train net output #0: loss = 0.113825 (* 1 = 0.113825 loss)
I0430 07:00:22.909061   966 sgd_solver.cpp:106] Iteration 154700, lr = 1e-22
I0430 07:01:12.529167   966 solver.cpp:228] Iteration 154800, loss = 0.0691462
I0430 07:01:12.529289   966 solver.cpp:244]     Train net output #0: loss = 0.0691462 (* 1 = 0.0691462 loss)
I0430 07:01:12.529294   966 sgd_solver.cpp:106] Iteration 154800, lr = 1e-22
I0430 07:02:02.150185   966 solver.cpp:228] Iteration 154900, loss = 0.187067
I0430 07:02:02.150352   966 solver.cpp:244]     Train net output #0: loss = 0.187067 (* 1 = 0.187067 loss)
I0430 07:02:02.150359   966 sgd_solver.cpp:106] Iteration 154900, lr = 1e-22
I0430 07:02:53.373133   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_155000.caffemodel
I0430 07:03:08.979913   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_155000.solverstate
I0430 07:03:09.186264   966 solver.cpp:337] Iteration 155000, Testing net (#0)
I0430 07:03:09.186386   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 07:03:09.186393   966 net.cpp:693] Ignoring source layer visualize
I0430 07:03:09.186394   966 net.cpp:693] Ignoring source layer fake
I0430 07:08:01.625886   966 solver.cpp:404]     Test net output #0: loss = 0.179659 (* 1 = 0.179659 loss)
I0430 07:08:01.937073   966 solver.cpp:228] Iteration 155000, loss = 0.0854612
I0430 07:08:01.937095   966 solver.cpp:244]     Train net output #0: loss = 0.0854612 (* 1 = 0.0854612 loss)
I0430 07:08:01.937117   966 sgd_solver.cpp:106] Iteration 155000, lr = 1e-22
I0430 07:08:51.567387   966 solver.cpp:228] Iteration 155100, loss = 0.0931233
I0430 07:08:51.567567   966 solver.cpp:244]     Train net output #0: loss = 0.0931233 (* 1 = 0.0931233 loss)
I0430 07:08:51.567574   966 sgd_solver.cpp:106] Iteration 155100, lr = 1e-22
I0430 07:09:41.201493   966 solver.cpp:228] Iteration 155200, loss = 0.130167
I0430 07:09:41.201661   966 solver.cpp:244]     Train net output #0: loss = 0.130167 (* 1 = 0.130167 loss)
I0430 07:09:41.201668   966 sgd_solver.cpp:106] Iteration 155200, lr = 1e-22
I0430 07:10:32.578680   966 solver.cpp:228] Iteration 155300, loss = 0.102211
I0430 07:10:32.578847   966 solver.cpp:244]     Train net output #0: loss = 0.102211 (* 1 = 0.102211 loss)
I0430 07:10:32.578855   966 sgd_solver.cpp:106] Iteration 155300, lr = 1e-22
I0430 07:11:22.198554   966 solver.cpp:228] Iteration 155400, loss = 0.14466
I0430 07:11:22.198695   966 solver.cpp:244]     Train net output #0: loss = 0.14466 (* 1 = 0.14466 loss)
I0430 07:11:22.198703   966 sgd_solver.cpp:106] Iteration 155400, lr = 1e-22
I0430 07:12:11.818706   966 solver.cpp:228] Iteration 155500, loss = 0.12357
I0430 07:12:11.818868   966 solver.cpp:244]     Train net output #0: loss = 0.12357 (* 1 = 0.12357 loss)
I0430 07:12:11.818876   966 sgd_solver.cpp:106] Iteration 155500, lr = 1e-22
I0430 07:13:03.205659   966 solver.cpp:228] Iteration 155600, loss = 0.0538919
I0430 07:13:03.205780   966 solver.cpp:244]     Train net output #0: loss = 0.0538919 (* 1 = 0.0538919 loss)
I0430 07:13:03.205786   966 sgd_solver.cpp:106] Iteration 155600, lr = 1e-22
I0430 07:13:52.816642   966 solver.cpp:228] Iteration 155700, loss = 0.159799
I0430 07:13:52.816803   966 solver.cpp:244]     Train net output #0: loss = 0.159799 (* 1 = 0.159799 loss)
I0430 07:13:52.816809   966 sgd_solver.cpp:106] Iteration 155700, lr = 1e-22
I0430 07:14:42.445921   966 solver.cpp:228] Iteration 155800, loss = 0.120096
I0430 07:14:42.446058   966 solver.cpp:244]     Train net output #0: loss = 0.120096 (* 1 = 0.120096 loss)
I0430 07:14:42.446064   966 sgd_solver.cpp:106] Iteration 155800, lr = 1e-22
I0430 07:15:32.063412   966 solver.cpp:228] Iteration 155900, loss = 0.322782
I0430 07:15:32.063565   966 solver.cpp:244]     Train net output #0: loss = 0.322782 (* 1 = 0.322782 loss)
I0430 07:15:32.063572   966 sgd_solver.cpp:106] Iteration 155900, lr = 1e-22
I0430 07:16:23.010226   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_156000.caffemodel
I0430 07:16:52.318125   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_156000.solverstate
I0430 07:16:52.521055   966 solver.cpp:337] Iteration 156000, Testing net (#0)
I0430 07:16:52.521183   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 07:16:52.521188   966 net.cpp:693] Ignoring source layer visualize
I0430 07:16:52.521189   966 net.cpp:693] Ignoring source layer fake
I0430 07:21:44.622692   966 solver.cpp:404]     Test net output #0: loss = 0.183064 (* 1 = 0.183064 loss)
I0430 07:21:44.933176   966 solver.cpp:228] Iteration 156000, loss = 0.120383
I0430 07:21:44.933212   966 solver.cpp:244]     Train net output #0: loss = 0.120383 (* 1 = 0.120383 loss)
I0430 07:21:44.933219   966 sgd_solver.cpp:106] Iteration 156000, lr = 1e-22
I0430 07:22:34.559505   966 solver.cpp:228] Iteration 156100, loss = 0.437543
I0430 07:22:34.559665   966 solver.cpp:244]     Train net output #0: loss = 0.437543 (* 1 = 0.437543 loss)
I0430 07:22:34.559672   966 sgd_solver.cpp:106] Iteration 156100, lr = 1e-22
I0430 07:23:24.176439   966 solver.cpp:228] Iteration 156200, loss = 0.108741
I0430 07:23:24.176584   966 solver.cpp:244]     Train net output #0: loss = 0.108741 (* 1 = 0.108741 loss)
I0430 07:23:24.176589   966 sgd_solver.cpp:106] Iteration 156200, lr = 1e-22
I0430 07:24:15.067633   966 solver.cpp:228] Iteration 156300, loss = 0.126733
I0430 07:24:15.067836   966 solver.cpp:244]     Train net output #0: loss = 0.126733 (* 1 = 0.126733 loss)
I0430 07:24:15.067845   966 sgd_solver.cpp:106] Iteration 156300, lr = 1e-22
I0430 07:25:04.685720   966 solver.cpp:228] Iteration 156400, loss = 0.145936
I0430 07:25:04.685899   966 solver.cpp:244]     Train net output #0: loss = 0.145936 (* 1 = 0.145936 loss)
I0430 07:25:04.685904   966 sgd_solver.cpp:106] Iteration 156400, lr = 1e-22
I0430 07:25:55.630676   966 solver.cpp:228] Iteration 156500, loss = 0.103279
I0430 07:25:55.630827   966 solver.cpp:244]     Train net output #0: loss = 0.103279 (* 1 = 0.103279 loss)
I0430 07:25:55.630834   966 sgd_solver.cpp:106] Iteration 156500, lr = 1e-22
I0430 07:26:45.253820   966 solver.cpp:228] Iteration 156600, loss = 0.176488
I0430 07:26:45.253979   966 solver.cpp:244]     Train net output #0: loss = 0.176488 (* 1 = 0.176488 loss)
I0430 07:26:45.253985   966 sgd_solver.cpp:106] Iteration 156600, lr = 1e-22
I0430 07:27:34.885040   966 solver.cpp:228] Iteration 156700, loss = 0.193739
I0430 07:27:34.885200   966 solver.cpp:244]     Train net output #0: loss = 0.193739 (* 1 = 0.193739 loss)
I0430 07:27:34.885207   966 sgd_solver.cpp:106] Iteration 156700, lr = 1e-22
I0430 07:28:26.021370   966 solver.cpp:228] Iteration 156800, loss = 0.0749115
I0430 07:28:26.021550   966 solver.cpp:244]     Train net output #0: loss = 0.0749115 (* 1 = 0.0749115 loss)
I0430 07:28:26.021558   966 sgd_solver.cpp:106] Iteration 156800, lr = 1e-22
I0430 07:29:15.639195   966 solver.cpp:228] Iteration 156900, loss = 0.0707993
I0430 07:29:15.639331   966 solver.cpp:244]     Train net output #0: loss = 0.0707993 (* 1 = 0.0707993 loss)
I0430 07:29:15.639338   966 sgd_solver.cpp:106] Iteration 156900, lr = 1e-22
I0430 07:30:04.954780   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_157000.caffemodel
I0430 07:30:16.424150   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_157000.solverstate
I0430 07:30:16.613265   966 solver.cpp:337] Iteration 157000, Testing net (#0)
I0430 07:30:16.613385   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 07:30:16.613407   966 net.cpp:693] Ignoring source layer visualize
I0430 07:30:16.613410   966 net.cpp:693] Ignoring source layer fake
I0430 07:35:09.353708   966 solver.cpp:404]     Test net output #0: loss = 0.19235 (* 1 = 0.19235 loss)
I0430 07:35:09.663390   966 solver.cpp:228] Iteration 157000, loss = 0.111952
I0430 07:35:09.663408   966 solver.cpp:244]     Train net output #0: loss = 0.111952 (* 1 = 0.111952 loss)
I0430 07:35:09.663430   966 sgd_solver.cpp:106] Iteration 157000, lr = 1e-22
I0430 07:36:01.040666   966 solver.cpp:228] Iteration 157100, loss = 0.124094
I0430 07:36:01.040844   966 solver.cpp:244]     Train net output #0: loss = 0.124094 (* 1 = 0.124094 loss)
I0430 07:36:01.040851   966 sgd_solver.cpp:106] Iteration 157100, lr = 1e-22
I0430 07:36:50.665042   966 solver.cpp:228] Iteration 157200, loss = 0.15956
I0430 07:36:50.665228   966 solver.cpp:244]     Train net output #0: loss = 0.15956 (* 1 = 0.15956 loss)
I0430 07:36:50.665235   966 sgd_solver.cpp:106] Iteration 157200, lr = 1e-22
I0430 07:37:40.293535   966 solver.cpp:228] Iteration 157300, loss = 0.0956375
I0430 07:37:40.293709   966 solver.cpp:244]     Train net output #0: loss = 0.0956375 (* 1 = 0.0956375 loss)
I0430 07:37:40.293715   966 sgd_solver.cpp:106] Iteration 157300, lr = 1e-22
I0430 07:38:31.751978   966 solver.cpp:228] Iteration 157400, loss = 0.179432
I0430 07:38:31.752148   966 solver.cpp:244]     Train net output #0: loss = 0.179432 (* 1 = 0.179432 loss)
I0430 07:38:31.752156   966 sgd_solver.cpp:106] Iteration 157400, lr = 1e-22
I0430 07:39:21.379675   966 solver.cpp:228] Iteration 157500, loss = 0.116565
I0430 07:39:21.379817   966 solver.cpp:244]     Train net output #0: loss = 0.116565 (* 1 = 0.116565 loss)
I0430 07:39:21.379823   966 sgd_solver.cpp:106] Iteration 157500, lr = 1e-23
I0430 07:40:11.014503   966 solver.cpp:228] Iteration 157600, loss = 0.109729
I0430 07:40:11.014642   966 solver.cpp:244]     Train net output #0: loss = 0.109729 (* 1 = 0.109729 loss)
I0430 07:40:11.014648   966 sgd_solver.cpp:106] Iteration 157600, lr = 1e-23
I0430 07:41:02.448592   966 solver.cpp:228] Iteration 157700, loss = 0.20429
I0430 07:41:02.448760   966 solver.cpp:244]     Train net output #0: loss = 0.20429 (* 1 = 0.20429 loss)
I0430 07:41:02.448766   966 sgd_solver.cpp:106] Iteration 157700, lr = 1e-23
I0430 07:41:52.069839   966 solver.cpp:228] Iteration 157800, loss = 0.179752
I0430 07:41:52.070003   966 solver.cpp:244]     Train net output #0: loss = 0.179752 (* 1 = 0.179752 loss)
I0430 07:41:52.070016   966 sgd_solver.cpp:106] Iteration 157800, lr = 1e-23
I0430 07:42:41.696204   966 solver.cpp:228] Iteration 157900, loss = 0.126116
I0430 07:42:41.696357   966 solver.cpp:244]     Train net output #0: loss = 0.126116 (* 1 = 0.126116 loss)
I0430 07:42:41.696362   966 sgd_solver.cpp:106] Iteration 157900, lr = 1e-23
I0430 07:43:31.012094   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_158000.caffemodel
I0430 07:43:59.600621   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_158000.solverstate
I0430 07:43:59.912600   966 solver.cpp:337] Iteration 158000, Testing net (#0)
I0430 07:43:59.912717   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 07:43:59.912725   966 net.cpp:693] Ignoring source layer visualize
I0430 07:43:59.912729   966 net.cpp:693] Ignoring source layer fake
I0430 07:48:53.262523   966 solver.cpp:404]     Test net output #0: loss = 0.183715 (* 1 = 0.183715 loss)
I0430 07:48:53.574436   966 solver.cpp:228] Iteration 158000, loss = 0.242338
I0430 07:48:53.574472   966 solver.cpp:244]     Train net output #0: loss = 0.242338 (* 1 = 0.242338 loss)
I0430 07:48:53.574479   966 sgd_solver.cpp:106] Iteration 158000, lr = 1e-23
I0430 07:49:45.097275   966 solver.cpp:228] Iteration 158100, loss = 0.142211
I0430 07:49:45.097486   966 solver.cpp:244]     Train net output #0: loss = 0.142211 (* 1 = 0.142211 loss)
I0430 07:49:45.097494   966 sgd_solver.cpp:106] Iteration 158100, lr = 1e-23
I0430 07:50:34.712712   966 solver.cpp:228] Iteration 158200, loss = 0.0982835
I0430 07:50:34.712884   966 solver.cpp:244]     Train net output #0: loss = 0.0982835 (* 1 = 0.0982835 loss)
I0430 07:50:34.712891   966 sgd_solver.cpp:106] Iteration 158200, lr = 1e-23
I0430 07:51:24.342941   966 solver.cpp:228] Iteration 158300, loss = 0.0793356
I0430 07:51:24.343133   966 solver.cpp:244]     Train net output #0: loss = 0.0793356 (* 1 = 0.0793356 loss)
I0430 07:51:24.343142   966 sgd_solver.cpp:106] Iteration 158300, lr = 1e-23
I0430 07:52:15.817503   966 solver.cpp:228] Iteration 158400, loss = 0.0690167
I0430 07:52:15.817665   966 solver.cpp:244]     Train net output #0: loss = 0.0690167 (* 1 = 0.0690167 loss)
I0430 07:52:15.817672   966 sgd_solver.cpp:106] Iteration 158400, lr = 1e-23
I0430 07:53:05.440726   966 solver.cpp:228] Iteration 158500, loss = 0.0943323
I0430 07:53:05.440917   966 solver.cpp:244]     Train net output #0: loss = 0.0943323 (* 1 = 0.0943323 loss)
I0430 07:53:05.440924   966 sgd_solver.cpp:106] Iteration 158500, lr = 1e-23
I0430 07:53:55.058346   966 solver.cpp:228] Iteration 158600, loss = 0.113946
I0430 07:53:55.058539   966 solver.cpp:244]     Train net output #0: loss = 0.113946 (* 1 = 0.113946 loss)
I0430 07:53:55.058547   966 sgd_solver.cpp:106] Iteration 158600, lr = 1e-23
I0430 07:54:44.674072   966 solver.cpp:228] Iteration 158700, loss = 0.175417
I0430 07:54:44.674226   966 solver.cpp:244]     Train net output #0: loss = 0.175417 (* 1 = 0.175417 loss)
I0430 07:54:44.674233   966 sgd_solver.cpp:106] Iteration 158700, lr = 1e-23
I0430 07:55:36.132923   966 solver.cpp:228] Iteration 158800, loss = 0.102745
I0430 07:55:36.133085   966 solver.cpp:244]     Train net output #0: loss = 0.102745 (* 1 = 0.102745 loss)
I0430 07:55:36.133091   966 sgd_solver.cpp:106] Iteration 158800, lr = 1e-23
I0430 07:56:25.747467   966 solver.cpp:228] Iteration 158900, loss = 0.101062
I0430 07:56:25.747618   966 solver.cpp:244]     Train net output #0: loss = 0.101062 (* 1 = 0.101062 loss)
I0430 07:56:25.747625   966 sgd_solver.cpp:106] Iteration 158900, lr = 1e-23
I0430 07:57:15.060825   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_159000.caffemodel
I0430 07:57:22.484225   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_159000.solverstate
I0430 07:57:22.674279   966 solver.cpp:337] Iteration 159000, Testing net (#0)
I0430 07:57:22.674401   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 07:57:22.674422   966 net.cpp:693] Ignoring source layer visualize
I0430 07:57:22.674424   966 net.cpp:693] Ignoring source layer fake
I0430 08:02:15.906110   966 solver.cpp:404]     Test net output #0: loss = 0.182524 (* 1 = 0.182524 loss)
I0430 08:02:16.216823   966 solver.cpp:228] Iteration 159000, loss = 0.120546
I0430 08:02:16.216845   966 solver.cpp:244]     Train net output #0: loss = 0.120546 (* 1 = 0.120546 loss)
I0430 08:02:16.216867   966 sgd_solver.cpp:106] Iteration 159000, lr = 1e-23
I0430 08:03:07.592685   966 solver.cpp:228] Iteration 159100, loss = 0.142018
I0430 08:03:07.592860   966 solver.cpp:244]     Train net output #0: loss = 0.142018 (* 1 = 0.142018 loss)
I0430 08:03:07.592867   966 sgd_solver.cpp:106] Iteration 159100, lr = 1e-23
I0430 08:03:57.212987   966 solver.cpp:228] Iteration 159200, loss = 0.0871069
I0430 08:03:57.213137   966 solver.cpp:244]     Train net output #0: loss = 0.0871069 (* 1 = 0.0871069 loss)
I0430 08:03:57.213145   966 sgd_solver.cpp:106] Iteration 159200, lr = 1e-23
I0430 08:04:46.834630   966 solver.cpp:228] Iteration 159300, loss = 0.105742
I0430 08:04:46.834802   966 solver.cpp:244]     Train net output #0: loss = 0.105742 (* 1 = 0.105742 loss)
I0430 08:04:46.834810   966 sgd_solver.cpp:106] Iteration 159300, lr = 1e-23
I0430 08:05:37.783545   966 solver.cpp:228] Iteration 159400, loss = 0.0984353
I0430 08:05:37.783711   966 solver.cpp:244]     Train net output #0: loss = 0.0984353 (* 1 = 0.0984353 loss)
I0430 08:05:37.783718   966 sgd_solver.cpp:106] Iteration 159400, lr = 1e-23
I0430 08:06:27.409205   966 solver.cpp:228] Iteration 159500, loss = 0.133654
I0430 08:06:27.409390   966 solver.cpp:244]     Train net output #0: loss = 0.133654 (* 1 = 0.133654 loss)
I0430 08:06:27.409399   966 sgd_solver.cpp:106] Iteration 159500, lr = 1e-23
I0430 08:07:18.411088   966 solver.cpp:228] Iteration 159600, loss = 0.142853
I0430 08:07:18.411253   966 solver.cpp:244]     Train net output #0: loss = 0.142853 (* 1 = 0.142853 loss)
I0430 08:07:18.411260   966 sgd_solver.cpp:106] Iteration 159600, lr = 1e-23
I0430 08:08:08.046377   966 solver.cpp:228] Iteration 159700, loss = 0.179003
I0430 08:08:08.046519   966 solver.cpp:244]     Train net output #0: loss = 0.179003 (* 1 = 0.179003 loss)
I0430 08:08:08.046525   966 sgd_solver.cpp:106] Iteration 159700, lr = 1e-23
I0430 08:08:57.674419   966 solver.cpp:228] Iteration 159800, loss = 0.10804
I0430 08:08:57.674607   966 solver.cpp:244]     Train net output #0: loss = 0.10804 (* 1 = 0.10804 loss)
I0430 08:08:57.674614   966 sgd_solver.cpp:106] Iteration 159800, lr = 1e-23
I0430 08:09:48.878113   966 solver.cpp:228] Iteration 159900, loss = 0.0323765
I0430 08:09:48.878301   966 solver.cpp:244]     Train net output #0: loss = 0.0323765 (* 1 = 0.0323765 loss)
I0430 08:09:48.878309   966 sgd_solver.cpp:106] Iteration 159900, lr = 1e-23
I0430 08:10:38.199770   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_160000.caffemodel
I0430 08:10:48.360368   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_160000.solverstate
I0430 08:10:48.549947   966 solver.cpp:337] Iteration 160000, Testing net (#0)
I0430 08:10:48.550068   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 08:10:48.550076   966 net.cpp:693] Ignoring source layer visualize
I0430 08:10:48.550092   966 net.cpp:693] Ignoring source layer fake
I0430 08:15:41.525863   966 solver.cpp:404]     Test net output #0: loss = 0.17923 (* 1 = 0.17923 loss)
I0430 08:15:41.835747   966 solver.cpp:228] Iteration 160000, loss = 0.125992
I0430 08:15:41.835765   966 solver.cpp:244]     Train net output #0: loss = 0.125992 (* 1 = 0.125992 loss)
I0430 08:15:41.835788   966 sgd_solver.cpp:106] Iteration 160000, lr = 1e-23
I0430 08:16:31.467499   966 solver.cpp:228] Iteration 160100, loss = 0.157956
I0430 08:16:31.467679   966 solver.cpp:244]     Train net output #0: loss = 0.157956 (* 1 = 0.157956 loss)
I0430 08:16:31.467689   966 sgd_solver.cpp:106] Iteration 160100, lr = 1e-23
I0430 08:17:23.022925   966 solver.cpp:228] Iteration 160200, loss = 0.270543
I0430 08:17:23.023108   966 solver.cpp:244]     Train net output #0: loss = 0.270543 (* 1 = 0.270543 loss)
I0430 08:17:23.023118   966 sgd_solver.cpp:106] Iteration 160200, lr = 1e-23
I0430 08:18:12.653885   966 solver.cpp:228] Iteration 160300, loss = 0.175831
I0430 08:18:12.654040   966 solver.cpp:244]     Train net output #0: loss = 0.175831 (* 1 = 0.175831 loss)
I0430 08:18:12.654047   966 sgd_solver.cpp:106] Iteration 160300, lr = 1e-23
I0430 08:19:02.294437   966 solver.cpp:228] Iteration 160400, loss = 0.210976
I0430 08:19:02.294747   966 solver.cpp:244]     Train net output #0: loss = 0.210976 (* 1 = 0.210976 loss)
I0430 08:19:02.294754   966 sgd_solver.cpp:106] Iteration 160400, lr = 1e-23
I0430 08:19:54.112076   966 solver.cpp:228] Iteration 160500, loss = 0.130811
I0430 08:19:54.112200   966 solver.cpp:244]     Train net output #0: loss = 0.130811 (* 1 = 0.130811 loss)
I0430 08:19:54.112208   966 sgd_solver.cpp:106] Iteration 160500, lr = 1e-23
I0430 08:20:43.733980   966 solver.cpp:228] Iteration 160600, loss = 0.179097
I0430 08:20:43.734143   966 solver.cpp:244]     Train net output #0: loss = 0.179097 (* 1 = 0.179097 loss)
I0430 08:20:43.734149   966 sgd_solver.cpp:106] Iteration 160600, lr = 1e-23
I0430 08:21:33.363629   966 solver.cpp:228] Iteration 160700, loss = 0.254308
I0430 08:21:33.363780   966 solver.cpp:244]     Train net output #0: loss = 0.254308 (* 1 = 0.254308 loss)
I0430 08:21:33.363787   966 sgd_solver.cpp:106] Iteration 160700, lr = 1e-23
I0430 08:22:22.995326   966 solver.cpp:228] Iteration 160800, loss = 0.194608
I0430 08:22:22.995471   966 solver.cpp:244]     Train net output #0: loss = 0.194608 (* 1 = 0.194608 loss)
I0430 08:22:22.995481   966 sgd_solver.cpp:106] Iteration 160800, lr = 1e-23
I0430 08:23:14.563980   966 solver.cpp:228] Iteration 160900, loss = 0.110138
I0430 08:23:14.564122   966 solver.cpp:244]     Train net output #0: loss = 0.110138 (* 1 = 0.110138 loss)
I0430 08:23:14.564129   966 sgd_solver.cpp:106] Iteration 160900, lr = 1e-23
I0430 08:24:03.883797   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_161000.caffemodel
I0430 08:24:07.778908   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_161000.solverstate
I0430 08:24:07.973400   966 solver.cpp:337] Iteration 161000, Testing net (#0)
I0430 08:24:07.973558   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 08:24:07.973565   966 net.cpp:693] Ignoring source layer visualize
I0430 08:24:07.973568   966 net.cpp:693] Ignoring source layer fake
I0430 08:29:04.537976   966 solver.cpp:404]     Test net output #0: loss = 0.183682 (* 1 = 0.183682 loss)
I0430 08:29:04.847618   966 solver.cpp:228] Iteration 161000, loss = 0.160429
I0430 08:29:04.847641   966 solver.cpp:244]     Train net output #0: loss = 0.160429 (* 1 = 0.160429 loss)
I0430 08:29:04.847664   966 sgd_solver.cpp:106] Iteration 161000, lr = 1e-23
I0430 08:29:54.475158   966 solver.cpp:228] Iteration 161100, loss = 0.178227
I0430 08:29:54.475329   966 solver.cpp:244]     Train net output #0: loss = 0.178227 (* 1 = 0.178227 loss)
I0430 08:29:54.475337   966 sgd_solver.cpp:106] Iteration 161100, lr = 1e-23
I0430 08:30:46.060142   966 solver.cpp:228] Iteration 161200, loss = 0.0815711
I0430 08:30:46.060304   966 solver.cpp:244]     Train net output #0: loss = 0.0815711 (* 1 = 0.0815711 loss)
I0430 08:30:46.060312   966 sgd_solver.cpp:106] Iteration 161200, lr = 1e-23
I0430 08:31:35.679342   966 solver.cpp:228] Iteration 161300, loss = 0.0904181
I0430 08:31:35.679530   966 solver.cpp:244]     Train net output #0: loss = 0.0904181 (* 1 = 0.0904181 loss)
I0430 08:31:35.679539   966 sgd_solver.cpp:106] Iteration 161300, lr = 1e-23
I0430 08:32:25.304463   966 solver.cpp:228] Iteration 161400, loss = 0.111488
I0430 08:32:25.304641   966 solver.cpp:244]     Train net output #0: loss = 0.111488 (* 1 = 0.111488 loss)
I0430 08:32:25.304649   966 sgd_solver.cpp:106] Iteration 161400, lr = 1e-23
I0430 08:33:16.759423   966 solver.cpp:228] Iteration 161500, loss = 0.146318
I0430 08:33:16.759589   966 solver.cpp:244]     Train net output #0: loss = 0.146318 (* 1 = 0.146318 loss)
I0430 08:33:16.759596   966 sgd_solver.cpp:106] Iteration 161500, lr = 1e-23
I0430 08:34:06.367629   966 solver.cpp:228] Iteration 161600, loss = 0.106802
I0430 08:34:06.367794   966 solver.cpp:244]     Train net output #0: loss = 0.106802 (* 1 = 0.106802 loss)
I0430 08:34:06.367800   966 sgd_solver.cpp:106] Iteration 161600, lr = 1e-23
I0430 08:34:55.985031   966 solver.cpp:228] Iteration 161700, loss = 0.122545
I0430 08:34:55.985186   966 solver.cpp:244]     Train net output #0: loss = 0.122545 (* 1 = 0.122545 loss)
I0430 08:34:55.985193   966 sgd_solver.cpp:106] Iteration 161700, lr = 1e-23
I0430 08:35:45.612622   966 solver.cpp:228] Iteration 161800, loss = 0.0784943
I0430 08:35:45.612777   966 solver.cpp:244]     Train net output #0: loss = 0.0784943 (* 1 = 0.0784943 loss)
I0430 08:35:45.612785   966 sgd_solver.cpp:106] Iteration 161800, lr = 1e-23
I0430 08:36:37.075155   966 solver.cpp:228] Iteration 161900, loss = 0.145675
I0430 08:36:37.075309   966 solver.cpp:244]     Train net output #0: loss = 0.145675 (* 1 = 0.145675 loss)
I0430 08:36:37.075316   966 sgd_solver.cpp:106] Iteration 161900, lr = 1e-23
I0430 08:37:26.385314   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_162000.caffemodel
I0430 08:38:02.063153   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_162000.solverstate
I0430 08:38:02.265918   966 solver.cpp:337] Iteration 162000, Testing net (#0)
I0430 08:38:02.266041   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 08:38:02.266047   966 net.cpp:693] Ignoring source layer visualize
I0430 08:38:02.266063   966 net.cpp:693] Ignoring source layer fake
I0430 08:42:55.412737   966 solver.cpp:404]     Test net output #0: loss = 0.191943 (* 1 = 0.191943 loss)
I0430 08:42:55.725224   966 solver.cpp:228] Iteration 162000, loss = 0.0832986
I0430 08:42:55.725261   966 solver.cpp:244]     Train net output #0: loss = 0.0832986 (* 1 = 0.0832986 loss)
I0430 08:42:55.725270   966 sgd_solver.cpp:106] Iteration 162000, lr = 1e-23
I0430 08:43:45.325021   966 solver.cpp:228] Iteration 162100, loss = 0.0290402
I0430 08:43:45.325193   966 solver.cpp:244]     Train net output #0: loss = 0.0290402 (* 1 = 0.0290402 loss)
I0430 08:43:45.325201   966 sgd_solver.cpp:106] Iteration 162100, lr = 1e-23
I0430 08:44:36.634462   966 solver.cpp:228] Iteration 162200, loss = 0.364985
I0430 08:44:36.634631   966 solver.cpp:244]     Train net output #0: loss = 0.364985 (* 1 = 0.364985 loss)
I0430 08:44:36.634639   966 sgd_solver.cpp:106] Iteration 162200, lr = 1e-23
I0430 08:45:26.246448   966 solver.cpp:228] Iteration 162300, loss = 0.15315
I0430 08:45:26.246646   966 solver.cpp:244]     Train net output #0: loss = 0.15315 (* 1 = 0.15315 loss)
I0430 08:45:26.246655   966 sgd_solver.cpp:106] Iteration 162300, lr = 1e-23
I0430 08:46:15.855242   966 solver.cpp:228] Iteration 162400, loss = 0.0930017
I0430 08:46:15.855438   966 solver.cpp:244]     Train net output #0: loss = 0.0930017 (* 1 = 0.0930017 loss)
I0430 08:46:15.855446   966 sgd_solver.cpp:106] Iteration 162400, lr = 1e-23
I0430 08:47:06.784680   966 solver.cpp:228] Iteration 162500, loss = 0.0959311
I0430 08:47:06.784847   966 solver.cpp:244]     Train net output #0: loss = 0.0959311 (* 1 = 0.0959311 loss)
I0430 08:47:06.784865   966 sgd_solver.cpp:106] Iteration 162500, lr = 1e-23
I0430 08:47:56.398591   966 solver.cpp:228] Iteration 162600, loss = 0.182064
I0430 08:47:56.398736   966 solver.cpp:244]     Train net output #0: loss = 0.182064 (* 1 = 0.182064 loss)
I0430 08:47:56.398756   966 sgd_solver.cpp:106] Iteration 162600, lr = 1e-23
I0430 08:48:46.026556   966 solver.cpp:228] Iteration 162700, loss = 0.119396
I0430 08:48:46.026696   966 solver.cpp:244]     Train net output #0: loss = 0.119396 (* 1 = 0.119396 loss)
I0430 08:48:46.026703   966 sgd_solver.cpp:106] Iteration 162700, lr = 1e-23
I0430 08:49:37.016093   966 solver.cpp:228] Iteration 162800, loss = 0.0947137
I0430 08:49:37.016288   966 solver.cpp:244]     Train net output #0: loss = 0.0947137 (* 1 = 0.0947137 loss)
I0430 08:49:37.016296   966 sgd_solver.cpp:106] Iteration 162800, lr = 1e-23
I0430 08:50:26.649492   966 solver.cpp:228] Iteration 162900, loss = 0.135802
I0430 08:50:26.649648   966 solver.cpp:244]     Train net output #0: loss = 0.135802 (* 1 = 0.135802 loss)
I0430 08:50:26.649655   966 sgd_solver.cpp:106] Iteration 162900, lr = 1e-23
I0430 08:51:17.539023   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_163000.caffemodel
I0430 08:51:38.035946   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_163000.solverstate
I0430 08:51:38.227102   966 solver.cpp:337] Iteration 163000, Testing net (#0)
I0430 08:51:38.227228   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 08:51:38.227249   966 net.cpp:693] Ignoring source layer visualize
I0430 08:51:38.227252   966 net.cpp:693] Ignoring source layer fake
I0430 08:56:30.631521   966 solver.cpp:404]     Test net output #0: loss = 0.183669 (* 1 = 0.183669 loss)
I0430 08:56:30.942172   966 solver.cpp:228] Iteration 163000, loss = 0.0648812
I0430 08:56:30.942191   966 solver.cpp:244]     Train net output #0: loss = 0.0648812 (* 1 = 0.0648812 loss)
I0430 08:56:30.942214   966 sgd_solver.cpp:106] Iteration 163000, lr = 1e-23
I0430 08:57:20.567278   966 solver.cpp:228] Iteration 163100, loss = 0.153484
I0430 08:57:20.567443   966 solver.cpp:244]     Train net output #0: loss = 0.153484 (* 1 = 0.153484 loss)
I0430 08:57:20.567450   966 sgd_solver.cpp:106] Iteration 163100, lr = 1e-23
I0430 08:58:10.202641   966 solver.cpp:228] Iteration 163200, loss = 0.0813255
I0430 08:58:10.202826   966 solver.cpp:244]     Train net output #0: loss = 0.0813255 (* 1 = 0.0813255 loss)
I0430 08:58:10.202832   966 sgd_solver.cpp:106] Iteration 163200, lr = 1e-23
I0430 08:59:01.641010   966 solver.cpp:228] Iteration 163300, loss = 0.158836
I0430 08:59:01.641191   966 solver.cpp:244]     Train net output #0: loss = 0.158836 (* 1 = 0.158836 loss)
I0430 08:59:01.641199   966 sgd_solver.cpp:106] Iteration 163300, lr = 1e-23
I0430 08:59:51.265120   966 solver.cpp:228] Iteration 163400, loss = 0.152336
I0430 08:59:51.265307   966 solver.cpp:244]     Train net output #0: loss = 0.152336 (* 1 = 0.152336 loss)
I0430 08:59:51.265316   966 sgd_solver.cpp:106] Iteration 163400, lr = 1e-23
I0430 09:00:40.895920   966 solver.cpp:228] Iteration 163500, loss = 0.212672
I0430 09:00:40.896066   966 solver.cpp:244]     Train net output #0: loss = 0.212672 (* 1 = 0.212672 loss)
I0430 09:00:40.896073   966 sgd_solver.cpp:106] Iteration 163500, lr = 1e-23
I0430 09:01:32.438598   966 solver.cpp:228] Iteration 163600, loss = 0.24808
I0430 09:01:32.438761   966 solver.cpp:244]     Train net output #0: loss = 0.24808 (* 1 = 0.24808 loss)
I0430 09:01:32.438769   966 sgd_solver.cpp:106] Iteration 163600, lr = 1e-23
I0430 09:02:22.064019   966 solver.cpp:228] Iteration 163700, loss = 0.083849
I0430 09:02:22.064177   966 solver.cpp:244]     Train net output #0: loss = 0.083849 (* 1 = 0.083849 loss)
I0430 09:02:22.064183   966 sgd_solver.cpp:106] Iteration 163700, lr = 1e-23
I0430 09:03:11.700300   966 solver.cpp:228] Iteration 163800, loss = 0.0645217
I0430 09:03:11.700462   966 solver.cpp:244]     Train net output #0: loss = 0.0645217 (* 1 = 0.0645217 loss)
I0430 09:03:11.700469   966 sgd_solver.cpp:106] Iteration 163800, lr = 1e-23
I0430 09:04:01.338299   966 solver.cpp:228] Iteration 163900, loss = 0.130826
I0430 09:04:01.338451   966 solver.cpp:244]     Train net output #0: loss = 0.130826 (* 1 = 0.130826 loss)
I0430 09:04:01.338457   966 sgd_solver.cpp:106] Iteration 163900, lr = 1e-23
I0430 09:04:52.672382   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_164000.caffemodel
I0430 09:05:15.976615   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_164000.solverstate
I0430 09:05:16.167673   966 solver.cpp:337] Iteration 164000, Testing net (#0)
I0430 09:05:16.167795   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 09:05:16.167816   966 net.cpp:693] Ignoring source layer visualize
I0430 09:05:16.167819   966 net.cpp:693] Ignoring source layer fake
I0430 09:10:09.301115   966 solver.cpp:404]     Test net output #0: loss = 0.182968 (* 1 = 0.182968 loss)
I0430 09:10:09.611378   966 solver.cpp:228] Iteration 164000, loss = 0.192902
I0430 09:10:09.611395   966 solver.cpp:244]     Train net output #0: loss = 0.192902 (* 1 = 0.192902 loss)
I0430 09:10:09.611419   966 sgd_solver.cpp:106] Iteration 164000, lr = 1e-23
I0430 09:10:59.240280   966 solver.cpp:228] Iteration 164100, loss = 0.149558
I0430 09:10:59.240437   966 solver.cpp:244]     Train net output #0: loss = 0.149558 (* 1 = 0.149558 loss)
I0430 09:10:59.240443   966 sgd_solver.cpp:106] Iteration 164100, lr = 1e-23
I0430 09:11:48.876425   966 solver.cpp:228] Iteration 164200, loss = 0.202322
I0430 09:11:48.876615   966 solver.cpp:244]     Train net output #0: loss = 0.202322 (* 1 = 0.202322 loss)
I0430 09:11:48.876622   966 sgd_solver.cpp:106] Iteration 164200, lr = 1e-23
I0430 09:12:40.343374   966 solver.cpp:228] Iteration 164300, loss = 0.102646
I0430 09:12:40.343569   966 solver.cpp:244]     Train net output #0: loss = 0.102646 (* 1 = 0.102646 loss)
I0430 09:12:40.343576   966 sgd_solver.cpp:106] Iteration 164300, lr = 1e-23
I0430 09:13:29.970414   966 solver.cpp:228] Iteration 164400, loss = 0.109956
I0430 09:13:29.970577   966 solver.cpp:244]     Train net output #0: loss = 0.109956 (* 1 = 0.109956 loss)
I0430 09:13:29.970583   966 sgd_solver.cpp:106] Iteration 164400, lr = 1e-23
I0430 09:14:19.598975   966 solver.cpp:228] Iteration 164500, loss = 0.116018
I0430 09:14:19.599146   966 solver.cpp:244]     Train net output #0: loss = 0.116018 (* 1 = 0.116018 loss)
I0430 09:14:19.599153   966 sgd_solver.cpp:106] Iteration 164500, lr = 1e-23
I0430 09:15:09.232321   966 solver.cpp:228] Iteration 164600, loss = 0.115185
I0430 09:15:09.232496   966 solver.cpp:244]     Train net output #0: loss = 0.115185 (* 1 = 0.115185 loss)
I0430 09:15:09.232502   966 sgd_solver.cpp:106] Iteration 164600, lr = 1e-23
I0430 09:16:00.648157   966 solver.cpp:228] Iteration 164700, loss = 0.105144
I0430 09:16:00.648322   966 solver.cpp:244]     Train net output #0: loss = 0.105144 (* 1 = 0.105144 loss)
I0430 09:16:00.648329   966 sgd_solver.cpp:106] Iteration 164700, lr = 1e-23
I0430 09:16:50.267266   966 solver.cpp:228] Iteration 164800, loss = 0.117086
I0430 09:16:50.267432   966 solver.cpp:244]     Train net output #0: loss = 0.117086 (* 1 = 0.117086 loss)
I0430 09:16:50.267438   966 sgd_solver.cpp:106] Iteration 164800, lr = 1e-23
I0430 09:17:39.885880   966 solver.cpp:228] Iteration 164900, loss = 0.101485
I0430 09:17:39.886044   966 solver.cpp:244]     Train net output #0: loss = 0.101485 (* 1 = 0.101485 loss)
I0430 09:17:39.886051   966 sgd_solver.cpp:106] Iteration 164900, lr = 1e-23
I0430 09:18:31.028910   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_165000.caffemodel
I0430 09:19:32.778252   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_165000.solverstate
I0430 09:19:32.985079   966 solver.cpp:337] Iteration 165000, Testing net (#0)
I0430 09:19:32.985200   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 09:19:32.985221   966 net.cpp:693] Ignoring source layer visualize
I0430 09:19:32.985224   966 net.cpp:693] Ignoring source layer fake
I0430 09:24:25.300175   966 solver.cpp:404]     Test net output #0: loss = 0.178835 (* 1 = 0.178835 loss)
I0430 09:24:25.610055   966 solver.cpp:228] Iteration 165000, loss = 0.126152
I0430 09:24:25.610076   966 solver.cpp:244]     Train net output #0: loss = 0.126152 (* 1 = 0.126152 loss)
I0430 09:24:25.610100   966 sgd_solver.cpp:106] Iteration 165000, lr = 1e-24
I0430 09:25:15.224684   966 solver.cpp:228] Iteration 165100, loss = 0.190407
I0430 09:25:15.224833   966 solver.cpp:244]     Train net output #0: loss = 0.190407 (* 1 = 0.190407 loss)
I0430 09:25:15.224839   966 sgd_solver.cpp:106] Iteration 165100, lr = 1e-24
I0430 09:26:04.840626   966 solver.cpp:228] Iteration 165200, loss = 0.103993
I0430 09:26:04.840796   966 solver.cpp:244]     Train net output #0: loss = 0.103993 (* 1 = 0.103993 loss)
I0430 09:26:04.840802   966 sgd_solver.cpp:106] Iteration 165200, lr = 1e-24
I0430 09:26:56.085849   966 solver.cpp:228] Iteration 165300, loss = 0.060266
I0430 09:26:56.086689   966 solver.cpp:244]     Train net output #0: loss = 0.060266 (* 1 = 0.060266 loss)
I0430 09:26:56.086711   966 sgd_solver.cpp:106] Iteration 165300, lr = 1e-24
I0430 09:27:45.697553   966 solver.cpp:228] Iteration 165400, loss = 0.0579879
I0430 09:27:45.697721   966 solver.cpp:244]     Train net output #0: loss = 0.0579879 (* 1 = 0.0579879 loss)
I0430 09:27:45.697727   966 sgd_solver.cpp:106] Iteration 165400, lr = 1e-24
I0430 09:28:35.313133   966 solver.cpp:228] Iteration 165500, loss = 0.160143
I0430 09:28:35.313295   966 solver.cpp:244]     Train net output #0: loss = 0.160143 (* 1 = 0.160143 loss)
I0430 09:28:35.313302   966 sgd_solver.cpp:106] Iteration 165500, lr = 1e-24
I0430 09:29:26.215900   966 solver.cpp:228] Iteration 165600, loss = 0.169603
I0430 09:29:26.216055   966 solver.cpp:244]     Train net output #0: loss = 0.169603 (* 1 = 0.169603 loss)
I0430 09:29:26.216063   966 sgd_solver.cpp:106] Iteration 165600, lr = 1e-24
I0430 09:30:15.835232   966 solver.cpp:228] Iteration 165700, loss = 0.171052
I0430 09:30:15.835381   966 solver.cpp:244]     Train net output #0: loss = 0.171052 (* 1 = 0.171052 loss)
I0430 09:30:15.835387   966 sgd_solver.cpp:106] Iteration 165700, lr = 1e-24
I0430 09:31:05.467466   966 solver.cpp:228] Iteration 165800, loss = 0.123512
I0430 09:31:05.467648   966 solver.cpp:244]     Train net output #0: loss = 0.123512 (* 1 = 0.123512 loss)
I0430 09:31:05.467655   966 sgd_solver.cpp:106] Iteration 165800, lr = 1e-24
I0430 09:31:56.413149   966 solver.cpp:228] Iteration 165900, loss = 0.142986
I0430 09:31:56.413307   966 solver.cpp:244]     Train net output #0: loss = 0.142986 (* 1 = 0.142986 loss)
I0430 09:31:56.413314   966 sgd_solver.cpp:106] Iteration 165900, lr = 1e-24
I0430 09:32:45.737144   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_166000.caffemodel
I0430 09:33:14.757419   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_166000.solverstate
I0430 09:33:14.950073   966 solver.cpp:337] Iteration 166000, Testing net (#0)
I0430 09:33:14.950213   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 09:33:14.950220   966 net.cpp:693] Ignoring source layer visualize
I0430 09:33:14.950223   966 net.cpp:693] Ignoring source layer fake
I0430 09:38:07.971168   966 solver.cpp:404]     Test net output #0: loss = 0.184777 (* 1 = 0.184777 loss)
I0430 09:38:08.281775   966 solver.cpp:228] Iteration 166000, loss = 0.107344
I0430 09:38:08.281810   966 solver.cpp:244]     Train net output #0: loss = 0.107344 (* 1 = 0.107344 loss)
I0430 09:38:08.281817   966 sgd_solver.cpp:106] Iteration 166000, lr = 1e-24
I0430 09:38:59.631239   966 solver.cpp:228] Iteration 166100, loss = 0.28122
I0430 09:38:59.631403   966 solver.cpp:244]     Train net output #0: loss = 0.28122 (* 1 = 0.28122 loss)
I0430 09:38:59.631412   966 sgd_solver.cpp:106] Iteration 166100, lr = 1e-24
I0430 09:39:49.259470   966 solver.cpp:228] Iteration 166200, loss = 0.0803072
I0430 09:39:49.259646   966 solver.cpp:244]     Train net output #0: loss = 0.0803072 (* 1 = 0.0803072 loss)
I0430 09:39:49.259654   966 sgd_solver.cpp:106] Iteration 166200, lr = 1e-24
I0430 09:40:38.893331   966 solver.cpp:228] Iteration 166300, loss = 0.127603
I0430 09:40:38.893522   966 solver.cpp:244]     Train net output #0: loss = 0.127603 (* 1 = 0.127603 loss)
I0430 09:40:38.893528   966 sgd_solver.cpp:106] Iteration 166300, lr = 1e-24
I0430 09:41:30.322459   966 solver.cpp:228] Iteration 166400, loss = 0.215466
I0430 09:41:30.322623   966 solver.cpp:244]     Train net output #0: loss = 0.215466 (* 1 = 0.215466 loss)
I0430 09:41:30.322629   966 sgd_solver.cpp:106] Iteration 166400, lr = 1e-24
I0430 09:42:19.948277   966 solver.cpp:228] Iteration 166500, loss = 0.129401
I0430 09:42:19.948441   966 solver.cpp:244]     Train net output #0: loss = 0.129401 (* 1 = 0.129401 loss)
I0430 09:42:19.948447   966 sgd_solver.cpp:106] Iteration 166500, lr = 1e-24
I0430 09:43:09.587256   966 solver.cpp:228] Iteration 166600, loss = 0.108509
I0430 09:43:09.587422   966 solver.cpp:244]     Train net output #0: loss = 0.108509 (* 1 = 0.108509 loss)
I0430 09:43:09.587429   966 sgd_solver.cpp:106] Iteration 166600, lr = 1e-24
I0430 09:43:59.228312   966 solver.cpp:228] Iteration 166700, loss = 0.229427
I0430 09:43:59.228466   966 solver.cpp:244]     Train net output #0: loss = 0.229427 (* 1 = 0.229427 loss)
I0430 09:43:59.228478   966 sgd_solver.cpp:106] Iteration 166700, lr = 1e-24
I0430 09:44:50.705364   966 solver.cpp:228] Iteration 166800, loss = 0.164174
I0430 09:44:50.705520   966 solver.cpp:244]     Train net output #0: loss = 0.164174 (* 1 = 0.164174 loss)
I0430 09:44:50.705528   966 sgd_solver.cpp:106] Iteration 166800, lr = 1e-24
I0430 09:45:40.334332   966 solver.cpp:228] Iteration 166900, loss = 0.171884
I0430 09:45:40.336410   966 solver.cpp:244]     Train net output #0: loss = 0.171884 (* 1 = 0.171884 loss)
I0430 09:45:40.336416   966 sgd_solver.cpp:106] Iteration 166900, lr = 1e-24
I0430 09:46:29.666810   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_167000.caffemodel
I0430 09:46:56.147714   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_167000.solverstate
I0430 09:46:56.339099   966 solver.cpp:337] Iteration 167000, Testing net (#0)
I0430 09:46:56.339218   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 09:46:56.339238   966 net.cpp:693] Ignoring source layer visualize
I0430 09:46:56.339241   966 net.cpp:693] Ignoring source layer fake
I0430 09:51:49.769819   966 solver.cpp:404]     Test net output #0: loss = 0.191728 (* 1 = 0.191728 loss)
I0430 09:51:50.079612   966 solver.cpp:228] Iteration 167000, loss = 0.104087
I0430 09:51:50.079649   966 solver.cpp:244]     Train net output #0: loss = 0.104087 (* 1 = 0.104087 loss)
I0430 09:51:50.079656   966 sgd_solver.cpp:106] Iteration 167000, lr = 1e-24
I0430 09:52:41.686259   966 solver.cpp:228] Iteration 167100, loss = 0.151456
I0430 09:52:41.686414   966 solver.cpp:244]     Train net output #0: loss = 0.151456 (* 1 = 0.151456 loss)
I0430 09:52:41.686421   966 sgd_solver.cpp:106] Iteration 167100, lr = 1e-24
I0430 09:53:31.317196   966 solver.cpp:228] Iteration 167200, loss = 0.286495
I0430 09:53:31.317358   966 solver.cpp:244]     Train net output #0: loss = 0.286495 (* 1 = 0.286495 loss)
I0430 09:53:31.317365   966 sgd_solver.cpp:106] Iteration 167200, lr = 1e-24
I0430 09:54:20.960141   966 solver.cpp:228] Iteration 167300, loss = 0.22454
I0430 09:54:20.960291   966 solver.cpp:244]     Train net output #0: loss = 0.22454 (* 1 = 0.22454 loss)
I0430 09:54:20.960297   966 sgd_solver.cpp:106] Iteration 167300, lr = 1e-24
I0430 09:55:10.580049   966 solver.cpp:228] Iteration 167400, loss = 0.267643
I0430 09:55:10.581511   966 solver.cpp:244]     Train net output #0: loss = 0.267643 (* 1 = 0.267643 loss)
I0430 09:55:10.581518   966 sgd_solver.cpp:106] Iteration 167400, lr = 1e-24
I0430 09:56:01.987329   966 solver.cpp:228] Iteration 167500, loss = 0.0915738
I0430 09:56:01.987498   966 solver.cpp:244]     Train net output #0: loss = 0.0915738 (* 1 = 0.0915738 loss)
I0430 09:56:01.987504   966 sgd_solver.cpp:106] Iteration 167500, lr = 1e-24
I0430 09:56:51.620303   966 solver.cpp:228] Iteration 167600, loss = 0.139147
I0430 09:56:51.621743   966 solver.cpp:244]     Train net output #0: loss = 0.139147 (* 1 = 0.139147 loss)
I0430 09:56:51.621752   966 sgd_solver.cpp:106] Iteration 167600, lr = 1e-24
I0430 09:57:41.254513   966 solver.cpp:228] Iteration 167700, loss = 0.106353
I0430 09:57:41.254669   966 solver.cpp:244]     Train net output #0: loss = 0.106353 (* 1 = 0.106353 loss)
I0430 09:57:41.254676   966 sgd_solver.cpp:106] Iteration 167700, lr = 1e-24
I0430 09:58:32.650465   966 solver.cpp:228] Iteration 167800, loss = 0.117879
I0430 09:58:32.650625   966 solver.cpp:244]     Train net output #0: loss = 0.117879 (* 1 = 0.117879 loss)
I0430 09:58:32.650632   966 sgd_solver.cpp:106] Iteration 167800, lr = 1e-24
I0430 09:59:22.267329   966 solver.cpp:228] Iteration 167900, loss = 0.174512
I0430 09:59:22.267474   966 solver.cpp:244]     Train net output #0: loss = 0.174512 (* 1 = 0.174512 loss)
I0430 09:59:22.267480   966 sgd_solver.cpp:106] Iteration 167900, lr = 1e-24
I0430 10:00:11.576741   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_168000.caffemodel
I0430 10:00:42.703303   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_168000.solverstate
I0430 10:00:42.905022   966 solver.cpp:337] Iteration 168000, Testing net (#0)
I0430 10:00:42.905159   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 10:00:42.905165   966 net.cpp:693] Ignoring source layer visualize
I0430 10:00:42.905167   966 net.cpp:693] Ignoring source layer fake
I0430 10:05:36.190973   966 solver.cpp:404]     Test net output #0: loss = 0.183164 (* 1 = 0.183164 loss)
I0430 10:05:36.502871   966 solver.cpp:228] Iteration 168000, loss = 0.108729
I0430 10:05:36.502907   966 solver.cpp:244]     Train net output #0: loss = 0.108729 (* 1 = 0.108729 loss)
I0430 10:05:36.502914   966 sgd_solver.cpp:106] Iteration 168000, lr = 1e-24
I0430 10:06:27.890215   966 solver.cpp:228] Iteration 168100, loss = 0.103537
I0430 10:06:27.890414   966 solver.cpp:244]     Train net output #0: loss = 0.103537 (* 1 = 0.103537 loss)
I0430 10:06:27.890422   966 sgd_solver.cpp:106] Iteration 168100, lr = 1e-24
I0430 10:07:17.503849   966 solver.cpp:228] Iteration 168200, loss = 0.0876125
I0430 10:07:17.504011   966 solver.cpp:244]     Train net output #0: loss = 0.0876125 (* 1 = 0.0876125 loss)
I0430 10:07:17.504019   966 sgd_solver.cpp:106] Iteration 168200, lr = 1e-24
I0430 10:08:07.113888   966 solver.cpp:228] Iteration 168300, loss = 0.0971901
I0430 10:08:07.114080   966 solver.cpp:244]     Train net output #0: loss = 0.0971901 (* 1 = 0.0971901 loss)
I0430 10:08:07.114087   966 sgd_solver.cpp:106] Iteration 168300, lr = 1e-24
I0430 10:08:56.726246   966 solver.cpp:228] Iteration 168400, loss = 0.140435
I0430 10:08:56.726409   966 solver.cpp:244]     Train net output #0: loss = 0.140435 (* 1 = 0.140435 loss)
I0430 10:08:56.726416   966 sgd_solver.cpp:106] Iteration 168400, lr = 1e-24
I0430 10:09:47.964437   966 solver.cpp:228] Iteration 168500, loss = 0.108294
I0430 10:09:47.964609   966 solver.cpp:244]     Train net output #0: loss = 0.108294 (* 1 = 0.108294 loss)
I0430 10:09:47.964615   966 sgd_solver.cpp:106] Iteration 168500, lr = 1e-24
I0430 10:10:37.580374   966 solver.cpp:228] Iteration 168600, loss = 0.197213
I0430 10:10:37.580535   966 solver.cpp:244]     Train net output #0: loss = 0.197213 (* 1 = 0.197213 loss)
I0430 10:10:37.580541   966 sgd_solver.cpp:106] Iteration 168600, lr = 1e-24
I0430 10:11:27.205557   966 solver.cpp:228] Iteration 168700, loss = 0.0902629
I0430 10:11:27.205759   966 solver.cpp:244]     Train net output #0: loss = 0.0902629 (* 1 = 0.0902629 loss)
I0430 10:11:27.205766   966 sgd_solver.cpp:106] Iteration 168700, lr = 1e-24
I0430 10:12:18.069967   966 solver.cpp:228] Iteration 168800, loss = 0.13083
I0430 10:12:18.070147   966 solver.cpp:244]     Train net output #0: loss = 0.13083 (* 1 = 0.13083 loss)
I0430 10:12:18.070152   966 sgd_solver.cpp:106] Iteration 168800, lr = 1e-24
I0430 10:13:07.701860   966 solver.cpp:228] Iteration 168900, loss = 0.178387
I0430 10:13:07.702033   966 solver.cpp:244]     Train net output #0: loss = 0.178387 (* 1 = 0.178387 loss)
I0430 10:13:07.702039   966 sgd_solver.cpp:106] Iteration 168900, lr = 1e-24
I0430 10:13:58.340548   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_169000.caffemodel
I0430 10:14:09.671481   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_169000.solverstate
I0430 10:14:09.859731   966 solver.cpp:337] Iteration 169000, Testing net (#0)
I0430 10:14:09.859853   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 10:14:09.859859   966 net.cpp:693] Ignoring source layer visualize
I0430 10:14:09.859863   966 net.cpp:693] Ignoring source layer fake
I0430 10:19:02.947753   966 solver.cpp:404]     Test net output #0: loss = 0.182656 (* 1 = 0.182656 loss)
I0430 10:19:03.260285   966 solver.cpp:228] Iteration 169000, loss = 0.188587
I0430 10:19:03.260303   966 solver.cpp:244]     Train net output #0: loss = 0.188587 (* 1 = 0.188587 loss)
I0430 10:19:03.260326   966 sgd_solver.cpp:106] Iteration 169000, lr = 1e-24
I0430 10:19:52.897753   966 solver.cpp:228] Iteration 169100, loss = 0.100042
I0430 10:19:52.897912   966 solver.cpp:244]     Train net output #0: loss = 0.100042 (* 1 = 0.100042 loss)
I0430 10:19:52.897918   966 sgd_solver.cpp:106] Iteration 169100, lr = 1e-24
I0430 10:20:42.531018   966 solver.cpp:228] Iteration 169200, loss = 0.155432
I0430 10:20:42.531213   966 solver.cpp:244]     Train net output #0: loss = 0.155432 (* 1 = 0.155432 loss)
I0430 10:20:42.531221   966 sgd_solver.cpp:106] Iteration 169200, lr = 1e-24
I0430 10:21:33.745065   966 solver.cpp:228] Iteration 169300, loss = 0.115113
I0430 10:21:33.745234   966 solver.cpp:244]     Train net output #0: loss = 0.115113 (* 1 = 0.115113 loss)
I0430 10:21:33.745241   966 sgd_solver.cpp:106] Iteration 169300, lr = 1e-24
I0430 10:22:23.380226   966 solver.cpp:228] Iteration 169400, loss = 0.0709771
I0430 10:22:23.380435   966 solver.cpp:244]     Train net output #0: loss = 0.0709771 (* 1 = 0.0709771 loss)
I0430 10:22:23.380445   966 sgd_solver.cpp:106] Iteration 169400, lr = 1e-24
I0430 10:23:13.012521   966 solver.cpp:228] Iteration 169500, loss = 0.0756697
I0430 10:23:13.012687   966 solver.cpp:244]     Train net output #0: loss = 0.0756697 (* 1 = 0.0756697 loss)
I0430 10:23:13.012693   966 sgd_solver.cpp:106] Iteration 169500, lr = 1e-24
I0430 10:24:04.584031   966 solver.cpp:228] Iteration 169600, loss = 0.181748
I0430 10:24:04.584193   966 solver.cpp:244]     Train net output #0: loss = 0.181748 (* 1 = 0.181748 loss)
I0430 10:24:04.584200   966 sgd_solver.cpp:106] Iteration 169600, lr = 1e-24
I0430 10:24:54.208827   966 solver.cpp:228] Iteration 169700, loss = 0.173585
I0430 10:24:54.208984   966 solver.cpp:244]     Train net output #0: loss = 0.173585 (* 1 = 0.173585 loss)
I0430 10:24:54.208991   966 sgd_solver.cpp:106] Iteration 169700, lr = 1e-24
I0430 10:25:43.844310   966 solver.cpp:228] Iteration 169800, loss = 0.0420669
I0430 10:25:43.844465   966 solver.cpp:244]     Train net output #0: loss = 0.0420669 (* 1 = 0.0420669 loss)
I0430 10:25:43.844471   966 sgd_solver.cpp:106] Iteration 169800, lr = 1e-24
I0430 10:26:35.591986   966 solver.cpp:228] Iteration 169900, loss = 0.0920485
I0430 10:26:35.592161   966 solver.cpp:244]     Train net output #0: loss = 0.0920485 (* 1 = 0.0920485 loss)
I0430 10:26:35.592169   966 sgd_solver.cpp:106] Iteration 169900, lr = 1e-24
I0430 10:27:24.903288   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_170000.caffemodel
I0430 10:27:51.859895   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_170000.solverstate
I0430 10:27:52.048648   966 solver.cpp:337] Iteration 170000, Testing net (#0)
I0430 10:27:52.048771   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 10:27:52.048779   966 net.cpp:693] Ignoring source layer visualize
I0430 10:27:52.048780   966 net.cpp:693] Ignoring source layer fake
I0430 10:32:44.432781   966 solver.cpp:404]     Test net output #0: loss = 0.178669 (* 1 = 0.178669 loss)
I0430 10:32:44.743134   966 solver.cpp:228] Iteration 170000, loss = 0.0683077
I0430 10:32:44.743154   966 solver.cpp:244]     Train net output #0: loss = 0.0683077 (* 1 = 0.0683077 loss)
I0430 10:32:44.743176   966 sgd_solver.cpp:106] Iteration 170000, lr = 1e-24
I0430 10:33:34.379717   966 solver.cpp:228] Iteration 170100, loss = 0.170393
I0430 10:33:34.379889   966 solver.cpp:244]     Train net output #0: loss = 0.170393 (* 1 = 0.170393 loss)
I0430 10:33:34.379897   966 sgd_solver.cpp:106] Iteration 170100, lr = 1e-24
I0430 10:34:26.041874   966 solver.cpp:228] Iteration 170200, loss = 0.175511
I0430 10:34:26.042039   966 solver.cpp:244]     Train net output #0: loss = 0.175511 (* 1 = 0.175511 loss)
I0430 10:34:26.042047   966 sgd_solver.cpp:106] Iteration 170200, lr = 1e-24
I0430 10:35:15.676041   966 solver.cpp:228] Iteration 170300, loss = 0.224721
I0430 10:35:15.677522   966 solver.cpp:244]     Train net output #0: loss = 0.224721 (* 1 = 0.224721 loss)
I0430 10:35:15.677531   966 sgd_solver.cpp:106] Iteration 170300, lr = 1e-24
I0430 10:36:05.304606   966 solver.cpp:228] Iteration 170400, loss = 0.114212
I0430 10:36:05.304769   966 solver.cpp:244]     Train net output #0: loss = 0.114212 (* 1 = 0.114212 loss)
I0430 10:36:05.304788   966 sgd_solver.cpp:106] Iteration 170400, lr = 1e-24
I0430 10:36:54.947417   966 solver.cpp:228] Iteration 170500, loss = 0.107877
I0430 10:36:54.947585   966 solver.cpp:244]     Train net output #0: loss = 0.107877 (* 1 = 0.107877 loss)
I0430 10:36:54.947594   966 sgd_solver.cpp:106] Iteration 170500, lr = 1e-24
I0430 10:37:46.506978   966 solver.cpp:228] Iteration 170600, loss = 0.10257
I0430 10:37:46.508697   966 solver.cpp:244]     Train net output #0: loss = 0.10257 (* 1 = 0.10257 loss)
I0430 10:37:46.508703   966 sgd_solver.cpp:106] Iteration 170600, lr = 1e-24
I0430 10:38:36.126935   966 solver.cpp:228] Iteration 170700, loss = 0.221653
I0430 10:38:36.128290   966 solver.cpp:244]     Train net output #0: loss = 0.221653 (* 1 = 0.221653 loss)
I0430 10:38:36.128296   966 sgd_solver.cpp:106] Iteration 170700, lr = 1e-24
I0430 10:39:25.759852   966 solver.cpp:228] Iteration 170800, loss = 0.12692
I0430 10:39:25.760010   966 solver.cpp:244]     Train net output #0: loss = 0.12692 (* 1 = 0.12692 loss)
I0430 10:39:25.760016   966 sgd_solver.cpp:106] Iteration 170800, lr = 1e-24
I0430 10:40:17.370652   966 solver.cpp:228] Iteration 170900, loss = 0.104421
I0430 10:40:17.371006   966 solver.cpp:244]     Train net output #0: loss = 0.104421 (* 1 = 0.104421 loss)
I0430 10:40:17.371016   966 sgd_solver.cpp:106] Iteration 170900, lr = 1e-24
I0430 10:41:06.666911   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_171000.caffemodel
I0430 10:41:15.039465   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_171000.solverstate
I0430 10:41:15.245103   966 solver.cpp:337] Iteration 171000, Testing net (#0)
I0430 10:41:15.245249   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 10:41:15.245254   966 net.cpp:693] Ignoring source layer visualize
I0430 10:41:15.245256   966 net.cpp:693] Ignoring source layer fake
I0430 10:46:10.085961   966 solver.cpp:404]     Test net output #0: loss = 0.18676 (* 1 = 0.18676 loss)
I0430 10:46:10.396296   966 solver.cpp:228] Iteration 171000, loss = 0.0655654
I0430 10:46:10.396334   966 solver.cpp:244]     Train net output #0: loss = 0.0655654 (* 1 = 0.0655654 loss)
I0430 10:46:10.396342   966 sgd_solver.cpp:106] Iteration 171000, lr = 1e-24
I0430 10:47:00.010771   966 solver.cpp:228] Iteration 171100, loss = 0.037313
I0430 10:47:00.010924   966 solver.cpp:244]     Train net output #0: loss = 0.037313 (* 1 = 0.037313 loss)
I0430 10:47:00.010931   966 sgd_solver.cpp:106] Iteration 171100, lr = 1e-24
I0430 10:47:51.507166   966 solver.cpp:228] Iteration 171200, loss = 0.193296
I0430 10:47:51.507359   966 solver.cpp:244]     Train net output #0: loss = 0.193296 (* 1 = 0.193296 loss)
I0430 10:47:51.507367   966 sgd_solver.cpp:106] Iteration 171200, lr = 1e-24
I0430 10:48:41.106184   966 solver.cpp:228] Iteration 171300, loss = 0.121819
I0430 10:48:41.106348   966 solver.cpp:244]     Train net output #0: loss = 0.121819 (* 1 = 0.121819 loss)
I0430 10:48:41.106355   966 sgd_solver.cpp:106] Iteration 171300, lr = 1e-24
I0430 10:49:30.720232   966 solver.cpp:228] Iteration 171400, loss = 0.109694
I0430 10:49:30.720402   966 solver.cpp:244]     Train net output #0: loss = 0.109694 (* 1 = 0.109694 loss)
I0430 10:49:30.720409   966 sgd_solver.cpp:106] Iteration 171400, lr = 1e-24
I0430 10:50:20.343255   966 solver.cpp:228] Iteration 171500, loss = 0.13794
I0430 10:50:20.343402   966 solver.cpp:244]     Train net output #0: loss = 0.13794 (* 1 = 0.13794 loss)
I0430 10:50:20.343408   966 sgd_solver.cpp:106] Iteration 171500, lr = 1e-24
I0430 10:51:11.709735   966 solver.cpp:228] Iteration 171600, loss = 0.0888965
I0430 10:51:11.709887   966 solver.cpp:244]     Train net output #0: loss = 0.0888965 (* 1 = 0.0888965 loss)
I0430 10:51:11.709895   966 sgd_solver.cpp:106] Iteration 171600, lr = 1e-24
I0430 10:52:01.320675   966 solver.cpp:228] Iteration 171700, loss = 0.0998132
I0430 10:52:01.320924   966 solver.cpp:244]     Train net output #0: loss = 0.0998132 (* 1 = 0.0998132 loss)
I0430 10:52:01.320932   966 sgd_solver.cpp:106] Iteration 171700, lr = 1e-24
I0430 10:52:50.945361   966 solver.cpp:228] Iteration 171800, loss = 0.118725
I0430 10:52:50.945523   966 solver.cpp:244]     Train net output #0: loss = 0.118725 (* 1 = 0.118725 loss)
I0430 10:52:50.945529   966 sgd_solver.cpp:106] Iteration 171800, lr = 1e-24
I0430 10:53:41.888604   966 solver.cpp:228] Iteration 171900, loss = 0.292791
I0430 10:53:41.888787   966 solver.cpp:244]     Train net output #0: loss = 0.292791 (* 1 = 0.292791 loss)
I0430 10:53:41.888794   966 sgd_solver.cpp:106] Iteration 171900, lr = 1e-24
I0430 10:54:31.197118   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_172000.caffemodel
I0430 10:55:01.649775   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_172000.solverstate
I0430 10:55:01.854991   966 solver.cpp:337] Iteration 172000, Testing net (#0)
I0430 10:55:01.855131   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 10:55:01.855137   966 net.cpp:693] Ignoring source layer visualize
I0430 10:55:01.855140   966 net.cpp:693] Ignoring source layer fake
I0430 10:59:54.547974   966 solver.cpp:404]     Test net output #0: loss = 0.190824 (* 1 = 0.190824 loss)
I0430 10:59:54.858341   966 solver.cpp:228] Iteration 172000, loss = 0.113053
I0430 10:59:54.858363   966 solver.cpp:244]     Train net output #0: loss = 0.113053 (* 1 = 0.113053 loss)
I0430 10:59:54.858386   966 sgd_solver.cpp:106] Iteration 172000, lr = 1e-24
I0430 11:00:45.857909   966 solver.cpp:228] Iteration 172100, loss = 0.120599
I0430 11:00:45.858090   966 solver.cpp:244]     Train net output #0: loss = 0.120599 (* 1 = 0.120599 loss)
I0430 11:00:45.858098   966 sgd_solver.cpp:106] Iteration 172100, lr = 1e-24
I0430 11:01:35.488159   966 solver.cpp:228] Iteration 172200, loss = 0.114676
I0430 11:01:35.488344   966 solver.cpp:244]     Train net output #0: loss = 0.114676 (* 1 = 0.114676 loss)
I0430 11:01:35.488353   966 sgd_solver.cpp:106] Iteration 172200, lr = 1e-24
I0430 11:02:25.113101   966 solver.cpp:228] Iteration 172300, loss = 0.168321
I0430 11:02:25.113261   966 solver.cpp:244]     Train net output #0: loss = 0.168321 (* 1 = 0.168321 loss)
I0430 11:02:25.113270   966 sgd_solver.cpp:106] Iteration 172300, lr = 1e-24
I0430 11:03:16.478423   966 solver.cpp:228] Iteration 172400, loss = 0.0676048
I0430 11:03:16.478572   966 solver.cpp:244]     Train net output #0: loss = 0.0676048 (* 1 = 0.0676048 loss)
I0430 11:03:16.478579   966 sgd_solver.cpp:106] Iteration 172400, lr = 1e-24
I0430 11:04:06.109911   966 solver.cpp:228] Iteration 172500, loss = 0.112313
I0430 11:04:06.110076   966 solver.cpp:244]     Train net output #0: loss = 0.112313 (* 1 = 0.112313 loss)
I0430 11:04:06.110083   966 sgd_solver.cpp:106] Iteration 172500, lr = 1e-25
I0430 11:04:55.742575   966 solver.cpp:228] Iteration 172600, loss = 0.30778
I0430 11:04:55.742753   966 solver.cpp:244]     Train net output #0: loss = 0.30778 (* 1 = 0.30778 loss)
I0430 11:04:55.742761   966 sgd_solver.cpp:106] Iteration 172600, lr = 1e-25
I0430 11:05:47.188273   966 solver.cpp:228] Iteration 172700, loss = 0.215357
I0430 11:05:47.188448   966 solver.cpp:244]     Train net output #0: loss = 0.215357 (* 1 = 0.215357 loss)
I0430 11:05:47.188458   966 sgd_solver.cpp:106] Iteration 172700, lr = 1e-25
I0430 11:06:36.826323   966 solver.cpp:228] Iteration 172800, loss = 0.0637461
I0430 11:06:36.826472   966 solver.cpp:244]     Train net output #0: loss = 0.0637461 (* 1 = 0.0637461 loss)
I0430 11:06:36.826479   966 sgd_solver.cpp:106] Iteration 172800, lr = 1e-25
I0430 11:07:26.459272   966 solver.cpp:228] Iteration 172900, loss = 0.147045
I0430 11:07:26.459419   966 solver.cpp:244]     Train net output #0: loss = 0.147045 (* 1 = 0.147045 loss)
I0430 11:07:26.459425   966 sgd_solver.cpp:106] Iteration 172900, lr = 1e-25
I0430 11:08:17.712057   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_173000.caffemodel
I0430 11:08:25.392141   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_173000.solverstate
I0430 11:08:25.574998   966 solver.cpp:337] Iteration 173000, Testing net (#0)
I0430 11:08:25.575127   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 11:08:25.575150   966 net.cpp:693] Ignoring source layer visualize
I0430 11:08:25.575152   966 net.cpp:693] Ignoring source layer fake
I0430 11:13:19.880306   966 solver.cpp:404]     Test net output #0: loss = 0.182456 (* 1 = 0.182456 loss)
I0430 11:13:20.190326   966 solver.cpp:228] Iteration 173000, loss = 0.221226
I0430 11:13:20.190346   966 solver.cpp:244]     Train net output #0: loss = 0.221226 (* 1 = 0.221226 loss)
I0430 11:13:20.190367   966 sgd_solver.cpp:106] Iteration 173000, lr = 1e-25
I0430 11:14:09.817641   966 solver.cpp:228] Iteration 173100, loss = 0.148689
I0430 11:14:09.817793   966 solver.cpp:244]     Train net output #0: loss = 0.148689 (* 1 = 0.148689 loss)
I0430 11:14:09.817800   966 sgd_solver.cpp:106] Iteration 173100, lr = 1e-25
I0430 11:14:59.455443   966 solver.cpp:228] Iteration 173200, loss = 0.0626999
I0430 11:14:59.455600   966 solver.cpp:244]     Train net output #0: loss = 0.0626999 (* 1 = 0.0626999 loss)
I0430 11:14:59.455610   966 sgd_solver.cpp:106] Iteration 173200, lr = 1e-25
I0430 11:15:49.103057   966 solver.cpp:228] Iteration 173300, loss = 0.0994958
I0430 11:15:49.103221   966 solver.cpp:244]     Train net output #0: loss = 0.0994958 (* 1 = 0.0994958 loss)
I0430 11:15:49.103229   966 sgd_solver.cpp:106] Iteration 173300, lr = 1e-25
I0430 11:16:40.676213   966 solver.cpp:228] Iteration 173400, loss = 0.195123
I0430 11:16:40.676367   966 solver.cpp:244]     Train net output #0: loss = 0.195123 (* 1 = 0.195123 loss)
I0430 11:16:40.676374   966 sgd_solver.cpp:106] Iteration 173400, lr = 1e-25
I0430 11:17:30.303995   966 solver.cpp:228] Iteration 173500, loss = 0.185289
I0430 11:17:30.304144   966 solver.cpp:244]     Train net output #0: loss = 0.185289 (* 1 = 0.185289 loss)
I0430 11:17:30.304152   966 sgd_solver.cpp:106] Iteration 173500, lr = 1e-25
I0430 11:18:19.929409   966 solver.cpp:228] Iteration 173600, loss = 0.221079
I0430 11:18:19.929574   966 solver.cpp:244]     Train net output #0: loss = 0.221079 (* 1 = 0.221079 loss)
I0430 11:18:19.929580   966 sgd_solver.cpp:106] Iteration 173600, lr = 1e-25
I0430 11:19:11.470412   966 solver.cpp:228] Iteration 173700, loss = 0.118427
I0430 11:19:11.470574   966 solver.cpp:244]     Train net output #0: loss = 0.118427 (* 1 = 0.118427 loss)
I0430 11:19:11.470584   966 sgd_solver.cpp:106] Iteration 173700, lr = 1e-25
I0430 11:20:01.096343   966 solver.cpp:228] Iteration 173800, loss = 0.129188
I0430 11:20:01.096504   966 solver.cpp:244]     Train net output #0: loss = 0.129188 (* 1 = 0.129188 loss)
I0430 11:20:01.096513   966 sgd_solver.cpp:106] Iteration 173800, lr = 1e-25
I0430 11:20:50.721318   966 solver.cpp:228] Iteration 173900, loss = 0.163263
I0430 11:20:50.721504   966 solver.cpp:244]     Train net output #0: loss = 0.163263 (* 1 = 0.163263 loss)
I0430 11:20:50.721511   966 sgd_solver.cpp:106] Iteration 173900, lr = 1e-25
I0430 11:21:42.159978   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_174000.caffemodel
I0430 11:22:25.750320   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_174000.solverstate
I0430 11:22:25.946313   966 solver.cpp:337] Iteration 174000, Testing net (#0)
I0430 11:22:25.946434   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 11:22:25.946441   966 net.cpp:693] Ignoring source layer visualize
I0430 11:22:25.946444   966 net.cpp:693] Ignoring source layer fake
I0430 11:27:19.223031   966 solver.cpp:404]     Test net output #0: loss = 0.182189 (* 1 = 0.182189 loss)
I0430 11:27:19.535374   966 solver.cpp:228] Iteration 174000, loss = 0.147835
I0430 11:27:19.535418   966 solver.cpp:244]     Train net output #0: loss = 0.147835 (* 1 = 0.147835 loss)
I0430 11:27:19.535425   966 sgd_solver.cpp:106] Iteration 174000, lr = 1e-25
I0430 11:28:09.137943   966 solver.cpp:228] Iteration 174100, loss = 0.0810961
I0430 11:28:09.138193   966 solver.cpp:244]     Train net output #0: loss = 0.0810961 (* 1 = 0.0810961 loss)
I0430 11:28:09.138202   966 sgd_solver.cpp:106] Iteration 174100, lr = 1e-25
I0430 11:28:58.749666   966 solver.cpp:228] Iteration 174200, loss = 0.133505
I0430 11:28:58.749830   966 solver.cpp:244]     Train net output #0: loss = 0.133505 (* 1 = 0.133505 loss)
I0430 11:28:58.749837   966 sgd_solver.cpp:106] Iteration 174200, lr = 1e-25
I0430 11:29:48.358388   966 solver.cpp:228] Iteration 174300, loss = 0.0974281
I0430 11:29:48.358551   966 solver.cpp:244]     Train net output #0: loss = 0.0974281 (* 1 = 0.0974281 loss)
I0430 11:29:48.358558   966 sgd_solver.cpp:106] Iteration 174300, lr = 1e-25
I0430 11:30:39.826091   966 solver.cpp:228] Iteration 174400, loss = 0.143043
I0430 11:30:39.826248   966 solver.cpp:244]     Train net output #0: loss = 0.143043 (* 1 = 0.143043 loss)
I0430 11:30:39.826256   966 sgd_solver.cpp:106] Iteration 174400, lr = 1e-25
I0430 11:31:29.423966   966 solver.cpp:228] Iteration 174500, loss = 0.138464
I0430 11:31:29.424124   966 solver.cpp:244]     Train net output #0: loss = 0.138464 (* 1 = 0.138464 loss)
I0430 11:31:29.424131   966 sgd_solver.cpp:106] Iteration 174500, lr = 1e-25
I0430 11:32:19.031147   966 solver.cpp:228] Iteration 174600, loss = 0.0766343
I0430 11:32:19.032907   966 solver.cpp:244]     Train net output #0: loss = 0.0766343 (* 1 = 0.0766343 loss)
I0430 11:32:19.032922   966 sgd_solver.cpp:106] Iteration 174600, lr = 1e-25
I0430 11:33:10.337023   966 solver.cpp:228] Iteration 174700, loss = 0.0667523
I0430 11:33:10.337189   966 solver.cpp:244]     Train net output #0: loss = 0.0667523 (* 1 = 0.0667523 loss)
I0430 11:33:10.337198   966 sgd_solver.cpp:106] Iteration 174700, lr = 1e-25
I0430 11:33:59.941728   966 solver.cpp:228] Iteration 174800, loss = 0.109475
I0430 11:33:59.943284   966 solver.cpp:244]     Train net output #0: loss = 0.109475 (* 1 = 0.109475 loss)
I0430 11:33:59.943290   966 sgd_solver.cpp:106] Iteration 174800, lr = 1e-25
I0430 11:34:49.542032   966 solver.cpp:228] Iteration 174900, loss = 0.12687
I0430 11:34:49.542203   966 solver.cpp:244]     Train net output #0: loss = 0.12687 (* 1 = 0.12687 loss)
I0430 11:34:49.542212   966 sgd_solver.cpp:106] Iteration 174900, lr = 1e-25
I0430 11:35:40.153429   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_175000.caffemodel
I0430 11:36:16.858502   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_175000.solverstate
I0430 11:36:17.065358   966 solver.cpp:337] Iteration 175000, Testing net (#0)
I0430 11:36:17.065529   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 11:36:17.065536   966 net.cpp:693] Ignoring source layer visualize
I0430 11:36:17.065538   966 net.cpp:693] Ignoring source layer fake
I0430 11:41:09.850407   966 solver.cpp:404]     Test net output #0: loss = 0.179068 (* 1 = 0.179068 loss)
I0430 11:41:10.163275   966 solver.cpp:228] Iteration 175000, loss = 0.157936
I0430 11:41:10.163314   966 solver.cpp:244]     Train net output #0: loss = 0.157936 (* 1 = 0.157936 loss)
I0430 11:41:10.163321   966 sgd_solver.cpp:106] Iteration 175000, lr = 1e-25
I0430 11:41:59.765979   966 solver.cpp:228] Iteration 175100, loss = 0.121752
I0430 11:41:59.766175   966 solver.cpp:244]     Train net output #0: loss = 0.121752 (* 1 = 0.121752 loss)
I0430 11:41:59.766181   966 sgd_solver.cpp:106] Iteration 175100, lr = 1e-25
I0430 11:42:49.377907   966 solver.cpp:228] Iteration 175200, loss = 0.125196
I0430 11:42:49.378083   966 solver.cpp:244]     Train net output #0: loss = 0.125196 (* 1 = 0.125196 loss)
I0430 11:42:49.378092   966 sgd_solver.cpp:106] Iteration 175200, lr = 1e-25
I0430 11:43:40.363955   966 solver.cpp:228] Iteration 175300, loss = 0.0925911
I0430 11:43:40.364118   966 solver.cpp:244]     Train net output #0: loss = 0.0925911 (* 1 = 0.0925911 loss)
I0430 11:43:40.364125   966 sgd_solver.cpp:106] Iteration 175300, lr = 1e-25
I0430 11:44:29.974858   966 solver.cpp:228] Iteration 175400, loss = 0.155227
I0430 11:44:29.975034   966 solver.cpp:244]     Train net output #0: loss = 0.155227 (* 1 = 0.155227 loss)
I0430 11:44:29.975041   966 sgd_solver.cpp:106] Iteration 175400, lr = 1e-25
I0430 11:45:21.167825   966 solver.cpp:228] Iteration 175500, loss = 0.135757
I0430 11:45:21.167978   966 solver.cpp:244]     Train net output #0: loss = 0.135757 (* 1 = 0.135757 loss)
I0430 11:45:21.167985   966 sgd_solver.cpp:106] Iteration 175500, lr = 1e-25
I0430 11:46:10.777256   966 solver.cpp:228] Iteration 175600, loss = 0.106534
I0430 11:46:10.777405   966 solver.cpp:244]     Train net output #0: loss = 0.106534 (* 1 = 0.106534 loss)
I0430 11:46:10.777413   966 sgd_solver.cpp:106] Iteration 175600, lr = 1e-25
I0430 11:47:00.382469   966 solver.cpp:228] Iteration 175700, loss = 0.177574
I0430 11:47:00.382629   966 solver.cpp:244]     Train net output #0: loss = 0.177574 (* 1 = 0.177574 loss)
I0430 11:47:00.382637   966 sgd_solver.cpp:106] Iteration 175700, lr = 1e-25
I0430 11:47:51.965693   966 solver.cpp:228] Iteration 175800, loss = 0.191747
I0430 11:47:51.965867   966 solver.cpp:244]     Train net output #0: loss = 0.191747 (* 1 = 0.191747 loss)
I0430 11:47:51.965874   966 sgd_solver.cpp:106] Iteration 175800, lr = 1e-25
I0430 11:48:41.584851   966 solver.cpp:228] Iteration 175900, loss = 0.10987
I0430 11:48:41.585001   966 solver.cpp:244]     Train net output #0: loss = 0.10987 (* 1 = 0.10987 loss)
I0430 11:48:41.585008   966 sgd_solver.cpp:106] Iteration 175900, lr = 1e-25
I0430 11:49:30.887471   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_176000.caffemodel
I0430 11:49:56.059201   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_176000.solverstate
I0430 11:49:56.256669   966 solver.cpp:337] Iteration 176000, Testing net (#0)
I0430 11:49:56.256796   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 11:49:56.256803   966 net.cpp:693] Ignoring source layer visualize
I0430 11:49:56.256805   966 net.cpp:693] Ignoring source layer fake
I0430 11:54:49.454087   966 solver.cpp:404]     Test net output #0: loss = 0.187928 (* 1 = 0.187928 loss)
I0430 11:54:49.766392   966 solver.cpp:228] Iteration 176000, loss = 0.193571
I0430 11:54:49.766427   966 solver.cpp:244]     Train net output #0: loss = 0.193571 (* 1 = 0.193571 loss)
I0430 11:54:49.766433   966 sgd_solver.cpp:106] Iteration 176000, lr = 1e-25
I0430 11:55:41.305044   966 solver.cpp:228] Iteration 176100, loss = 0.14539
I0430 11:55:41.305203   966 solver.cpp:244]     Train net output #0: loss = 0.14539 (* 1 = 0.14539 loss)
I0430 11:55:41.305210   966 sgd_solver.cpp:106] Iteration 176100, lr = 1e-25
I0430 11:56:30.911259   966 solver.cpp:228] Iteration 176200, loss = 0.148326
I0430 11:56:30.911443   966 solver.cpp:244]     Train net output #0: loss = 0.148326 (* 1 = 0.148326 loss)
I0430 11:56:30.911450   966 sgd_solver.cpp:106] Iteration 176200, lr = 1e-25
I0430 11:57:20.524194   966 solver.cpp:228] Iteration 176300, loss = 0.169643
I0430 11:57:20.524361   966 solver.cpp:244]     Train net output #0: loss = 0.169643 (* 1 = 0.169643 loss)
I0430 11:57:20.524369   966 sgd_solver.cpp:106] Iteration 176300, lr = 1e-25
I0430 11:58:10.138507   966 solver.cpp:228] Iteration 176400, loss = 0.203484
I0430 11:58:10.138670   966 solver.cpp:244]     Train net output #0: loss = 0.203484 (* 1 = 0.203484 loss)
I0430 11:58:10.138677   966 sgd_solver.cpp:106] Iteration 176400, lr = 1e-25
I0430 11:59:01.750929   966 solver.cpp:228] Iteration 176500, loss = 0.225935
I0430 11:59:01.751097   966 solver.cpp:244]     Train net output #0: loss = 0.225935 (* 1 = 0.225935 loss)
I0430 11:59:01.751106   966 sgd_solver.cpp:106] Iteration 176500, lr = 1e-25
I0430 11:59:51.360666   966 solver.cpp:228] Iteration 176600, loss = 0.230443
I0430 11:59:51.360841   966 solver.cpp:244]     Train net output #0: loss = 0.230443 (* 1 = 0.230443 loss)
I0430 11:59:51.360852   966 sgd_solver.cpp:106] Iteration 176600, lr = 1e-25
I0430 12:00:40.978895   966 solver.cpp:228] Iteration 176700, loss = 0.0798412
I0430 12:00:40.979080   966 solver.cpp:244]     Train net output #0: loss = 0.0798412 (* 1 = 0.0798412 loss)
I0430 12:00:40.979089   966 sgd_solver.cpp:106] Iteration 176700, lr = 1e-25
I0430 12:01:32.523252   966 solver.cpp:228] Iteration 176800, loss = 0.119482
I0430 12:01:32.523416   966 solver.cpp:244]     Train net output #0: loss = 0.119482 (* 1 = 0.119482 loss)
I0430 12:01:32.523423   966 sgd_solver.cpp:106] Iteration 176800, lr = 1e-25
I0430 12:02:22.128800   966 solver.cpp:228] Iteration 176900, loss = 0.107329
I0430 12:02:22.128962   966 solver.cpp:244]     Train net output #0: loss = 0.107329 (* 1 = 0.107329 loss)
I0430 12:02:22.128969   966 sgd_solver.cpp:106] Iteration 176900, lr = 1e-25
I0430 12:03:11.430460   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_177000.caffemodel
I0430 12:03:44.751101   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_177000.solverstate
I0430 12:03:44.948660   966 solver.cpp:337] Iteration 177000, Testing net (#0)
I0430 12:03:44.948799   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 12:03:44.948804   966 net.cpp:693] Ignoring source layer visualize
I0430 12:03:44.948807   966 net.cpp:693] Ignoring source layer fake
I0430 12:08:38.345655   966 solver.cpp:404]     Test net output #0: loss = 0.190137 (* 1 = 0.190137 loss)
I0430 12:08:38.656215   966 solver.cpp:228] Iteration 177000, loss = 0.187598
I0430 12:08:38.656257   966 solver.cpp:244]     Train net output #0: loss = 0.187598 (* 1 = 0.187598 loss)
I0430 12:08:38.656265   966 sgd_solver.cpp:106] Iteration 177000, lr = 1e-25
I0430 12:09:28.269222   966 solver.cpp:228] Iteration 177100, loss = 0.152771
I0430 12:09:28.270203   966 solver.cpp:244]     Train net output #0: loss = 0.152771 (* 1 = 0.152771 loss)
I0430 12:09:28.270212   966 sgd_solver.cpp:106] Iteration 177100, lr = 1e-25
I0430 12:10:19.693483   966 solver.cpp:228] Iteration 177200, loss = 0.104949
I0430 12:10:19.693692   966 solver.cpp:244]     Train net output #0: loss = 0.104949 (* 1 = 0.104949 loss)
I0430 12:10:19.693701   966 sgd_solver.cpp:106] Iteration 177200, lr = 1e-25
I0430 12:11:09.305486   966 solver.cpp:228] Iteration 177300, loss = 0.127361
I0430 12:11:09.305652   966 solver.cpp:244]     Train net output #0: loss = 0.127361 (* 1 = 0.127361 loss)
I0430 12:11:09.305660   966 sgd_solver.cpp:106] Iteration 177300, lr = 1e-25
I0430 12:11:58.907500   966 solver.cpp:228] Iteration 177400, loss = 0.105214
I0430 12:11:58.907655   966 solver.cpp:244]     Train net output #0: loss = 0.105214 (* 1 = 0.105214 loss)
I0430 12:11:58.907662   966 sgd_solver.cpp:106] Iteration 177400, lr = 1e-25
I0430 12:12:50.381557   966 solver.cpp:228] Iteration 177500, loss = 0.10779
I0430 12:12:50.381711   966 solver.cpp:244]     Train net output #0: loss = 0.10779 (* 1 = 0.10779 loss)
I0430 12:12:50.381718   966 sgd_solver.cpp:106] Iteration 177500, lr = 1e-25
I0430 12:13:39.981336   966 solver.cpp:228] Iteration 177600, loss = 0.233491
I0430 12:13:39.981479   966 solver.cpp:244]     Train net output #0: loss = 0.233491 (* 1 = 0.233491 loss)
I0430 12:13:39.981487   966 sgd_solver.cpp:106] Iteration 177600, lr = 1e-25
I0430 12:14:29.590026   966 solver.cpp:228] Iteration 177700, loss = 0.137449
I0430 12:14:29.591374   966 solver.cpp:244]     Train net output #0: loss = 0.137449 (* 1 = 0.137449 loss)
I0430 12:14:29.591397   966 sgd_solver.cpp:106] Iteration 177700, lr = 1e-25
I0430 12:15:20.911702   966 solver.cpp:228] Iteration 177800, loss = 0.106247
I0430 12:15:20.912572   966 solver.cpp:244]     Train net output #0: loss = 0.106247 (* 1 = 0.106247 loss)
I0430 12:15:20.912580   966 sgd_solver.cpp:106] Iteration 177800, lr = 1e-25
I0430 12:16:10.515198   966 solver.cpp:228] Iteration 177900, loss = 0.150067
I0430 12:16:10.515377   966 solver.cpp:244]     Train net output #0: loss = 0.150067 (* 1 = 0.150067 loss)
I0430 12:16:10.515384   966 sgd_solver.cpp:106] Iteration 177900, lr = 1e-25
I0430 12:16:59.806169   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_178000.caffemodel
I0430 12:17:21.251265   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_178000.solverstate
I0430 12:17:21.442378   966 solver.cpp:337] Iteration 178000, Testing net (#0)
I0430 12:17:21.442502   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 12:17:21.442508   966 net.cpp:693] Ignoring source layer visualize
I0430 12:17:21.442510   966 net.cpp:693] Ignoring source layer fake
I0430 12:22:15.260268   966 solver.cpp:404]     Test net output #0: loss = 0.182005 (* 1 = 0.182005 loss)
I0430 12:22:15.570583   966 solver.cpp:228] Iteration 178000, loss = 0.201758
I0430 12:22:15.570621   966 solver.cpp:244]     Train net output #0: loss = 0.201758 (* 1 = 0.201758 loss)
I0430 12:22:15.570629   966 sgd_solver.cpp:106] Iteration 178000, lr = 1e-25
I0430 12:23:06.518468   966 solver.cpp:228] Iteration 178100, loss = 0.13444
I0430 12:23:06.518817   966 solver.cpp:244]     Train net output #0: loss = 0.13444 (* 1 = 0.13444 loss)
I0430 12:23:06.518833   966 sgd_solver.cpp:106] Iteration 178100, lr = 1e-25
I0430 12:23:56.127158   966 solver.cpp:228] Iteration 178200, loss = 0.123896
I0430 12:23:56.127306   966 solver.cpp:244]     Train net output #0: loss = 0.123896 (* 1 = 0.123896 loss)
I0430 12:23:56.127313   966 sgd_solver.cpp:106] Iteration 178200, lr = 1e-25
I0430 12:24:45.745923   966 solver.cpp:228] Iteration 178300, loss = 0.10837
I0430 12:24:45.746109   966 solver.cpp:244]     Train net output #0: loss = 0.10837 (* 1 = 0.10837 loss)
I0430 12:24:45.746114   966 sgd_solver.cpp:106] Iteration 178300, lr = 1e-25
I0430 12:25:36.848986   966 solver.cpp:228] Iteration 178400, loss = 0.343459
I0430 12:25:36.849146   966 solver.cpp:244]     Train net output #0: loss = 0.343459 (* 1 = 0.343459 loss)
I0430 12:25:36.849153   966 sgd_solver.cpp:106] Iteration 178400, lr = 1e-25
I0430 12:26:26.468176   966 solver.cpp:228] Iteration 178500, loss = 0.346599
I0430 12:26:26.468624   966 solver.cpp:244]     Train net output #0: loss = 0.346599 (* 1 = 0.346599 loss)
I0430 12:26:26.468633   966 sgd_solver.cpp:106] Iteration 178500, lr = 1e-25
I0430 12:27:17.709647   966 solver.cpp:228] Iteration 178600, loss = 0.06096
I0430 12:27:17.709822   966 solver.cpp:244]     Train net output #0: loss = 0.06096 (* 1 = 0.06096 loss)
I0430 12:27:17.709830   966 sgd_solver.cpp:106] Iteration 178600, lr = 1e-25
I0430 12:28:07.323812   966 solver.cpp:228] Iteration 178700, loss = 0.140144
I0430 12:28:07.324128   966 solver.cpp:244]     Train net output #0: loss = 0.140144 (* 1 = 0.140144 loss)
I0430 12:28:07.324136   966 sgd_solver.cpp:106] Iteration 178700, lr = 1e-25
I0430 12:28:56.948361   966 solver.cpp:228] Iteration 178800, loss = 0.104224
I0430 12:28:56.948549   966 solver.cpp:244]     Train net output #0: loss = 0.104224 (* 1 = 0.104224 loss)
I0430 12:28:56.948556   966 sgd_solver.cpp:106] Iteration 178800, lr = 1e-25
I0430 12:29:48.385653   966 solver.cpp:228] Iteration 178900, loss = 0.182959
I0430 12:29:48.385807   966 solver.cpp:244]     Train net output #0: loss = 0.182959 (* 1 = 0.182959 loss)
I0430 12:29:48.385815   966 sgd_solver.cpp:106] Iteration 178900, lr = 1e-25
I0430 12:30:37.688218   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_179000.caffemodel
I0430 12:30:54.705977   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_179000.solverstate
I0430 12:30:54.911695   966 solver.cpp:337] Iteration 179000, Testing net (#0)
I0430 12:30:54.911834   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 12:30:54.911839   966 net.cpp:693] Ignoring source layer visualize
I0430 12:30:54.911842   966 net.cpp:693] Ignoring source layer fake
I0430 12:35:48.257228   966 solver.cpp:404]     Test net output #0: loss = 0.181795 (* 1 = 0.181795 loss)
I0430 12:35:48.569680   966 solver.cpp:228] Iteration 179000, loss = 0.20974
I0430 12:35:48.569701   966 solver.cpp:244]     Train net output #0: loss = 0.20974 (* 1 = 0.20974 loss)
I0430 12:35:48.569712   966 sgd_solver.cpp:106] Iteration 179000, lr = 1e-25
I0430 12:36:38.193727   966 solver.cpp:228] Iteration 179100, loss = 0.196817
I0430 12:36:38.193922   966 solver.cpp:244]     Train net output #0: loss = 0.196817 (* 1 = 0.196817 loss)
I0430 12:36:38.193929   966 sgd_solver.cpp:106] Iteration 179100, lr = 1e-25
I0430 12:37:27.818835   966 solver.cpp:228] Iteration 179200, loss = 0.193681
I0430 12:37:27.818995   966 solver.cpp:244]     Train net output #0: loss = 0.193681 (* 1 = 0.193681 loss)
I0430 12:37:27.819003   966 sgd_solver.cpp:106] Iteration 179200, lr = 1e-25
I0430 12:38:19.371295   966 solver.cpp:228] Iteration 179300, loss = 0.102768
I0430 12:38:19.371498   966 solver.cpp:244]     Train net output #0: loss = 0.102768 (* 1 = 0.102768 loss)
I0430 12:38:19.371507   966 sgd_solver.cpp:106] Iteration 179300, lr = 1e-25
I0430 12:39:08.988579   966 solver.cpp:228] Iteration 179400, loss = 0.0837181
I0430 12:39:08.988754   966 solver.cpp:244]     Train net output #0: loss = 0.0837181 (* 1 = 0.0837181 loss)
I0430 12:39:08.988761   966 sgd_solver.cpp:106] Iteration 179400, lr = 1e-25
I0430 12:39:58.622229   966 solver.cpp:228] Iteration 179500, loss = 0.164966
I0430 12:39:58.622412   966 solver.cpp:244]     Train net output #0: loss = 0.164966 (* 1 = 0.164966 loss)
I0430 12:39:58.622419   966 sgd_solver.cpp:106] Iteration 179500, lr = 1e-25
I0430 12:40:50.204602   966 solver.cpp:228] Iteration 179600, loss = 0.103006
I0430 12:40:50.204764   966 solver.cpp:244]     Train net output #0: loss = 0.103006 (* 1 = 0.103006 loss)
I0430 12:40:50.204772   966 sgd_solver.cpp:106] Iteration 179600, lr = 1e-25
I0430 12:41:39.820562   966 solver.cpp:228] Iteration 179700, loss = 0.208531
I0430 12:41:39.820724   966 solver.cpp:244]     Train net output #0: loss = 0.208531 (* 1 = 0.208531 loss)
I0430 12:41:39.820731   966 sgd_solver.cpp:106] Iteration 179700, lr = 1e-25
I0430 12:42:29.442363   966 solver.cpp:228] Iteration 179800, loss = 0.104588
I0430 12:42:29.442512   966 solver.cpp:244]     Train net output #0: loss = 0.104588 (* 1 = 0.104588 loss)
I0430 12:42:29.442517   966 sgd_solver.cpp:106] Iteration 179800, lr = 1e-25
I0430 12:43:19.058523   966 solver.cpp:228] Iteration 179900, loss = 0.253402
I0430 12:43:19.058677   966 solver.cpp:244]     Train net output #0: loss = 0.253402 (* 1 = 0.253402 loss)
I0430 12:43:19.058684   966 sgd_solver.cpp:106] Iteration 179900, lr = 1e-25
I0430 12:44:10.247458   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_180000.caffemodel
I0430 12:44:45.657780   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_180000.solverstate
I0430 12:44:45.858386   966 solver.cpp:337] Iteration 180000, Testing net (#0)
I0430 12:44:45.858530   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 12:44:45.858536   966 net.cpp:693] Ignoring source layer visualize
I0430 12:44:45.858538   966 net.cpp:693] Ignoring source layer fake
I0430 12:49:39.031823   966 solver.cpp:404]     Test net output #0: loss = 0.179773 (* 1 = 0.179773 loss)
I0430 12:49:39.342243   966 solver.cpp:228] Iteration 180000, loss = 0.0993174
I0430 12:49:39.342288   966 solver.cpp:244]     Train net output #0: loss = 0.0993174 (* 1 = 0.0993174 loss)
I0430 12:49:39.342295   966 sgd_solver.cpp:106] Iteration 180000, lr = 1e-26
I0430 12:50:28.953898   966 solver.cpp:228] Iteration 180100, loss = 0.397813
I0430 12:50:28.954077   966 solver.cpp:244]     Train net output #0: loss = 0.397813 (* 1 = 0.397813 loss)
I0430 12:50:28.954083   966 sgd_solver.cpp:106] Iteration 180100, lr = 1e-26
I0430 12:51:18.566669   966 solver.cpp:228] Iteration 180200, loss = 0.0818025
I0430 12:51:18.566865   966 solver.cpp:244]     Train net output #0: loss = 0.0818025 (* 1 = 0.0818025 loss)
I0430 12:51:18.566872   966 sgd_solver.cpp:106] Iteration 180200, lr = 1e-26
I0430 12:52:10.022169   966 solver.cpp:228] Iteration 180300, loss = 0.0707244
I0430 12:52:10.022416   966 solver.cpp:244]     Train net output #0: loss = 0.0707244 (* 1 = 0.0707244 loss)
I0430 12:52:10.022430   966 sgd_solver.cpp:106] Iteration 180300, lr = 1e-26
I0430 12:52:59.630151   966 solver.cpp:228] Iteration 180400, loss = 0.0906903
I0430 12:52:59.630295   966 solver.cpp:244]     Train net output #0: loss = 0.0906903 (* 1 = 0.0906903 loss)
I0430 12:52:59.630302   966 sgd_solver.cpp:106] Iteration 180400, lr = 1e-26
I0430 12:53:49.247069   966 solver.cpp:228] Iteration 180500, loss = 0.158808
I0430 12:53:49.247248   966 solver.cpp:244]     Train net output #0: loss = 0.158808 (* 1 = 0.158808 loss)
I0430 12:53:49.247256   966 sgd_solver.cpp:106] Iteration 180500, lr = 1e-26
I0430 12:54:40.737725   966 solver.cpp:228] Iteration 180600, loss = 0.132187
I0430 12:54:40.737891   966 solver.cpp:244]     Train net output #0: loss = 0.132187 (* 1 = 0.132187 loss)
I0430 12:54:40.737898   966 sgd_solver.cpp:106] Iteration 180600, lr = 1e-26
I0430 12:55:30.348316   966 solver.cpp:228] Iteration 180700, loss = 0.115481
I0430 12:55:30.348891   966 solver.cpp:244]     Train net output #0: loss = 0.115481 (* 1 = 0.115481 loss)
I0430 12:55:30.348914   966 sgd_solver.cpp:106] Iteration 180700, lr = 1e-26
I0430 12:56:19.958637   966 solver.cpp:228] Iteration 180800, loss = 0.133299
I0430 12:56:19.958806   966 solver.cpp:244]     Train net output #0: loss = 0.133299 (* 1 = 0.133299 loss)
I0430 12:56:19.958812   966 sgd_solver.cpp:106] Iteration 180800, lr = 1e-26
I0430 12:57:09.570777   966 solver.cpp:228] Iteration 180900, loss = 0.113452
I0430 12:57:09.570976   966 solver.cpp:244]     Train net output #0: loss = 0.113452 (* 1 = 0.113452 loss)
I0430 12:57:09.570986   966 sgd_solver.cpp:106] Iteration 180900, lr = 1e-26
I0430 12:58:00.714928   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_181000.caffemodel
I0430 12:58:18.320829   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_181000.solverstate
I0430 12:58:18.520985   966 solver.cpp:337] Iteration 181000, Testing net (#0)
I0430 12:58:18.521128   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 12:58:18.521134   966 net.cpp:693] Ignoring source layer visualize
I0430 12:58:18.521137   966 net.cpp:693] Ignoring source layer fake
I0430 13:03:11.104351   966 solver.cpp:404]     Test net output #0: loss = 0.189039 (* 1 = 0.189039 loss)
I0430 13:03:11.414366   966 solver.cpp:228] Iteration 181000, loss = 0.126429
I0430 13:03:11.414402   966 solver.cpp:244]     Train net output #0: loss = 0.126429 (* 1 = 0.126429 loss)
I0430 13:03:11.414408   966 sgd_solver.cpp:106] Iteration 181000, lr = 1e-26
I0430 13:04:01.022053   966 solver.cpp:228] Iteration 181100, loss = 0.118867
I0430 13:04:01.022225   966 solver.cpp:244]     Train net output #0: loss = 0.118867 (* 1 = 0.118867 loss)
I0430 13:04:01.022233   966 sgd_solver.cpp:106] Iteration 181100, lr = 1e-26
I0430 13:04:50.642493   966 solver.cpp:228] Iteration 181200, loss = 0.134598
I0430 13:04:50.642663   966 solver.cpp:244]     Train net output #0: loss = 0.134598 (* 1 = 0.134598 loss)
I0430 13:04:50.642669   966 sgd_solver.cpp:106] Iteration 181200, lr = 1e-26
I0430 13:05:41.565919   966 solver.cpp:228] Iteration 181300, loss = 0.103556
I0430 13:05:41.566097   966 solver.cpp:244]     Train net output #0: loss = 0.103556 (* 1 = 0.103556 loss)
I0430 13:05:41.566104   966 sgd_solver.cpp:106] Iteration 181300, lr = 1e-26
I0430 13:06:31.183620   966 solver.cpp:228] Iteration 181400, loss = 0.163415
I0430 13:06:31.183778   966 solver.cpp:244]     Train net output #0: loss = 0.163415 (* 1 = 0.163415 loss)
I0430 13:06:31.183784   966 sgd_solver.cpp:106] Iteration 181400, lr = 1e-26
I0430 13:07:22.186486   966 solver.cpp:228] Iteration 181500, loss = 0.146293
I0430 13:07:22.186673   966 solver.cpp:244]     Train net output #0: loss = 0.146293 (* 1 = 0.146293 loss)
I0430 13:07:22.186681   966 sgd_solver.cpp:106] Iteration 181500, lr = 1e-26
I0430 13:08:11.801461   966 solver.cpp:228] Iteration 181600, loss = 0.150954
I0430 13:08:11.801622   966 solver.cpp:244]     Train net output #0: loss = 0.150954 (* 1 = 0.150954 loss)
I0430 13:08:11.801630   966 sgd_solver.cpp:106] Iteration 181600, lr = 1e-26
I0430 13:09:01.423013   966 solver.cpp:228] Iteration 181700, loss = 0.0626012
I0430 13:09:01.423249   966 solver.cpp:244]     Train net output #0: loss = 0.0626012 (* 1 = 0.0626012 loss)
I0430 13:09:01.423269   966 sgd_solver.cpp:106] Iteration 181700, lr = 1e-26
I0430 13:09:52.577899   966 solver.cpp:228] Iteration 181800, loss = 0.0944697
I0430 13:09:52.578058   966 solver.cpp:244]     Train net output #0: loss = 0.0944697 (* 1 = 0.0944697 loss)
I0430 13:09:52.578065   966 sgd_solver.cpp:106] Iteration 181800, lr = 1e-26
I0430 13:10:42.197638   966 solver.cpp:228] Iteration 181900, loss = 0.134455
I0430 13:10:42.197785   966 solver.cpp:244]     Train net output #0: loss = 0.134455 (* 1 = 0.134455 loss)
I0430 13:10:42.197791   966 sgd_solver.cpp:106] Iteration 181900, lr = 1e-26
I0430 13:11:31.504245   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_182000.caffemodel
I0430 13:12:05.377497   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_182000.solverstate
I0430 13:12:05.573370   966 solver.cpp:337] Iteration 182000, Testing net (#0)
I0430 13:12:05.573514   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 13:12:05.573521   966 net.cpp:693] Ignoring source layer visualize
I0430 13:12:05.573523   966 net.cpp:693] Ignoring source layer fake
I0430 13:16:57.913700   966 solver.cpp:404]     Test net output #0: loss = 0.188769 (* 1 = 0.188769 loss)
I0430 13:16:58.223079   966 solver.cpp:228] Iteration 182000, loss = 0.145429
I0430 13:16:58.223098   966 solver.cpp:244]     Train net output #0: loss = 0.145429 (* 1 = 0.145429 loss)
I0430 13:16:58.223104   966 sgd_solver.cpp:106] Iteration 182000, lr = 1e-26
I0430 13:17:49.537797   966 solver.cpp:228] Iteration 182100, loss = 0.150637
I0430 13:17:49.537925   966 solver.cpp:244]     Train net output #0: loss = 0.150637 (* 1 = 0.150637 loss)
I0430 13:17:49.537932   966 sgd_solver.cpp:106] Iteration 182100, lr = 1e-26
I0430 13:18:39.153481   966 solver.cpp:228] Iteration 182200, loss = 0.0923098
I0430 13:18:39.153612   966 solver.cpp:244]     Train net output #0: loss = 0.0923098 (* 1 = 0.0923098 loss)
I0430 13:18:39.153620   966 sgd_solver.cpp:106] Iteration 182200, lr = 1e-26
I0430 13:19:28.772511   966 solver.cpp:228] Iteration 182300, loss = 0.149545
I0430 13:19:28.772629   966 solver.cpp:244]     Train net output #0: loss = 0.149545 (* 1 = 0.149545 loss)
I0430 13:19:28.772634   966 sgd_solver.cpp:106] Iteration 182300, lr = 1e-26
I0430 13:20:20.189504   966 solver.cpp:228] Iteration 182400, loss = 0.134035
I0430 13:20:20.189652   966 solver.cpp:244]     Train net output #0: loss = 0.134035 (* 1 = 0.134035 loss)
I0430 13:20:20.189658   966 sgd_solver.cpp:106] Iteration 182400, lr = 1e-26
I0430 13:21:09.797291   966 solver.cpp:228] Iteration 182500, loss = 0.157552
I0430 13:21:09.797410   966 solver.cpp:244]     Train net output #0: loss = 0.157552 (* 1 = 0.157552 loss)
I0430 13:21:09.797417   966 sgd_solver.cpp:106] Iteration 182500, lr = 1e-26
I0430 13:21:59.415061   966 solver.cpp:228] Iteration 182600, loss = 0.177293
I0430 13:21:59.415223   966 solver.cpp:244]     Train net output #0: loss = 0.177293 (* 1 = 0.177293 loss)
I0430 13:21:59.415230   966 sgd_solver.cpp:106] Iteration 182600, lr = 1e-26
I0430 13:22:50.833441   966 solver.cpp:228] Iteration 182700, loss = 0.159531
I0430 13:22:50.833596   966 solver.cpp:244]     Train net output #0: loss = 0.159531 (* 1 = 0.159531 loss)
I0430 13:22:50.833603   966 sgd_solver.cpp:106] Iteration 182700, lr = 1e-26
I0430 13:23:40.432937   966 solver.cpp:228] Iteration 182800, loss = 0.066104
I0430 13:23:40.433060   966 solver.cpp:244]     Train net output #0: loss = 0.066104 (* 1 = 0.066104 loss)
I0430 13:23:40.433066   966 sgd_solver.cpp:106] Iteration 182800, lr = 1e-26
I0430 13:24:30.042140   966 solver.cpp:228] Iteration 182900, loss = 0.0890384
I0430 13:24:30.042284   966 solver.cpp:244]     Train net output #0: loss = 0.0890384 (* 1 = 0.0890384 loss)
I0430 13:24:30.042290   966 sgd_solver.cpp:106] Iteration 182900, lr = 1e-26
I0430 13:25:19.339846   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_183000.caffemodel
I0430 13:25:19.713956   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_183000.solverstate
I0430 13:25:19.902082   966 solver.cpp:337] Iteration 183000, Testing net (#0)
I0430 13:25:19.902217   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 13:25:19.902222   966 net.cpp:693] Ignoring source layer visualize
I0430 13:25:19.902225   966 net.cpp:693] Ignoring source layer fake
I0430 13:30:12.403256   966 solver.cpp:404]     Test net output #0: loss = 0.182118 (* 1 = 0.182118 loss)
I0430 13:30:12.713837   966 solver.cpp:228] Iteration 183000, loss = 0.162744
I0430 13:30:12.713855   966 solver.cpp:244]     Train net output #0: loss = 0.162744 (* 1 = 0.162744 loss)
I0430 13:30:12.713861   966 sgd_solver.cpp:106] Iteration 183000, lr = 1e-26
I0430 13:31:04.077349   966 solver.cpp:228] Iteration 183100, loss = 0.257226
I0430 13:31:04.077494   966 solver.cpp:244]     Train net output #0: loss = 0.257226 (* 1 = 0.257226 loss)
I0430 13:31:04.077500   966 sgd_solver.cpp:106] Iteration 183100, lr = 1e-26
I0430 13:31:53.668851   966 solver.cpp:228] Iteration 183200, loss = 0.120202
I0430 13:31:53.668999   966 solver.cpp:244]     Train net output #0: loss = 0.120202 (* 1 = 0.120202 loss)
I0430 13:31:53.669006   966 sgd_solver.cpp:106] Iteration 183200, lr = 1e-26
I0430 13:32:43.270830   966 solver.cpp:228] Iteration 183300, loss = 0.0862561
I0430 13:32:43.270972   966 solver.cpp:244]     Train net output #0: loss = 0.0862561 (* 1 = 0.0862561 loss)
I0430 13:32:43.270977   966 sgd_solver.cpp:106] Iteration 183300, lr = 1e-26
I0430 13:33:34.624863   966 solver.cpp:228] Iteration 183400, loss = 0.083699
I0430 13:33:34.624999   966 solver.cpp:244]     Train net output #0: loss = 0.083699 (* 1 = 0.083699 loss)
I0430 13:33:34.625005   966 sgd_solver.cpp:106] Iteration 183400, lr = 1e-26
I0430 13:34:24.216441   966 solver.cpp:228] Iteration 183500, loss = 0.0869422
I0430 13:34:24.216591   966 solver.cpp:244]     Train net output #0: loss = 0.0869422 (* 1 = 0.0869422 loss)
I0430 13:34:24.216598   966 sgd_solver.cpp:106] Iteration 183500, lr = 1e-26
I0430 13:35:13.813094   966 solver.cpp:228] Iteration 183600, loss = 0.1048
I0430 13:35:13.813256   966 solver.cpp:244]     Train net output #0: loss = 0.1048 (* 1 = 0.1048 loss)
I0430 13:35:13.813263   966 sgd_solver.cpp:106] Iteration 183600, lr = 1e-26
I0430 13:36:05.143529   966 solver.cpp:228] Iteration 183700, loss = 0.0767673
I0430 13:36:05.143671   966 solver.cpp:244]     Train net output #0: loss = 0.0767673 (* 1 = 0.0767673 loss)
I0430 13:36:05.143676   966 sgd_solver.cpp:106] Iteration 183700, lr = 1e-26
I0430 13:36:54.736006   966 solver.cpp:228] Iteration 183800, loss = 0.0261314
I0430 13:36:54.737522   966 solver.cpp:244]     Train net output #0: loss = 0.0261314 (* 1 = 0.0261314 loss)
I0430 13:36:54.737529   966 sgd_solver.cpp:106] Iteration 183800, lr = 1e-26
I0430 13:37:44.327816   966 solver.cpp:228] Iteration 183900, loss = 0.102685
I0430 13:37:44.327952   966 solver.cpp:244]     Train net output #0: loss = 0.102685 (* 1 = 0.102685 loss)
I0430 13:37:44.327958   966 sgd_solver.cpp:106] Iteration 183900, lr = 1e-26
I0430 13:38:33.608409   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_184000.caffemodel
I0430 13:38:33.979018   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_184000.solverstate
I0430 13:38:34.160894   966 solver.cpp:337] Iteration 184000, Testing net (#0)
I0430 13:38:34.160960   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 13:38:34.160964   966 net.cpp:693] Ignoring source layer visualize
I0430 13:38:34.160965   966 net.cpp:693] Ignoring source layer fake
I0430 13:43:26.518223   966 solver.cpp:404]     Test net output #0: loss = 0.181231 (* 1 = 0.181231 loss)
I0430 13:43:26.827975   966 solver.cpp:228] Iteration 184000, loss = 0.0982873
I0430 13:43:26.827996   966 solver.cpp:244]     Train net output #0: loss = 0.0982873 (* 1 = 0.0982873 loss)
I0430 13:43:26.828001   966 sgd_solver.cpp:106] Iteration 184000, lr = 1e-26
I0430 13:44:18.017217   966 solver.cpp:228] Iteration 184100, loss = 0.0812193
I0430 13:44:18.017396   966 solver.cpp:244]     Train net output #0: loss = 0.0812193 (* 1 = 0.0812193 loss)
I0430 13:44:18.017403   966 sgd_solver.cpp:106] Iteration 184100, lr = 1e-26
I0430 13:45:07.601042   966 solver.cpp:228] Iteration 184200, loss = 0.163355
I0430 13:45:07.601197   966 solver.cpp:244]     Train net output #0: loss = 0.163355 (* 1 = 0.163355 loss)
I0430 13:45:07.601202   966 sgd_solver.cpp:106] Iteration 184200, lr = 1e-26
I0430 13:45:57.193159   966 solver.cpp:228] Iteration 184300, loss = 0.317327
I0430 13:45:57.193502   966 solver.cpp:244]     Train net output #0: loss = 0.317327 (* 1 = 0.317327 loss)
I0430 13:45:57.193511   966 sgd_solver.cpp:106] Iteration 184300, lr = 1e-26
I0430 13:46:48.012980   966 solver.cpp:228] Iteration 184400, loss = 0.0970611
I0430 13:46:48.013125   966 solver.cpp:244]     Train net output #0: loss = 0.0970611 (* 1 = 0.0970611 loss)
I0430 13:46:48.013133   966 sgd_solver.cpp:106] Iteration 184400, lr = 1e-26
I0430 13:47:37.615505   966 solver.cpp:228] Iteration 184500, loss = 0.127033
I0430 13:47:37.615631   966 solver.cpp:244]     Train net output #0: loss = 0.127033 (* 1 = 0.127033 loss)
I0430 13:47:37.615638   966 sgd_solver.cpp:106] Iteration 184500, lr = 1e-26
I0430 13:48:28.487082   966 solver.cpp:228] Iteration 184600, loss = 0.10128
I0430 13:48:28.487257   966 solver.cpp:244]     Train net output #0: loss = 0.10128 (* 1 = 0.10128 loss)
I0430 13:48:28.487263   966 sgd_solver.cpp:106] Iteration 184600, lr = 1e-26
I0430 13:49:18.077857   966 solver.cpp:228] Iteration 184700, loss = 0.123951
I0430 13:49:18.078027   966 solver.cpp:244]     Train net output #0: loss = 0.123951 (* 1 = 0.123951 loss)
I0430 13:49:18.078034   966 sgd_solver.cpp:106] Iteration 184700, lr = 1e-26
I0430 13:50:07.686550   966 solver.cpp:228] Iteration 184800, loss = 0.11021
I0430 13:50:07.686712   966 solver.cpp:244]     Train net output #0: loss = 0.11021 (* 1 = 0.11021 loss)
I0430 13:50:07.686718   966 sgd_solver.cpp:106] Iteration 184800, lr = 1e-26
I0430 13:50:58.781440   966 solver.cpp:228] Iteration 184900, loss = 0.132276
I0430 13:50:58.781585   966 solver.cpp:244]     Train net output #0: loss = 0.132276 (* 1 = 0.132276 loss)
I0430 13:50:58.781592   966 sgd_solver.cpp:106] Iteration 184900, lr = 1e-26
I0430 13:51:48.068707   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_185000.caffemodel
I0430 13:51:48.430928   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_185000.solverstate
I0430 13:51:48.613840   966 solver.cpp:337] Iteration 185000, Testing net (#0)
I0430 13:51:48.613920   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 13:51:48.613940   966 net.cpp:693] Ignoring source layer visualize
I0430 13:51:48.613942   966 net.cpp:693] Ignoring source layer fake
I0430 13:56:40.916625   966 solver.cpp:404]     Test net output #0: loss = 0.180203 (* 1 = 0.180203 loss)
I0430 13:56:41.226760   966 solver.cpp:228] Iteration 185000, loss = 0.106757
I0430 13:56:41.226780   966 solver.cpp:244]     Train net output #0: loss = 0.106757 (* 1 = 0.106757 loss)
I0430 13:56:41.226786   966 sgd_solver.cpp:106] Iteration 185000, lr = 1e-26
I0430 13:57:30.829491   966 solver.cpp:228] Iteration 185100, loss = 0.150959
I0430 13:57:30.829694   966 solver.cpp:244]     Train net output #0: loss = 0.150959 (* 1 = 0.150959 loss)
I0430 13:57:30.829700   966 sgd_solver.cpp:106] Iteration 185100, lr = 1e-26
I0430 13:58:22.127784   966 solver.cpp:228] Iteration 185200, loss = 0.246677
I0430 13:58:22.127943   966 solver.cpp:244]     Train net output #0: loss = 0.246677 (* 1 = 0.246677 loss)
I0430 13:58:22.127948   966 sgd_solver.cpp:106] Iteration 185200, lr = 1e-26
I0430 13:59:11.723247   966 solver.cpp:228] Iteration 185300, loss = 0.0392088
I0430 13:59:11.723402   966 solver.cpp:244]     Train net output #0: loss = 0.0392088 (* 1 = 0.0392088 loss)
I0430 13:59:11.723408   966 sgd_solver.cpp:106] Iteration 185300, lr = 1e-26
I0430 14:00:01.317975   966 solver.cpp:228] Iteration 185400, loss = 0.236082
I0430 14:00:01.318110   966 solver.cpp:244]     Train net output #0: loss = 0.236082 (* 1 = 0.236082 loss)
I0430 14:00:01.318117   966 sgd_solver.cpp:106] Iteration 185400, lr = 1e-26
I0430 14:00:52.701380   966 solver.cpp:228] Iteration 185500, loss = 0.11601
I0430 14:00:52.701524   966 solver.cpp:244]     Train net output #0: loss = 0.11601 (* 1 = 0.11601 loss)
I0430 14:00:52.701530   966 sgd_solver.cpp:106] Iteration 185500, lr = 1e-26
I0430 14:01:42.298086   966 solver.cpp:228] Iteration 185600, loss = 0.218657
I0430 14:01:42.298208   966 solver.cpp:244]     Train net output #0: loss = 0.218657 (* 1 = 0.218657 loss)
I0430 14:01:42.298214   966 sgd_solver.cpp:106] Iteration 185600, lr = 1e-26
I0430 14:02:31.897804   966 solver.cpp:228] Iteration 185700, loss = 0.173253
I0430 14:02:31.897946   966 solver.cpp:244]     Train net output #0: loss = 0.173253 (* 1 = 0.173253 loss)
I0430 14:02:31.897953   966 sgd_solver.cpp:106] Iteration 185700, lr = 1e-26
I0430 14:03:21.500315   966 solver.cpp:228] Iteration 185800, loss = 0.240979
I0430 14:03:21.500458   966 solver.cpp:244]     Train net output #0: loss = 0.240979 (* 1 = 0.240979 loss)
I0430 14:03:21.500465   966 sgd_solver.cpp:106] Iteration 185800, lr = 1e-26
I0430 14:04:12.918516   966 solver.cpp:228] Iteration 185900, loss = 0.220961
I0430 14:04:12.918665   966 solver.cpp:244]     Train net output #0: loss = 0.220961 (* 1 = 0.220961 loss)
I0430 14:04:12.918673   966 sgd_solver.cpp:106] Iteration 185900, lr = 1e-26
I0430 14:05:02.212412   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_186000.caffemodel
I0430 14:05:02.586364   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_186000.solverstate
I0430 14:05:02.769497   966 solver.cpp:337] Iteration 186000, Testing net (#0)
I0430 14:05:02.769582   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 14:05:02.769603   966 net.cpp:693] Ignoring source layer visualize
I0430 14:05:02.769605   966 net.cpp:693] Ignoring source layer fake
I0430 14:09:54.968173   966 solver.cpp:404]     Test net output #0: loss = 0.19052 (* 1 = 0.19052 loss)
I0430 14:09:55.278316   966 solver.cpp:228] Iteration 186000, loss = 0.112119
I0430 14:09:55.278352   966 solver.cpp:244]     Train net output #0: loss = 0.112119 (* 1 = 0.112119 loss)
I0430 14:09:55.278358   966 sgd_solver.cpp:106] Iteration 186000, lr = 1e-26
I0430 14:10:44.876122   966 solver.cpp:228] Iteration 186100, loss = 0.0728346
I0430 14:10:44.876308   966 solver.cpp:244]     Train net output #0: loss = 0.0728346 (* 1 = 0.0728346 loss)
I0430 14:10:44.876322   966 sgd_solver.cpp:106] Iteration 186100, lr = 1e-26
I0430 14:11:36.234105   966 solver.cpp:228] Iteration 186200, loss = 0.0794492
I0430 14:11:36.234237   966 solver.cpp:244]     Train net output #0: loss = 0.0794492 (* 1 = 0.0794492 loss)
I0430 14:11:36.234243   966 sgd_solver.cpp:106] Iteration 186200, lr = 1e-26
I0430 14:12:25.823696   966 solver.cpp:228] Iteration 186300, loss = 0.0924165
I0430 14:12:25.823854   966 solver.cpp:244]     Train net output #0: loss = 0.0924165 (* 1 = 0.0924165 loss)
I0430 14:12:25.823861   966 sgd_solver.cpp:106] Iteration 186300, lr = 1e-26
I0430 14:13:15.421452   966 solver.cpp:228] Iteration 186400, loss = 0.128471
I0430 14:13:15.421591   966 solver.cpp:244]     Train net output #0: loss = 0.128471 (* 1 = 0.128471 loss)
I0430 14:13:15.421597   966 sgd_solver.cpp:106] Iteration 186400, lr = 1e-26
I0430 14:14:06.755656   966 solver.cpp:228] Iteration 186500, loss = 0.169076
I0430 14:14:06.755815   966 solver.cpp:244]     Train net output #0: loss = 0.169076 (* 1 = 0.169076 loss)
I0430 14:14:06.755823   966 sgd_solver.cpp:106] Iteration 186500, lr = 1e-26
I0430 14:14:56.339097   966 solver.cpp:228] Iteration 186600, loss = 0.112062
I0430 14:14:56.339226   966 solver.cpp:244]     Train net output #0: loss = 0.112062 (* 1 = 0.112062 loss)
I0430 14:14:56.339232   966 sgd_solver.cpp:106] Iteration 186600, lr = 1e-26
I0430 14:15:45.928517   966 solver.cpp:228] Iteration 186700, loss = 0.0945378
I0430 14:15:45.928699   966 solver.cpp:244]     Train net output #0: loss = 0.0945378 (* 1 = 0.0945378 loss)
I0430 14:15:45.928705   966 sgd_solver.cpp:106] Iteration 186700, lr = 1e-26
I0430 14:16:35.515858   966 solver.cpp:228] Iteration 186800, loss = 0.121389
I0430 14:16:35.516026   966 solver.cpp:244]     Train net output #0: loss = 0.121389 (* 1 = 0.121389 loss)
I0430 14:16:35.516032   966 sgd_solver.cpp:106] Iteration 186800, lr = 1e-26
I0430 14:17:26.828842   966 solver.cpp:228] Iteration 186900, loss = 0.0812802
I0430 14:17:26.828994   966 solver.cpp:244]     Train net output #0: loss = 0.0812802 (* 1 = 0.0812802 loss)
I0430 14:17:26.829001   966 sgd_solver.cpp:106] Iteration 186900, lr = 1e-26
I0430 14:18:16.107334   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_187000.caffemodel
I0430 14:18:16.472882   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_187000.solverstate
I0430 14:18:16.658310   966 solver.cpp:337] Iteration 187000, Testing net (#0)
I0430 14:18:16.658388   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 14:18:16.658391   966 net.cpp:693] Ignoring source layer visualize
I0430 14:18:16.658409   966 net.cpp:693] Ignoring source layer fake
I0430 14:23:09.258347   966 solver.cpp:404]     Test net output #0: loss = 0.187679 (* 1 = 0.187679 loss)
I0430 14:23:09.568194   966 solver.cpp:228] Iteration 187000, loss = 0.114532
I0430 14:23:09.568210   966 solver.cpp:244]     Train net output #0: loss = 0.114532 (* 1 = 0.114532 loss)
I0430 14:23:09.568217   966 sgd_solver.cpp:106] Iteration 187000, lr = 1e-26
I0430 14:23:59.155864   966 solver.cpp:228] Iteration 187100, loss = 0.121956
I0430 14:23:59.156020   966 solver.cpp:244]     Train net output #0: loss = 0.121956 (* 1 = 0.121956 loss)
I0430 14:23:59.156026   966 sgd_solver.cpp:106] Iteration 187100, lr = 1e-26
I0430 14:24:50.318063   966 solver.cpp:228] Iteration 187200, loss = 0.148709
I0430 14:24:50.318231   966 solver.cpp:244]     Train net output #0: loss = 0.148709 (* 1 = 0.148709 loss)
I0430 14:24:50.318238   966 sgd_solver.cpp:106] Iteration 187200, lr = 1e-26
I0430 14:25:39.907657   966 solver.cpp:228] Iteration 187300, loss = 0.155916
I0430 14:25:39.907807   966 solver.cpp:244]     Train net output #0: loss = 0.155916 (* 1 = 0.155916 loss)
I0430 14:25:39.907814   966 sgd_solver.cpp:106] Iteration 187300, lr = 1e-26
I0430 14:26:29.501318   966 solver.cpp:228] Iteration 187400, loss = 0.153989
I0430 14:26:29.501488   966 solver.cpp:244]     Train net output #0: loss = 0.153989 (* 1 = 0.153989 loss)
I0430 14:26:29.501494   966 sgd_solver.cpp:106] Iteration 187400, lr = 1e-26
I0430 14:27:20.304939   966 solver.cpp:228] Iteration 187500, loss = 0.0939211
I0430 14:27:20.305086   966 solver.cpp:244]     Train net output #0: loss = 0.0939211 (* 1 = 0.0939211 loss)
I0430 14:27:20.305094   966 sgd_solver.cpp:106] Iteration 187500, lr = 1e-27
I0430 14:28:09.882277   966 solver.cpp:228] Iteration 187600, loss = 0.139957
I0430 14:28:09.882405   966 solver.cpp:244]     Train net output #0: loss = 0.139957 (* 1 = 0.139957 loss)
I0430 14:28:09.882411   966 sgd_solver.cpp:106] Iteration 187600, lr = 1e-27
I0430 14:29:00.727835   966 solver.cpp:228] Iteration 187700, loss = 0.184441
I0430 14:29:00.727974   966 solver.cpp:244]     Train net output #0: loss = 0.184441 (* 1 = 0.184441 loss)
I0430 14:29:00.727980   966 sgd_solver.cpp:106] Iteration 187700, lr = 1e-27
I0430 14:29:50.318279   966 solver.cpp:228] Iteration 187800, loss = 0.130978
I0430 14:29:50.318430   966 solver.cpp:244]     Train net output #0: loss = 0.130978 (* 1 = 0.130978 loss)
I0430 14:29:50.318437   966 sgd_solver.cpp:106] Iteration 187800, lr = 1e-27
I0430 14:30:39.916368   966 solver.cpp:228] Iteration 187900, loss = 0.250046
I0430 14:30:39.916525   966 solver.cpp:244]     Train net output #0: loss = 0.250046 (* 1 = 0.250046 loss)
I0430 14:30:39.916532   966 sgd_solver.cpp:106] Iteration 187900, lr = 1e-27
I0430 14:31:30.680167   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_188000.caffemodel
I0430 14:31:31.049984   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_188000.solverstate
I0430 14:31:31.238355   966 solver.cpp:337] Iteration 188000, Testing net (#0)
I0430 14:31:31.238451   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 14:31:31.238456   966 net.cpp:693] Ignoring source layer visualize
I0430 14:31:31.238457   966 net.cpp:693] Ignoring source layer fake
I0430 14:36:23.109462   966 solver.cpp:404]     Test net output #0: loss = 0.181853 (* 1 = 0.181853 loss)
I0430 14:36:23.421736   966 solver.cpp:228] Iteration 188000, loss = 0.129096
I0430 14:36:23.421757   966 solver.cpp:244]     Train net output #0: loss = 0.129096 (* 1 = 0.129096 loss)
I0430 14:36:23.421779   966 sgd_solver.cpp:106] Iteration 188000, lr = 1e-27
I0430 14:37:13.014713   966 solver.cpp:228] Iteration 188100, loss = 0.108642
I0430 14:37:13.014849   966 solver.cpp:244]     Train net output #0: loss = 0.108642 (* 1 = 0.108642 loss)
I0430 14:37:13.014856   966 sgd_solver.cpp:106] Iteration 188100, lr = 1e-27
I0430 14:38:02.609288   966 solver.cpp:228] Iteration 188200, loss = 0.141581
I0430 14:38:02.609421   966 solver.cpp:244]     Train net output #0: loss = 0.141581 (* 1 = 0.141581 loss)
I0430 14:38:02.609426   966 sgd_solver.cpp:106] Iteration 188200, lr = 1e-27
I0430 14:38:53.887606   966 solver.cpp:228] Iteration 188300, loss = 0.120885
I0430 14:38:53.887745   966 solver.cpp:244]     Train net output #0: loss = 0.120885 (* 1 = 0.120885 loss)
I0430 14:38:53.887768   966 sgd_solver.cpp:106] Iteration 188300, lr = 1e-27
I0430 14:39:43.484927   966 solver.cpp:228] Iteration 188400, loss = 0.193228
I0430 14:39:43.485047   966 solver.cpp:244]     Train net output #0: loss = 0.193228 (* 1 = 0.193228 loss)
I0430 14:39:43.485054   966 sgd_solver.cpp:106] Iteration 188400, lr = 1e-27
I0430 14:40:33.090801   966 solver.cpp:228] Iteration 188500, loss = 0.359747
I0430 14:40:33.090940   966 solver.cpp:244]     Train net output #0: loss = 0.359747 (* 1 = 0.359747 loss)
I0430 14:40:33.090947   966 sgd_solver.cpp:106] Iteration 188500, lr = 1e-27
I0430 14:41:24.482635   966 solver.cpp:228] Iteration 188600, loss = 0.122759
I0430 14:41:24.482758   966 solver.cpp:244]     Train net output #0: loss = 0.122759 (* 1 = 0.122759 loss)
I0430 14:41:24.482765   966 sgd_solver.cpp:106] Iteration 188600, lr = 1e-27
I0430 14:42:14.078928   966 solver.cpp:228] Iteration 188700, loss = 0.153693
I0430 14:42:14.079063   966 solver.cpp:244]     Train net output #0: loss = 0.153693 (* 1 = 0.153693 loss)
I0430 14:42:14.079071   966 sgd_solver.cpp:106] Iteration 188700, lr = 1e-27
I0430 14:43:03.682000   966 solver.cpp:228] Iteration 188800, loss = 0.105246
I0430 14:43:03.682152   966 solver.cpp:244]     Train net output #0: loss = 0.105246 (* 1 = 0.105246 loss)
I0430 14:43:03.682158   966 sgd_solver.cpp:106] Iteration 188800, lr = 1e-27
I0430 14:43:53.286550   966 solver.cpp:228] Iteration 188900, loss = 0.10071
I0430 14:43:53.286691   966 solver.cpp:244]     Train net output #0: loss = 0.10071 (* 1 = 0.10071 loss)
I0430 14:43:53.286697   966 sgd_solver.cpp:106] Iteration 188900, lr = 1e-27
I0430 14:44:44.340127   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_189000.caffemodel
I0430 14:44:44.790814   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_189000.solverstate
I0430 14:44:44.974498   966 solver.cpp:337] Iteration 189000, Testing net (#0)
I0430 14:44:44.974577   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 14:44:44.974584   966 net.cpp:693] Ignoring source layer visualize
I0430 14:44:44.974587   966 net.cpp:693] Ignoring source layer fake
I0430 14:49:37.217679   966 solver.cpp:404]     Test net output #0: loss = 0.180658 (* 1 = 0.180658 loss)
I0430 14:49:37.527467   966 solver.cpp:228] Iteration 189000, loss = 0.0896853
I0430 14:49:37.527487   966 solver.cpp:244]     Train net output #0: loss = 0.0896853 (* 1 = 0.0896853 loss)
I0430 14:49:37.527493   966 sgd_solver.cpp:106] Iteration 189000, lr = 1e-27
I0430 14:50:27.122331   966 solver.cpp:228] Iteration 189100, loss = 0.171834
I0430 14:50:27.122493   966 solver.cpp:244]     Train net output #0: loss = 0.171834 (* 1 = 0.171834 loss)
I0430 14:50:27.122499   966 sgd_solver.cpp:106] Iteration 189100, lr = 1e-27
I0430 14:51:16.723469   966 solver.cpp:228] Iteration 189200, loss = 0.209597
I0430 14:51:16.723597   966 solver.cpp:244]     Train net output #0: loss = 0.209597 (* 1 = 0.209597 loss)
I0430 14:51:16.723603   966 sgd_solver.cpp:106] Iteration 189200, lr = 1e-27
I0430 14:52:08.056321   966 solver.cpp:228] Iteration 189300, loss = 0.280926
I0430 14:52:08.056448   966 solver.cpp:244]     Train net output #0: loss = 0.280926 (* 1 = 0.280926 loss)
I0430 14:52:08.056455   966 sgd_solver.cpp:106] Iteration 189300, lr = 1e-27
I0430 14:52:57.645355   966 solver.cpp:228] Iteration 189400, loss = 0.102514
I0430 14:52:57.645512   966 solver.cpp:244]     Train net output #0: loss = 0.102514 (* 1 = 0.102514 loss)
I0430 14:52:57.645520   966 sgd_solver.cpp:106] Iteration 189400, lr = 1e-27
I0430 14:53:47.250322   966 solver.cpp:228] Iteration 189500, loss = 0.124905
I0430 14:53:47.250479   966 solver.cpp:244]     Train net output #0: loss = 0.124905 (* 1 = 0.124905 loss)
I0430 14:53:47.250485   966 sgd_solver.cpp:106] Iteration 189500, lr = 1e-27
I0430 14:54:36.848206   966 solver.cpp:228] Iteration 189600, loss = 0.148331
I0430 14:54:36.848356   966 solver.cpp:244]     Train net output #0: loss = 0.148331 (* 1 = 0.148331 loss)
I0430 14:54:36.848361   966 sgd_solver.cpp:106] Iteration 189600, lr = 1e-27
I0430 14:55:28.163995   966 solver.cpp:228] Iteration 189700, loss = 0.0971278
I0430 14:55:28.164180   966 solver.cpp:244]     Train net output #0: loss = 0.0971278 (* 1 = 0.0971278 loss)
I0430 14:55:28.164188   966 sgd_solver.cpp:106] Iteration 189700, lr = 1e-27
I0430 14:56:17.756705   966 solver.cpp:228] Iteration 189800, loss = 0.139128
I0430 14:56:17.756849   966 solver.cpp:244]     Train net output #0: loss = 0.139128 (* 1 = 0.139128 loss)
I0430 14:56:17.756855   966 sgd_solver.cpp:106] Iteration 189800, lr = 1e-27
I0430 14:57:07.361078   966 solver.cpp:228] Iteration 189900, loss = 0.148945
I0430 14:57:07.361232   966 solver.cpp:244]     Train net output #0: loss = 0.148945 (* 1 = 0.148945 loss)
I0430 14:57:07.361238   966 sgd_solver.cpp:106] Iteration 189900, lr = 1e-27
I0430 14:57:58.362993   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_190000.caffemodel
I0430 14:57:58.904896   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_190000.solverstate
I0430 14:57:59.087224   966 solver.cpp:337] Iteration 190000, Testing net (#0)
I0430 14:57:59.087287   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 14:57:59.087291   966 net.cpp:693] Ignoring source layer visualize
I0430 14:57:59.087293   966 net.cpp:693] Ignoring source layer fake
I0430 15:02:51.442970   966 solver.cpp:404]     Test net output #0: loss = 0.180655 (* 1 = 0.180655 loss)
I0430 15:02:51.752964   966 solver.cpp:228] Iteration 190000, loss = 0.0936769
I0430 15:02:51.752984   966 solver.cpp:244]     Train net output #0: loss = 0.0936769 (* 1 = 0.0936769 loss)
I0430 15:02:51.753005   966 sgd_solver.cpp:106] Iteration 190000, lr = 1e-27
I0430 15:03:41.345626   966 solver.cpp:228] Iteration 190100, loss = 0.13961
I0430 15:03:41.345763   966 solver.cpp:244]     Train net output #0: loss = 0.13961 (* 1 = 0.13961 loss)
I0430 15:03:41.345770   966 sgd_solver.cpp:106] Iteration 190100, lr = 1e-27
I0430 15:04:30.940191   966 solver.cpp:228] Iteration 190200, loss = 0.111231
I0430 15:04:30.940320   966 solver.cpp:244]     Train net output #0: loss = 0.111231 (* 1 = 0.111231 loss)
I0430 15:04:30.940326   966 sgd_solver.cpp:106] Iteration 190200, lr = 1e-27
I0430 15:05:22.118042   966 solver.cpp:228] Iteration 190300, loss = 0.0977502
I0430 15:05:22.118196   966 solver.cpp:244]     Train net output #0: loss = 0.0977502 (* 1 = 0.0977502 loss)
I0430 15:05:22.118203   966 sgd_solver.cpp:106] Iteration 190300, lr = 1e-27
I0430 15:06:11.700939   966 solver.cpp:228] Iteration 190400, loss = 0.0608476
I0430 15:06:11.701107   966 solver.cpp:244]     Train net output #0: loss = 0.0608476 (* 1 = 0.0608476 loss)
I0430 15:06:11.701113   966 sgd_solver.cpp:106] Iteration 190400, lr = 1e-27
I0430 15:07:01.295238   966 solver.cpp:228] Iteration 190500, loss = 0.434335
I0430 15:07:01.295402   966 solver.cpp:244]     Train net output #0: loss = 0.434335 (* 1 = 0.434335 loss)
I0430 15:07:01.295408   966 sgd_solver.cpp:106] Iteration 190500, lr = 1e-27
I0430 15:07:52.115703   966 solver.cpp:228] Iteration 190600, loss = 0.13048
I0430 15:07:52.115836   966 solver.cpp:244]     Train net output #0: loss = 0.13048 (* 1 = 0.13048 loss)
I0430 15:07:52.115842   966 sgd_solver.cpp:106] Iteration 190600, lr = 1e-27
I0430 15:08:41.716110   966 solver.cpp:228] Iteration 190700, loss = 0.118339
I0430 15:08:41.716244   966 solver.cpp:244]     Train net output #0: loss = 0.118339 (* 1 = 0.118339 loss)
I0430 15:08:41.716250   966 sgd_solver.cpp:106] Iteration 190700, lr = 1e-27
I0430 15:09:31.318789   966 solver.cpp:228] Iteration 190800, loss = 0.114525
I0430 15:09:31.318919   966 solver.cpp:244]     Train net output #0: loss = 0.114525 (* 1 = 0.114525 loss)
I0430 15:09:31.318925   966 sgd_solver.cpp:106] Iteration 190800, lr = 1e-27
I0430 15:10:22.188571   966 solver.cpp:228] Iteration 190900, loss = 0.214466
I0430 15:10:22.188711   966 solver.cpp:244]     Train net output #0: loss = 0.214466 (* 1 = 0.214466 loss)
I0430 15:10:22.188719   966 sgd_solver.cpp:106] Iteration 190900, lr = 1e-27
I0430 15:11:11.481817   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_191000.caffemodel
I0430 15:11:12.316494   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_191000.solverstate
I0430 15:11:12.496450   966 solver.cpp:337] Iteration 191000, Testing net (#0)
I0430 15:11:12.496516   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 15:11:12.496520   966 net.cpp:693] Ignoring source layer visualize
I0430 15:11:12.496520   966 net.cpp:693] Ignoring source layer fake
I0430 15:16:05.018707   966 solver.cpp:404]     Test net output #0: loss = 0.192018 (* 1 = 0.192018 loss)
I0430 15:16:05.330596   966 solver.cpp:228] Iteration 191000, loss = 0.0970089
I0430 15:16:05.330632   966 solver.cpp:244]     Train net output #0: loss = 0.0970089 (* 1 = 0.0970089 loss)
I0430 15:16:05.330639   966 sgd_solver.cpp:106] Iteration 191000, lr = 1e-27
I0430 15:16:56.425606   966 solver.cpp:228] Iteration 191100, loss = 0.113498
I0430 15:16:56.425777   966 solver.cpp:244]     Train net output #0: loss = 0.113498 (* 1 = 0.113498 loss)
I0430 15:16:56.425784   966 sgd_solver.cpp:106] Iteration 191100, lr = 1e-27
I0430 15:17:46.024281   966 solver.cpp:228] Iteration 191200, loss = 0.0786607
I0430 15:17:46.024428   966 solver.cpp:244]     Train net output #0: loss = 0.0786607 (* 1 = 0.0786607 loss)
I0430 15:17:46.024435   966 sgd_solver.cpp:106] Iteration 191200, lr = 1e-27
I0430 15:18:35.631825   966 solver.cpp:228] Iteration 191300, loss = 0.101727
I0430 15:18:35.631963   966 solver.cpp:244]     Train net output #0: loss = 0.101727 (* 1 = 0.101727 loss)
I0430 15:18:35.631969   966 sgd_solver.cpp:106] Iteration 191300, lr = 1e-27
I0430 15:19:26.921588   966 solver.cpp:228] Iteration 191400, loss = 0.123356
I0430 15:19:26.922919   966 solver.cpp:244]     Train net output #0: loss = 0.123356 (* 1 = 0.123356 loss)
I0430 15:19:26.922942   966 sgd_solver.cpp:106] Iteration 191400, lr = 1e-27
I0430 15:20:16.521379   966 solver.cpp:228] Iteration 191500, loss = 0.185839
I0430 15:20:16.521544   966 solver.cpp:244]     Train net output #0: loss = 0.185839 (* 1 = 0.185839 loss)
I0430 15:20:16.521551   966 sgd_solver.cpp:106] Iteration 191500, lr = 1e-27
I0430 15:21:06.116660   966 solver.cpp:228] Iteration 191600, loss = 0.149241
I0430 15:21:06.116822   966 solver.cpp:244]     Train net output #0: loss = 0.149241 (* 1 = 0.149241 loss)
I0430 15:21:06.116830   966 sgd_solver.cpp:106] Iteration 191600, lr = 1e-27
I0430 15:21:55.723932   966 solver.cpp:228] Iteration 191700, loss = 0.156672
I0430 15:21:55.724098   966 solver.cpp:244]     Train net output #0: loss = 0.156672 (* 1 = 0.156672 loss)
I0430 15:21:55.724104   966 sgd_solver.cpp:106] Iteration 191700, lr = 1e-27
I0430 15:22:47.090332   966 solver.cpp:228] Iteration 191800, loss = 0.153641
I0430 15:22:47.090469   966 solver.cpp:244]     Train net output #0: loss = 0.153641 (* 1 = 0.153641 loss)
I0430 15:22:47.090476   966 sgd_solver.cpp:106] Iteration 191800, lr = 1e-27
I0430 15:23:36.690268   966 solver.cpp:228] Iteration 191900, loss = 0.20223
I0430 15:23:36.690402   966 solver.cpp:244]     Train net output #0: loss = 0.20223 (* 1 = 0.20223 loss)
I0430 15:23:36.690407   966 sgd_solver.cpp:106] Iteration 191900, lr = 1e-27
I0430 15:24:25.989213   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_192000.caffemodel
I0430 15:24:26.620668   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_192000.solverstate
I0430 15:24:26.810497   966 solver.cpp:337] Iteration 192000, Testing net (#0)
I0430 15:24:26.810575   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 15:24:26.810578   966 net.cpp:693] Ignoring source layer visualize
I0430 15:24:26.810580   966 net.cpp:693] Ignoring source layer fake
I0430 15:29:19.188087   966 solver.cpp:404]     Test net output #0: loss = 0.186006 (* 1 = 0.186006 loss)
I0430 15:29:19.497855   966 solver.cpp:228] Iteration 192000, loss = 0.191249
I0430 15:29:19.497875   966 solver.cpp:244]     Train net output #0: loss = 0.191249 (* 1 = 0.191249 loss)
I0430 15:29:19.497881   966 sgd_solver.cpp:106] Iteration 192000, lr = 1e-27
I0430 15:30:10.880111   966 solver.cpp:228] Iteration 192100, loss = 0.136716
I0430 15:30:10.880244   966 solver.cpp:244]     Train net output #0: loss = 0.136716 (* 1 = 0.136716 loss)
I0430 15:30:10.880250   966 sgd_solver.cpp:106] Iteration 192100, lr = 1e-27
I0430 15:31:00.471720   966 solver.cpp:228] Iteration 192200, loss = 0.083047
I0430 15:31:00.471849   966 solver.cpp:244]     Train net output #0: loss = 0.083047 (* 1 = 0.083047 loss)
I0430 15:31:00.471856   966 sgd_solver.cpp:106] Iteration 192200, lr = 1e-27
I0430 15:31:50.073390   966 solver.cpp:228] Iteration 192300, loss = 0.115088
I0430 15:31:50.073559   966 solver.cpp:244]     Train net output #0: loss = 0.115088 (* 1 = 0.115088 loss)
I0430 15:31:50.073566   966 sgd_solver.cpp:106] Iteration 192300, lr = 1e-27
I0430 15:32:39.671890   966 solver.cpp:228] Iteration 192400, loss = 0.112267
I0430 15:32:39.672071   966 solver.cpp:244]     Train net output #0: loss = 0.112267 (* 1 = 0.112267 loss)
I0430 15:32:39.672078   966 sgd_solver.cpp:106] Iteration 192400, lr = 1e-27
I0430 15:33:31.019997   966 solver.cpp:228] Iteration 192500, loss = 0.0953263
I0430 15:33:31.020159   966 solver.cpp:244]     Train net output #0: loss = 0.0953263 (* 1 = 0.0953263 loss)
I0430 15:33:31.020164   966 sgd_solver.cpp:106] Iteration 192500, lr = 1e-27
I0430 15:34:20.616382   966 solver.cpp:228] Iteration 192600, loss = 0.110741
I0430 15:34:20.616546   966 solver.cpp:244]     Train net output #0: loss = 0.110741 (* 1 = 0.110741 loss)
I0430 15:34:20.616552   966 sgd_solver.cpp:106] Iteration 192600, lr = 1e-27
I0430 15:35:10.219743   966 solver.cpp:228] Iteration 192700, loss = 0.263726
I0430 15:35:10.219904   966 solver.cpp:244]     Train net output #0: loss = 0.263726 (* 1 = 0.263726 loss)
I0430 15:35:10.219911   966 sgd_solver.cpp:106] Iteration 192700, lr = 1e-27
I0430 15:36:01.526440   966 solver.cpp:228] Iteration 192800, loss = 0.124193
I0430 15:36:01.526587   966 solver.cpp:244]     Train net output #0: loss = 0.124193 (* 1 = 0.124193 loss)
I0430 15:36:01.526592   966 sgd_solver.cpp:106] Iteration 192800, lr = 1e-27
I0430 15:36:51.120649   966 solver.cpp:228] Iteration 192900, loss = 0.156223
I0430 15:36:51.120786   966 solver.cpp:244]     Train net output #0: loss = 0.156223 (* 1 = 0.156223 loss)
I0430 15:36:51.120792   966 sgd_solver.cpp:106] Iteration 192900, lr = 1e-27
I0430 15:37:40.409034   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_193000.caffemodel
I0430 15:37:52.937016   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_193000.solverstate
I0430 15:37:53.124536   966 solver.cpp:337] Iteration 193000, Testing net (#0)
I0430 15:37:53.124614   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 15:37:53.124632   966 net.cpp:693] Ignoring source layer visualize
I0430 15:37:53.124635   966 net.cpp:693] Ignoring source layer fake
I0430 15:42:45.055040   966 solver.cpp:404]     Test net output #0: loss = 0.182048 (* 1 = 0.182048 loss)
I0430 15:42:45.364732   966 solver.cpp:228] Iteration 193000, loss = 0.182014
I0430 15:42:45.364753   966 solver.cpp:244]     Train net output #0: loss = 0.182014 (* 1 = 0.182014 loss)
I0430 15:42:45.364758   966 sgd_solver.cpp:106] Iteration 193000, lr = 1e-27
I0430 15:43:36.670276   966 solver.cpp:228] Iteration 193100, loss = 0.114411
I0430 15:43:36.670503   966 solver.cpp:244]     Train net output #0: loss = 0.114411 (* 1 = 0.114411 loss)
I0430 15:43:36.670509   966 sgd_solver.cpp:106] Iteration 193100, lr = 1e-27
I0430 15:44:26.255791   966 solver.cpp:228] Iteration 193200, loss = 0.140925
I0430 15:44:26.255947   966 solver.cpp:244]     Train net output #0: loss = 0.140925 (* 1 = 0.140925 loss)
I0430 15:44:26.255954   966 sgd_solver.cpp:106] Iteration 193200, lr = 1e-27
I0430 15:45:15.840222   966 solver.cpp:228] Iteration 193300, loss = 0.198506
I0430 15:45:15.840368   966 solver.cpp:244]     Train net output #0: loss = 0.198506 (* 1 = 0.198506 loss)
I0430 15:45:15.840374   966 sgd_solver.cpp:106] Iteration 193300, lr = 1e-27
I0430 15:46:05.432898   966 solver.cpp:228] Iteration 193400, loss = 0.105242
I0430 15:46:05.433049   966 solver.cpp:244]     Train net output #0: loss = 0.105242 (* 1 = 0.105242 loss)
I0430 15:46:05.433056   966 sgd_solver.cpp:106] Iteration 193400, lr = 1e-27
I0430 15:46:56.580979   966 solver.cpp:228] Iteration 193500, loss = 0.117652
I0430 15:46:56.581120   966 solver.cpp:244]     Train net output #0: loss = 0.117652 (* 1 = 0.117652 loss)
I0430 15:46:56.581126   966 sgd_solver.cpp:106] Iteration 193500, lr = 1e-27
I0430 15:47:46.168893   966 solver.cpp:228] Iteration 193600, loss = 0.0960296
I0430 15:47:46.169049   966 solver.cpp:244]     Train net output #0: loss = 0.0960296 (* 1 = 0.0960296 loss)
I0430 15:47:46.169054   966 sgd_solver.cpp:106] Iteration 193600, lr = 1e-27
I0430 15:48:35.758016   966 solver.cpp:228] Iteration 193700, loss = 0.403699
I0430 15:48:35.758167   966 solver.cpp:244]     Train net output #0: loss = 0.403699 (* 1 = 0.403699 loss)
I0430 15:48:35.758173   966 sgd_solver.cpp:106] Iteration 193700, lr = 1e-27
I0430 15:49:26.560752   966 solver.cpp:228] Iteration 193800, loss = 0.10496
I0430 15:49:26.560915   966 solver.cpp:244]     Train net output #0: loss = 0.10496 (* 1 = 0.10496 loss)
I0430 15:49:26.560922   966 sgd_solver.cpp:106] Iteration 193800, lr = 1e-27
I0430 15:50:16.152705   966 solver.cpp:228] Iteration 193900, loss = 0.0982003
I0430 15:50:16.153076   966 solver.cpp:244]     Train net output #0: loss = 0.0982003 (* 1 = 0.0982003 loss)
I0430 15:50:16.153084   966 sgd_solver.cpp:106] Iteration 193900, lr = 1e-27
I0430 15:51:06.715518   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_194000.caffemodel
I0430 15:51:12.268820   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_194000.solverstate
I0430 15:51:12.451987   966 solver.cpp:337] Iteration 194000, Testing net (#0)
I0430 15:51:12.452051   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 15:51:12.452055   966 net.cpp:693] Ignoring source layer visualize
I0430 15:51:12.452057   966 net.cpp:693] Ignoring source layer fake
I0430 15:56:04.368424   966 solver.cpp:404]     Test net output #0: loss = 0.180163 (* 1 = 0.180163 loss)
I0430 15:56:04.678062   966 solver.cpp:228] Iteration 194000, loss = 0.116724
I0430 15:56:04.678099   966 solver.cpp:244]     Train net output #0: loss = 0.116724 (* 1 = 0.116724 loss)
I0430 15:56:04.678105   966 sgd_solver.cpp:106] Iteration 194000, lr = 1e-27
I0430 15:56:54.280763   966 solver.cpp:228] Iteration 194100, loss = 0.170541
I0430 15:56:54.280899   966 solver.cpp:244]     Train net output #0: loss = 0.170541 (* 1 = 0.170541 loss)
I0430 15:56:54.280905   966 sgd_solver.cpp:106] Iteration 194100, lr = 1e-27
I0430 15:57:43.877532   966 solver.cpp:228] Iteration 194200, loss = 0.166406
I0430 15:57:43.877645   966 solver.cpp:244]     Train net output #0: loss = 0.166406 (* 1 = 0.166406 loss)
I0430 15:57:43.877650   966 sgd_solver.cpp:106] Iteration 194200, lr = 1e-27
I0430 15:58:34.934767   966 solver.cpp:228] Iteration 194300, loss = 0.0910331
I0430 15:58:34.934895   966 solver.cpp:244]     Train net output #0: loss = 0.0910331 (* 1 = 0.0910331 loss)
I0430 15:58:34.934902   966 sgd_solver.cpp:106] Iteration 194300, lr = 1e-27
I0430 15:59:24.531796   966 solver.cpp:228] Iteration 194400, loss = 0.108667
I0430 15:59:24.531922   966 solver.cpp:244]     Train net output #0: loss = 0.108667 (* 1 = 0.108667 loss)
I0430 15:59:24.531929   966 sgd_solver.cpp:106] Iteration 194400, lr = 1e-27
I0430 16:00:15.835507   966 solver.cpp:228] Iteration 194500, loss = 0.0730969
I0430 16:00:15.835646   966 solver.cpp:244]     Train net output #0: loss = 0.0730969 (* 1 = 0.0730969 loss)
I0430 16:00:15.835654   966 sgd_solver.cpp:106] Iteration 194500, lr = 1e-27
I0430 16:01:05.438072   966 solver.cpp:228] Iteration 194600, loss = 0.16533
I0430 16:01:05.438215   966 solver.cpp:244]     Train net output #0: loss = 0.16533 (* 1 = 0.16533 loss)
I0430 16:01:05.438221   966 sgd_solver.cpp:106] Iteration 194600, lr = 1e-27
I0430 16:01:55.046728   966 solver.cpp:228] Iteration 194700, loss = 0.154276
I0430 16:01:55.046864   966 solver.cpp:244]     Train net output #0: loss = 0.154276 (* 1 = 0.154276 loss)
I0430 16:01:55.046871   966 sgd_solver.cpp:106] Iteration 194700, lr = 1e-27
I0430 16:02:44.661916   966 solver.cpp:228] Iteration 194800, loss = 0.0875387
I0430 16:02:44.662093   966 solver.cpp:244]     Train net output #0: loss = 0.0875387 (* 1 = 0.0875387 loss)
I0430 16:02:44.662099   966 sgd_solver.cpp:106] Iteration 194800, lr = 1e-27
I0430 16:03:36.048046   966 solver.cpp:228] Iteration 194900, loss = 0.166137
I0430 16:03:36.048213   966 solver.cpp:244]     Train net output #0: loss = 0.166137 (* 1 = 0.166137 loss)
I0430 16:03:36.048219   966 sgd_solver.cpp:106] Iteration 194900, lr = 1e-27
I0430 16:04:25.344585   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_195000.caffemodel
I0430 16:04:39.626449   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_195000.solverstate
I0430 16:04:39.817548   966 solver.cpp:337] Iteration 195000, Testing net (#0)
I0430 16:04:39.817618   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 16:04:39.817625   966 net.cpp:693] Ignoring source layer visualize
I0430 16:04:39.817626   966 net.cpp:693] Ignoring source layer fake
I0430 16:09:31.804023   966 solver.cpp:404]     Test net output #0: loss = 0.181513 (* 1 = 0.181513 loss)
I0430 16:09:32.114645   966 solver.cpp:228] Iteration 195000, loss = 0.0877892
I0430 16:09:32.114665   966 solver.cpp:244]     Train net output #0: loss = 0.0877892 (* 1 = 0.0877892 loss)
I0430 16:09:32.114671   966 sgd_solver.cpp:106] Iteration 195000, lr = 1e-28
I0430 16:10:21.730332   966 solver.cpp:228] Iteration 195100, loss = 0.051519
I0430 16:10:21.730468   966 solver.cpp:244]     Train net output #0: loss = 0.051519 (* 1 = 0.051519 loss)
I0430 16:10:21.730474   966 sgd_solver.cpp:106] Iteration 195100, lr = 1e-28
I0430 16:11:13.125213   966 solver.cpp:228] Iteration 195200, loss = 0.118602
I0430 16:11:13.125375   966 solver.cpp:244]     Train net output #0: loss = 0.118602 (* 1 = 0.118602 loss)
I0430 16:11:13.125382   966 sgd_solver.cpp:106] Iteration 195200, lr = 1e-28
I0430 16:12:02.729665   966 solver.cpp:228] Iteration 195300, loss = 0.156912
I0430 16:12:02.729830   966 solver.cpp:244]     Train net output #0: loss = 0.156912 (* 1 = 0.156912 loss)
I0430 16:12:02.729836   966 sgd_solver.cpp:106] Iteration 195300, lr = 1e-28
I0430 16:12:52.332345   966 solver.cpp:228] Iteration 195400, loss = 0.194651
I0430 16:12:52.332528   966 solver.cpp:244]     Train net output #0: loss = 0.194651 (* 1 = 0.194651 loss)
I0430 16:12:52.332536   966 sgd_solver.cpp:106] Iteration 195400, lr = 1e-28
I0430 16:13:41.927187   966 solver.cpp:228] Iteration 195500, loss = 0.176096
I0430 16:13:41.927351   966 solver.cpp:244]     Train net output #0: loss = 0.176096 (* 1 = 0.176096 loss)
I0430 16:13:41.927357   966 sgd_solver.cpp:106] Iteration 195500, lr = 1e-28
I0430 16:14:33.298857   966 solver.cpp:228] Iteration 195600, loss = 0.127446
I0430 16:14:33.298985   966 solver.cpp:244]     Train net output #0: loss = 0.127446 (* 1 = 0.127446 loss)
I0430 16:14:33.298992   966 sgd_solver.cpp:106] Iteration 195600, lr = 1e-28
I0430 16:15:22.903126   966 solver.cpp:228] Iteration 195700, loss = 0.0988351
I0430 16:15:22.903259   966 solver.cpp:244]     Train net output #0: loss = 0.0988351 (* 1 = 0.0988351 loss)
I0430 16:15:22.903264   966 sgd_solver.cpp:106] Iteration 195700, lr = 1e-28
I0430 16:16:12.512712   966 solver.cpp:228] Iteration 195800, loss = 0.116953
I0430 16:16:12.512832   966 solver.cpp:244]     Train net output #0: loss = 0.116953 (* 1 = 0.116953 loss)
I0430 16:16:12.512838   966 sgd_solver.cpp:106] Iteration 195800, lr = 1e-28
I0430 16:17:03.838062   966 solver.cpp:228] Iteration 195900, loss = 0.133893
I0430 16:17:03.838202   966 solver.cpp:244]     Train net output #0: loss = 0.133893 (* 1 = 0.133893 loss)
I0430 16:17:03.838208   966 sgd_solver.cpp:106] Iteration 195900, lr = 1e-28
I0430 16:17:53.117558   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_196000.caffemodel
I0430 16:18:00.122349   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_196000.solverstate
I0430 16:18:00.311717   966 solver.cpp:337] Iteration 196000, Testing net (#0)
I0430 16:18:00.311813   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 16:18:00.311816   966 net.cpp:693] Ignoring source layer visualize
I0430 16:18:00.311818   966 net.cpp:693] Ignoring source layer fake
I0430 16:22:52.800828   966 solver.cpp:404]     Test net output #0: loss = 0.191447 (* 1 = 0.191447 loss)
I0430 16:22:53.111400   966 solver.cpp:228] Iteration 196000, loss = 0.0910046
I0430 16:22:53.111420   966 solver.cpp:244]     Train net output #0: loss = 0.0910046 (* 1 = 0.0910046 loss)
I0430 16:22:53.111426   966 sgd_solver.cpp:106] Iteration 196000, lr = 1e-28
I0430 16:23:42.716842   966 solver.cpp:228] Iteration 196100, loss = 0.152817
I0430 16:23:42.717000   966 solver.cpp:244]     Train net output #0: loss = 0.152817 (* 1 = 0.152817 loss)
I0430 16:23:42.717006   966 sgd_solver.cpp:106] Iteration 196100, lr = 1e-28
I0430 16:24:34.071045   966 solver.cpp:228] Iteration 196200, loss = 0.0593027
I0430 16:24:34.071202   966 solver.cpp:244]     Train net output #0: loss = 0.0593027 (* 1 = 0.0593027 loss)
I0430 16:24:34.071208   966 sgd_solver.cpp:106] Iteration 196200, lr = 1e-28
I0430 16:25:23.657248   966 solver.cpp:228] Iteration 196300, loss = 0.0747642
I0430 16:25:23.657379   966 solver.cpp:244]     Train net output #0: loss = 0.0747642 (* 1 = 0.0747642 loss)
I0430 16:25:23.657387   966 sgd_solver.cpp:106] Iteration 196300, lr = 1e-28
I0430 16:26:13.249620   966 solver.cpp:228] Iteration 196400, loss = 0.0902058
I0430 16:26:13.249768   966 solver.cpp:244]     Train net output #0: loss = 0.0902058 (* 1 = 0.0902058 loss)
I0430 16:26:13.249773   966 sgd_solver.cpp:106] Iteration 196400, lr = 1e-28
I0430 16:27:02.846901   966 solver.cpp:228] Iteration 196500, loss = 0.141727
I0430 16:27:02.847048   966 solver.cpp:244]     Train net output #0: loss = 0.141727 (* 1 = 0.141727 loss)
I0430 16:27:02.847054   966 sgd_solver.cpp:106] Iteration 196500, lr = 1e-28
I0430 16:27:54.039228   966 solver.cpp:228] Iteration 196600, loss = 0.363108
I0430 16:27:54.039417   966 solver.cpp:244]     Train net output #0: loss = 0.363108 (* 1 = 0.363108 loss)
I0430 16:27:54.039424   966 sgd_solver.cpp:106] Iteration 196600, lr = 1e-28
I0430 16:28:43.626510   966 solver.cpp:228] Iteration 196700, loss = 0.108743
I0430 16:28:43.626670   966 solver.cpp:244]     Train net output #0: loss = 0.108743 (* 1 = 0.108743 loss)
I0430 16:28:43.626677   966 sgd_solver.cpp:106] Iteration 196700, lr = 1e-28
I0430 16:29:33.223616   966 solver.cpp:228] Iteration 196800, loss = 0.123355
I0430 16:29:33.223747   966 solver.cpp:244]     Train net output #0: loss = 0.123355 (* 1 = 0.123355 loss)
I0430 16:29:33.223752   966 sgd_solver.cpp:106] Iteration 196800, lr = 1e-28
I0430 16:30:24.039727   966 solver.cpp:228] Iteration 196900, loss = 0.0836349
I0430 16:30:24.039866   966 solver.cpp:244]     Train net output #0: loss = 0.0836349 (* 1 = 0.0836349 loss)
I0430 16:30:24.039872   966 sgd_solver.cpp:106] Iteration 196900, lr = 1e-28
I0430 16:31:13.330888   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_197000.caffemodel
I0430 16:31:19.839548   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_197000.solverstate
I0430 16:31:20.030146   966 solver.cpp:337] Iteration 197000, Testing net (#0)
I0430 16:31:20.030241   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 16:31:20.030247   966 net.cpp:693] Ignoring source layer visualize
I0430 16:31:20.030251   966 net.cpp:693] Ignoring source layer fake
I0430 16:36:12.437124   966 solver.cpp:404]     Test net output #0: loss = 0.185859 (* 1 = 0.185859 loss)
I0430 16:36:12.748102   966 solver.cpp:228] Iteration 197000, loss = 0.0811018
I0430 16:36:12.748122   966 solver.cpp:244]     Train net output #0: loss = 0.0811018 (* 1 = 0.0811018 loss)
I0430 16:36:12.748128   966 sgd_solver.cpp:106] Iteration 197000, lr = 1e-28
I0430 16:37:03.625952   966 solver.cpp:228] Iteration 197100, loss = 0.170314
I0430 16:37:03.626111   966 solver.cpp:244]     Train net output #0: loss = 0.170314 (* 1 = 0.170314 loss)
I0430 16:37:03.626117   966 sgd_solver.cpp:106] Iteration 197100, lr = 1e-28
I0430 16:37:53.228651   966 solver.cpp:228] Iteration 197200, loss = 0.121291
I0430 16:37:53.229277   966 solver.cpp:244]     Train net output #0: loss = 0.121291 (* 1 = 0.121291 loss)
I0430 16:37:53.229300   966 sgd_solver.cpp:106] Iteration 197200, lr = 1e-28
I0430 16:38:42.827666   966 solver.cpp:228] Iteration 197300, loss = 0.0510807
I0430 16:38:42.827800   966 solver.cpp:244]     Train net output #0: loss = 0.0510807 (* 1 = 0.0510807 loss)
I0430 16:38:42.827805   966 sgd_solver.cpp:106] Iteration 197300, lr = 1e-28
I0430 16:39:33.897166   966 solver.cpp:228] Iteration 197400, loss = 0.058025
I0430 16:39:33.897306   966 solver.cpp:244]     Train net output #0: loss = 0.058025 (* 1 = 0.058025 loss)
I0430 16:39:33.897313   966 sgd_solver.cpp:106] Iteration 197400, lr = 1e-28
I0430 16:40:23.498531   966 solver.cpp:228] Iteration 197500, loss = 0.15794
I0430 16:40:23.498661   966 solver.cpp:244]     Train net output #0: loss = 0.15794 (* 1 = 0.15794 loss)
I0430 16:40:23.498667   966 sgd_solver.cpp:106] Iteration 197500, lr = 1e-28
I0430 16:41:13.101336   966 solver.cpp:228] Iteration 197600, loss = 0.0780721
I0430 16:41:13.101459   966 solver.cpp:244]     Train net output #0: loss = 0.0780721 (* 1 = 0.0780721 loss)
I0430 16:41:13.101466   966 sgd_solver.cpp:106] Iteration 197600, lr = 1e-28
I0430 16:42:04.428431   966 solver.cpp:228] Iteration 197700, loss = 0.05952
I0430 16:42:04.428552   966 solver.cpp:244]     Train net output #0: loss = 0.05952 (* 1 = 0.05952 loss)
I0430 16:42:04.428558   966 sgd_solver.cpp:106] Iteration 197700, lr = 1e-28
I0430 16:42:54.030520   966 solver.cpp:228] Iteration 197800, loss = 0.115758
I0430 16:42:54.030675   966 solver.cpp:244]     Train net output #0: loss = 0.115758 (* 1 = 0.115758 loss)
I0430 16:42:54.030681   966 sgd_solver.cpp:106] Iteration 197800, lr = 1e-28
I0430 16:43:43.639686   966 solver.cpp:228] Iteration 197900, loss = 0.208923
I0430 16:43:43.639858   966 solver.cpp:244]     Train net output #0: loss = 0.208923 (* 1 = 0.208923 loss)
I0430 16:43:43.639863   966 sgd_solver.cpp:106] Iteration 197900, lr = 1e-28
I0430 16:44:34.712505   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_198000.caffemodel
I0430 16:44:51.377351   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_198000.solverstate
I0430 16:44:51.564924   966 solver.cpp:337] Iteration 198000, Testing net (#0)
I0430 16:44:51.564988   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 16:44:51.564992   966 net.cpp:693] Ignoring source layer visualize
I0430 16:44:51.564993   966 net.cpp:693] Ignoring source layer fake
I0430 16:49:43.512504   966 solver.cpp:404]     Test net output #0: loss = 0.182376 (* 1 = 0.182376 loss)
I0430 16:49:43.822141   966 solver.cpp:228] Iteration 198000, loss = 0.13022
I0430 16:49:43.822162   966 solver.cpp:244]     Train net output #0: loss = 0.13022 (* 1 = 0.13022 loss)
I0430 16:49:43.822185   966 sgd_solver.cpp:106] Iteration 198000, lr = 1e-28
I0430 16:50:33.425328   966 solver.cpp:228] Iteration 198100, loss = 0.224363
I0430 16:50:33.425482   966 solver.cpp:244]     Train net output #0: loss = 0.224363 (* 1 = 0.224363 loss)
I0430 16:50:33.425489   966 sgd_solver.cpp:106] Iteration 198100, lr = 1e-28
I0430 16:51:23.040522   966 solver.cpp:228] Iteration 198200, loss = 0.119522
I0430 16:51:23.040663   966 solver.cpp:244]     Train net output #0: loss = 0.119522 (* 1 = 0.119522 loss)
I0430 16:51:23.040669   966 sgd_solver.cpp:106] Iteration 198200, lr = 1e-28
I0430 16:52:12.647718   966 solver.cpp:228] Iteration 198300, loss = 0.108305
I0430 16:52:12.647835   966 solver.cpp:244]     Train net output #0: loss = 0.108305 (* 1 = 0.108305 loss)
I0430 16:52:12.647841   966 sgd_solver.cpp:106] Iteration 198300, lr = 1e-28
I0430 16:53:04.031975   966 solver.cpp:228] Iteration 198400, loss = 0.170915
I0430 16:53:04.032135   966 solver.cpp:244]     Train net output #0: loss = 0.170915 (* 1 = 0.170915 loss)
I0430 16:53:04.032142   966 sgd_solver.cpp:106] Iteration 198400, lr = 1e-28
I0430 16:53:53.639750   966 solver.cpp:228] Iteration 198500, loss = 0.206243
I0430 16:53:53.639873   966 solver.cpp:244]     Train net output #0: loss = 0.206243 (* 1 = 0.206243 loss)
I0430 16:53:53.639880   966 sgd_solver.cpp:106] Iteration 198500, lr = 1e-28
I0430 16:54:43.250921   966 solver.cpp:228] Iteration 198600, loss = 0.152722
I0430 16:54:43.251049   966 solver.cpp:244]     Train net output #0: loss = 0.152722 (* 1 = 0.152722 loss)
I0430 16:54:43.251056   966 sgd_solver.cpp:106] Iteration 198600, lr = 1e-28
I0430 16:55:34.610397   966 solver.cpp:228] Iteration 198700, loss = 0.150029
I0430 16:55:34.610525   966 solver.cpp:244]     Train net output #0: loss = 0.150029 (* 1 = 0.150029 loss)
I0430 16:55:34.610532   966 sgd_solver.cpp:106] Iteration 198700, lr = 1e-28
I0430 16:56:24.217325   966 solver.cpp:228] Iteration 198800, loss = 0.130586
I0430 16:56:24.217494   966 solver.cpp:244]     Train net output #0: loss = 0.130586 (* 1 = 0.130586 loss)
I0430 16:56:24.217500   966 sgd_solver.cpp:106] Iteration 198800, lr = 1e-28
I0430 16:57:13.827188   966 solver.cpp:228] Iteration 198900, loss = 0.117641
I0430 16:57:13.827349   966 solver.cpp:244]     Train net output #0: loss = 0.117641 (* 1 = 0.117641 loss)
I0430 16:57:13.827356   966 sgd_solver.cpp:106] Iteration 198900, lr = 1e-28
I0430 16:58:04.846093   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_199000.caffemodel
I0430 16:58:17.521347   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_199000.solverstate
I0430 16:58:17.707849   966 solver.cpp:337] Iteration 199000, Testing net (#0)
I0430 16:58:17.707948   966 net.cpp:693] Ignoring source layer score_conv_u0d-score_0_split
I0430 16:58:17.707953   966 net.cpp:693] Ignoring source layer visualize
I0430 16:58:17.707955   966 net.cpp:693] Ignoring source layer fake
I0430 17:03:09.780349   966 solver.cpp:404]     Test net output #0: loss = 0.179913 (* 1 = 0.179913 loss)
I0430 17:03:10.089702   966 solver.cpp:228] Iteration 199000, loss = 0.125994
I0430 17:03:10.089722   966 solver.cpp:244]     Train net output #0: loss = 0.125994 (* 1 = 0.125994 loss)
I0430 17:03:10.089745   966 sgd_solver.cpp:106] Iteration 199000, lr = 1e-28
I0430 17:03:59.681756   966 solver.cpp:228] Iteration 199100, loss = 0.115456
I0430 17:03:59.681893   966 solver.cpp:244]     Train net output #0: loss = 0.115456 (* 1 = 0.115456 loss)
I0430 17:03:59.681900   966 sgd_solver.cpp:106] Iteration 199100, lr = 1e-28
I0430 17:04:49.277860   966 solver.cpp:228] Iteration 199200, loss = 0.126289
I0430 17:04:49.277997   966 solver.cpp:244]     Train net output #0: loss = 0.126289 (* 1 = 0.126289 loss)
I0430 17:04:49.278003   966 sgd_solver.cpp:106] Iteration 199200, lr = 1e-28
I0430 17:05:38.870748   966 solver.cpp:228] Iteration 199300, loss = 0.0493829
I0430 17:05:38.870887   966 solver.cpp:244]     Train net output #0: loss = 0.0493829 (* 1 = 0.0493829 loss)
I0430 17:05:38.870893   966 sgd_solver.cpp:106] Iteration 199300, lr = 1e-28
I0430 17:06:30.175096   966 solver.cpp:228] Iteration 199400, loss = 0.160582
I0430 17:06:30.175216   966 solver.cpp:244]     Train net output #0: loss = 0.160582 (* 1 = 0.160582 loss)
I0430 17:06:30.175222   966 sgd_solver.cpp:106] Iteration 199400, lr = 1e-28
I0430 17:07:19.767202   966 solver.cpp:228] Iteration 199500, loss = 0.121198
I0430 17:07:19.767354   966 solver.cpp:244]     Train net output #0: loss = 0.121198 (* 1 = 0.121198 loss)
I0430 17:07:19.767360   966 sgd_solver.cpp:106] Iteration 199500, lr = 1e-28
I0430 17:08:09.360648   966 solver.cpp:228] Iteration 199600, loss = 0.100863
I0430 17:08:09.360801   966 solver.cpp:244]     Train net output #0: loss = 0.100863 (* 1 = 0.100863 loss)
I0430 17:08:09.360810   966 sgd_solver.cpp:106] Iteration 199600, lr = 1e-28
I0430 17:09:00.548682   966 solver.cpp:228] Iteration 199700, loss = 0.0842546
I0430 17:09:00.548835   966 solver.cpp:244]     Train net output #0: loss = 0.0842546 (* 1 = 0.0842546 loss)
I0430 17:09:00.548842   966 sgd_solver.cpp:106] Iteration 199700, lr = 1e-28
I0430 17:09:50.130669   966 solver.cpp:228] Iteration 199800, loss = 0.620815
I0430 17:09:50.130884   966 solver.cpp:244]     Train net output #0: loss = 0.620815 (* 1 = 0.620815 loss)
I0430 17:09:50.130903   966 sgd_solver.cpp:106] Iteration 199800, lr = 1e-28
I0430 17:10:39.730536   966 solver.cpp:228] Iteration 199900, loss = 0.115106
I0430 17:10:39.730679   966 solver.cpp:244]     Train net output #0: loss = 0.115106 (* 1 = 0.115106 loss)
I0430 17:10:39.730685   966 sgd_solver.cpp:106] Iteration 199900, lr = 1e-28
I0430 17:11:30.237093   966 solver.cpp:454] Snapshotting to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_200000.caffemodel
I0430 17:11:33.526881   966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/unet_weighted_batchnorm_3/unet_weighted_batchnorm_3_iter_200000.solverstate
